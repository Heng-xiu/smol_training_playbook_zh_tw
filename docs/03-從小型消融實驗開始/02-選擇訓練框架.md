### 選擇訓練框架

我們需要做的第一個決定是使用哪個框架來訓練我們的模型，以及用它來執行所有的消融實驗。這個選擇涉及平衡三個關鍵考量：

> ⚠️ 不要當英雄，在消融實驗和最終執行之間切換訓練框架。那是通往痛苦的道路。

1. 框架必須支援我們的目標架構，或讓我們能輕鬆擴展它
2. 它需要穩定且可用於生產，不易在訓練中途神祕地崩潰
3. 它應該提供強大的吞吐量，讓我們能快速迭代並充分利用算力預算

在實踐中，這些要求可能相互牽制，產生權衡。讓我們看看可用的選項。

| 框架 | 功能 | 久經考驗 | 優化程度 | 程式碼行數（核心/總計） | 可擴展性與除錯 |
|---|---|---|---|---|---|
| **Megatron-LM** | ✅ 廣泛 | ✅ Kimi-K2, Nemotron | ✅ 3D 並行先驅 | 93k / 269k | ⚠️ 初學者困難 |
| **DeepSpeed** | ✅ 廣泛 | ✅ BLOOM, GLM | ✅ ZeRO & 3D 並行先驅 | 94k / 194k | ⚠️ 初學者困難 |
| **TorchTitan** | ⚡ 功能集成長中 | ⚠️ 較新但經 PyTorch 團隊測試 | ⚡ 為密集模型優化，MoE 改進中 | 7k / 9k | ⚡ 中等：需要並行知識 |
| **Nanotron** | 🎯  精簡，為 HF 預訓練量身打造 | ✅ 是（StarCoder, SmolLM） | ✅ 優化（UltraScale Playbook） | 15k / 66k | ⚡ 中等：需要並行知識 |

上表總結了流行框架之間的關鍵權衡。前三個框架的程式碼行數來自 TorchTitan 技術報告（[Liang et al., 2025](https://arxiv.org/abs/2410.06511)）。讓我們更詳細地討論每一個：

[Megatron-LM](https://github.com/NVIDIA/Megatron-LM) 來自 Nvidia，已經存在多年且久經考驗。它驅動著像 Kimi 的 K2（[Team et al., 2025](https://arxiv.org/abs/2507.20534)）這樣的模型，提供穩固的吞吐量，並擁有我們想要的大部分生產功能。但這種成熟度伴隨著複雜性：當你是新手時，程式碼庫可能難以導航和修改。

[DeepSpeed](https://github.com/deepspeedai/DeepSpeed) 屬於類似類別。它是 ZeRO 優化的先驅，驅動了像 BLOOM 和 GLM 這樣的模型。與 Megatron-LM 一樣，它經過廣泛的實戰測試和優化，但面臨相同的複雜性挑戰。龐大的程式碼庫（總計 194k 行）在你剛開始時可能令人望而生畏，特別是在實作自訂功能或除錯意外行為時。

另一方面，PyTorch 最近的 [TorchTitan](https://github.com/pytorch/torchtitan) 函式庫輕量得多，更易於導航，得益於其緊湊和模組化的程式碼庫。它擁有預訓練所需的核心功能，非常適合快速實驗。然而，作為較新的框架，它沒有經過那麼多實戰測試，在積極開發中可能還有點不穩定。

我們走了一條不同的道路，從頭開始建立了自己的框架 nanotron。這給了我們完全的靈活性和對大規模預訓練的深入理解；這些洞見後來演變成了[超大規模實戰手冊](https://huggingface.co/spaces/nanotron/ultrascale-playbook)。自從我們開源這個函式庫以來，我們也從社群獲得了寶貴的回饋，儘管在大多數情況下我們必須首先自己對功能進行實戰測試。該框架現在支援我們訓練所需的所有生產功能，但我們仍在建構像 MoE 支援這樣的領域。

從頭開始建立在當時是有意義的，但它需要在團隊專業知識和時間上進行重大投資，以除錯問題和添加缺失的功能。一個強大的替代方案是 fork 現有框架並根據你的需求增強它。例如，Thinking Machines Lab 將他們的內部預訓練函式庫建構為 TorchTitan 的 fork（[來源](https://x.com/cHHillee/status/1949470943291805832)）。

最終，你的選擇取決於你團隊的專業知識、目標功能，以及你願意在開發上投入多少時間，而不是使用最適合生產的選項。

> 💡 如果多個框架支援你的需求，在你的特定硬體上比較它們的吞吐量。對於快速實驗和速度執行，更簡單的程式碼庫通常獲勝。
