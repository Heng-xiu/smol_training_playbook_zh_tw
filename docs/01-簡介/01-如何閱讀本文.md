### 如何閱讀本文

你不需要從頭到尾線性閱讀這篇文章，事實上它已經長到不太可能一口氣讀完了。這篇文章由幾個獨立的板塊組成，你可以跳過或單獨閱讀：

- **訓練羅盤**：一場關於「是否應該預訓練自己的模型」的高層次探討。在燒光所有風投的錢之前，我們會引導你思考一些根本性的問題，以及如何系統性地做出決策。這是一個宏觀章節，如果你想直奔技術內容，可以快速跳過這部分。

- **預訓練**：訓練羅盤之後的章節涵蓋了構建扎實預訓練方案所需的一切：如何進行消融實驗、選擇評估指標、混合資料源、做出架構抉擇、調優超參數，以及最終撐過訓練馬拉松。這一部分同樣適用於那些不打算從零開始預訓練，但對持續預訓練（又稱中期訓練）感興趣的讀者。

- **後訓練**：在這部分你會學到榨乾預訓練模型潛力的所有技巧。從 SFT、DPO 到 GRPO 的後訓練全套字母表，以及模型融合的黑魔法與煉金術。關於如何讓這些演算法真正奏效的知識，大多來自痛苦的教訓，我們會在這裡分享我們的經驗，希望能幫你少踩一些坑。

- **基礎設施**：如果說預訓練是蛋糕，後訓練是糖霜和櫻桃，那麼基礎設施就是工業級烤箱。沒有它，什麼都做不成；如果它壞了，你愉快的周日烘焙時光就會變成火災現場。關於如何理解、分析和除錯 GPU 叢集的知識散落在網際網路的各個函式庫、文件和論壇中。這個章節會帶你了解 GPU 布局、CPU/GPU/節點/儲存之間的通訊模式，以及如何識別和克服瓶頸。

那麼，從哪裡開始呢？選擇你最感興趣的章節，出發吧！

如果你有問題或想法，歡迎在[社群討論區](https://huggingface.co/spaces/HuggingFaceTB/smol-training-playbook/discussions)開啟話題！
