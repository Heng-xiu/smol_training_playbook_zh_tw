### 架構選擇

如果你觀察像 Qwen3、Gemma3 或 DeepSeek v3 這樣的近期模型,你會發現儘管它們各有差異,但都共享同一個基礎——2017年引入的 transformer 架構([Vaswani et al., 2023](https://arxiv.org/abs/1706.03762))。這些年來改變的不是基本結構,而是對其核心組件的精煉。無論你正在建構密集型模型、MoE,還是混合架構,你都在使用這些相同的基礎模組。

這些精煉來自於追求更好效能和應對特定挑戰的團隊:推理時的記憶體限制、大規模訓練的不穩定性,或處理更長上下文的需求。一些修改,如從 Multi-Head Attention (MHA) 轉向更具算力效率的注意力變體(如 Grouped Query Attention (GQA) ([Ainslie et al., 2023](https://arxiv.org/abs/2305.13245))),現在已被廣泛採用。其他如不同的位置編碼方案,仍在討論中。最終,今天的實驗將結晶成明天的基準線。

那麼現代 LLM 今天實際上使用什麼呢?讓我們看看領先模型的共識。不幸的是,並非所有模型都公開其訓練細節,但我們從 DeepSeek、OLMo、Kimi 和 SmolLM 等家族獲得了足夠的透明度,可以看到當前的局面:

| 模型 | 架構 | 參數 | 訓練 Token 數 | Attention | 上下文長度(最終) | 位置編碼 | 精度 | 初始化(std) | 優化器 | 最大 LR | LR 排程 | Warmup 步數 | 批次大小 |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| DeepSeek LLM 7B | Dense | 7B | 2T | GQA | 4K | RoPE | BF16 | 0.006 | AdamW | 4.2×10⁻⁴ | Multi-Step | 2K | 9.4M |
| DeepSeek LLM 67B | Dense | 67B | 2T | GQA | 4K | RoPE | BF16 | 0.006 | AdamW | 3.2×10⁻⁴ | Multi-Step | 2K | 18.9M |
| DeepSeek V2 | MoE | 236B (21B active) | 8.1T | MLA | 128K | Partial RoPE | - | 0.006 | AdamW | 2.4×10⁻⁴ | Multi-Step | 2K | 9.4M→37.7M (warmup 225B) |
| DeepSeek V3 | MoE | 671B (37B active) | 14.8T | MLA | 129K | Partial RoPE | FP8 | 0.006 | AdamW | 2.2×10⁻⁴ | Multi-Step + Cosine | 2K | 12.6M→62.9M (warmup 469B) |
| MiniMax-01 | MoE + Hybrid | 456B (45.9 active) | 11.4T | Linear attention + GQA | 4M | Partial RoPE | - | Xavier init with deepnorm scaling | AdamW | 2×10⁻⁴ | Multi-Step | 500 | 16M→32M→64M→128M |
| Kimi K2 | MoE | 1T (32B active) | 15.5T | MLA | 128K | Partial RoPE | BF16 | likely 0.006 | MuonClip | 2×10⁻⁴ | WSD | 500 | 67M |
| OLMo 2 7B | Dense | 7B | 5T | MHA | 4K | RoPE | BF16 | 0.02 | AdamW | 3×10⁻⁴ | Cosine | 2K | 4.2M |
| SmolLM3 | Dense | 3B | 11T | GQA | 128K | NoPE | BF16 | 0.02 | AdamW | 2×10⁻⁴ | WSD | 2K | 2.3M |

如果你還不理解其中一些術語,如 MLA 或 NoPE 或 WSD,別擔心。我們將在本節中解釋每一個。現在,只需注意多樣性:不同的注意力機制(MHA、GQA、MLA)、位置編碼(RoPE、NoPE、partial RoPE)和學習率排程(Cosine、Multi-Step、WSD)。

看著這一長串架構選擇,要弄清楚從哪裡開始確實有點令人不知所措。如同大多數這類情況,我們將一步步來,逐漸建立所有必要的知識。我們將首先專注於最簡單的基礎架構(密集型模型),並詳細研究每個架構面向。之後,我們將深入探討 MoE 和 Hybrid 模型,並討論何時使用它們是好選擇。最後,我們探索 tokenizer,這個常被忽視和低估的組件。我們應該使用現有的還是訓練自己的?我們甚至如何評估 tokenizer 是否優秀?

> **消融實驗設置**
>
> 在本章的其餘部分,我們透過使用上述章節中描述的設置進行消融實驗來驗證大多數架構選擇:我們的 1B 基準線模型(遵循 Llama3.2 1B 架構)在 FineWeb-Edu、FineMath 和 Python-Edu 混合的 45B token 上訓練。對於每個實驗,我們展示訓練損失曲線和下游評估分數,以評估每個修改的影響。你可以在 [HuggingFaceTB/training-guide-nanotron-configs](https://huggingface.co/datasets/HuggingFaceTB/training-guide-nanotron-configs/tree/main) 找到所有執行的配置。

Sebastian Raschka 的這篇[部落格文章](https://sebastianraschka.com/blog/2025/the-big-llm-architecture-comparison.html)對 2025 年的現代 LLM 架構提供了很好的概覽。

但現在讓我們從每個 LLM 的核心開始:注意力機制。

#### **Attention**

transformer 架構研究中最活躍的領域之一是注意力機制。雖然前饋層在預訓練期間主宰算力,但注意力在推理時成為主要瓶頸(特別是在長上下文時),它推高算力成本,而 KV cache 快速消耗 GPU 記憶體,降低吞吐量。讓我們快速瀏覽主要的注意力機制以及它們如何在容量和速度之間進行權衡。

**我的注意力需要多少個頭?**

_Multi-head attention (MHA)_ 是原始 transformer 引入的標準注意力([Vaswani et al., 2023](https://arxiv.org/abs/1706.03762))。主要想法是你有 N 個注意力頭,各自獨立執行相同的檢索任務:將隱藏狀態轉換為 queries、keys 和 values,然後使用當前 query 透過匹配 keys 來檢索最相關的 token,最後轉發與匹配 token 相關聯的 value。在推理時,我們不需要重新計算過去 token 的 KV 值,可以重用它們。過去 KV 值的記憶體稱為 _KV-Cache_。隨著上下文視窗增長,這個 cache 可能很快成為推理瓶頸並消耗大量 GPU 記憶體。這裡有一個簡單的計算來估計 Llama 3 架構的 KV-Cache 記憶體 $s_{KV}$,序列長度為 8192:

> 💡 查看 [Jay Alamar 的著名部落格](https://jalammar.github.io/illustrated-transformer/)文章快速複習!

$$\begin{equation} \begin{aligned} s_{KV} &= 2 \times n_{bytes} \times seq \times n_{layers} \times n_{heads} \times dim_{heads} \\ &= 2 \times 2 \times8192 \times 32 \times 32 \times 128 =4 \text{ GB} \textit{ (Llama 3 8B)} \\ &= 2 \times 2 \times8192 \times 80 \times 64 \times 128 =20 \text{ GB} \textit{ (Llama 3 70B)} \end{aligned} \end{equation}$$

注意領先因子 2 來自儲存 key 和 value cache。如你所見,cache 隨序列長度線性增加,但上下文視窗已呈指數增長,現在達到數百萬 token。因此,提高 cache 效率將使在推理時擴展上下文變得更加容易。

自然要問的問題是:我們真的需要為每個頭準備新的 KV 值嗎?可能不需要,Multi-Query Attention (MQA) ([Shazeer, 2019](https://arxiv.org/abs/1911.02150)) 和 Grouped Query Attention (GQA) ([Ainslie et al., 2023](https://arxiv.org/abs/2305.13245)) 都解決了這個問題。最簡單的情況是跨所有頭共享 KV 值,從而將 KV cache 的大小除以 $n_{heads}$,例如對於 Llama 3 70B 減少 64 倍!這就是 MQA 的想法,在某些模型如 StarCoder 中被用作 MHA 的替代方案。然而,我們可能會放棄比我們願意的更多注意力容量,因此我們可以考慮中間路線,在頭組之間共享 KV 值,例如 4 個頭共享相同的 KV 值。這就是 GQA 方法,在 MQA 和 MHA 之間取得平衡。

最近,DeepSeek-v2(v3 中也使用)引入了 _Multi-Latent Attention (MLA)_ ([DeepSeek-AI et al., 2024](https://arxiv.org/abs/2405.04434)),它使用不同的策略來壓縮 cache:不是減少 KV 值的數量,而是減少它們的大小,並簡單地儲存一個可以在執行時解壓縮為 KV 值的潛在變數。透過這種方法,他們成功地將 cache 減少到相當於 2.25 組的 GQA,同時提供比 MHA 更強的效能!為了使這與 RoPE 一起工作,需要一個額外的小潛在向量的小調整。在 DeepSeek-v2 中,他們為主要潛在變數選擇了 $4*dim_{head}$,為 RoPE 部分選擇了 $1/2*dim_{head}$,因此總共 $4.5*dim_{head}$,同時用於 K 和 V,從而降低了領先因子 2。

> 💡 **RoPE (Rotary Position Embeddings)** 是一種透過根據 query 和 key 向量在序列中的位置旋轉它們來編碼位置資訊的方法。它在今天的 LLM 中被普遍使用。

你可以在下圖中看到每種注意力機制的視覺化解釋:

**圖例**
Values | Keys | Queries | 推理時快取

**Multi head attention**
V K Q | V K Q | V K Q | V K Q | V K Q | V K Q | V K Q | V K Q

**Multi query attention**
K V | Q Q Q Q Q Q Q Q

**Grouped query attention**
K V | K V | K V | K V | Q Q Q Q Q Q Q Q

**Multi head latent attention**
V K Q | V K Q | V K Q | V K Q | V K Q | V K Q | V K Q | V K Q → Compressed Latent KV projection

_Multi-Head Attention (MHA)、Grouped-Query Attention (GQA)、Multi-Query Attention (MQA) 和 Multi-head Latent Attention (MLA) 的簡化示意圖。透過將 keys 和 values 聯合壓縮到潛在向量中,MLA 顯著減少了推理期間的 KV cache。_

下表比較了我們剛才討論的注意力機制。為簡單起見,我們比較每個 token 使用的參數,如果你想計算總記憶體,只需乘以每個參數的位元組數(通常為 2)和序列長度:

| Attention 機制 | 每個 token 的 KV-Cache 參數 |
|---|---|
| MHA | $= 2 \times n_{heads} \times n_{layers} \times dim_{head}$ |
| MQA | $= 2 \times 1 \times n_{layers} \times dim_{head}$ |
| GQA | $= 2 \times g \times n_{layers} \times dim_{head}$ (通常 g=2,4,8) |
| MLA | $= 4.5 \times n_{layers} \times dim_{head}$ |

現在讓我們看看這些注意力機制在實際實驗中的表現!

**消融實驗 - GQA 勝過 MHA**

這裡我們比較不同的注意力機制。我們的[基準線](https://huggingface.co/datasets/HuggingFaceTB/ablations-training-configs/blob/main/baseline_config_1B.yaml)模型使用 32 個頭和 8 個 KV 頭,對應於比率為 32/8=4 的 GQA。如果我們使用 MHA,或者如果我們使用更少的 KV 頭和更高的 GQA 比率,效能會如何變化?

> 💡 一些函式庫稱 GQA 比率為:Query groups = Query heads / KV heads

改變 KV 頭的數量會影響參數數量,特別是對於 MHA 的情況。為了一致性,我們調整 MHA 執行的層數,因為否則會有 100M+ 的參數差異;對於其餘的,我們保持預設的 16 層。

| Attention 類型 | Query 頭數 | KV 頭數 | 層數 | 參數數量 | 備註 |
|---|---|---|---|---|---|
| MQA | 32 | 1 | 16 | 1.21B | |
| GQA (比率 16) | 32 | 2 | 16 | 1.21B | |
| GQA (比率 8) | 32 | 4 | 16 | 1.22B | **我們的基準線** |
| GQA (比率 4) | 32 | 8 | 16 | 1.24B | |
| GQA (比率 2) | 32 | 16 | 15 | 1.22B | 減少層數 |
| MHA | 32 | 32 | 14 | 1.20B | 減少層數 |
| GQA (比率 2) | 32 | 16 | 16 | 1.27B | 太大 - 未消融 |
| MHA | 32 | 32 | 16 | 1.34B | 太大 - 未消融 |

所以我們比較 MHA、MQA 和 4 種 GQA 設置(比率 2、4、8、16)。你可以在[這裡](https://huggingface.co/datasets/HuggingFaceTB/training-guide-nanotron-configs/tree/main/attention)找到 nanotron 配置。

觀察消融實驗結果,我們發現 MQA 和 GQA 16 組(分別只留下 1 和 2 個 KV 頭)的效能明顯低於 MHA。另一方面,具有 2、4 和 8 組的 GQA 配置大致匹配 MHA 效能。

_重置視圖_
_圖例_
HellaSwag | MMLU | ARC | PIQA | OpenBookQA | WinoGrande

結果在損失曲線和下游評估中都是一致的。我們在 HellaSwag、MMLU 和 ARC 等基準測試中清楚地觀察到這一點,而 OpenBookQA 和 WinoGrande 等基準測試則顯示出一點雜訊。

基於這些消融實驗,GQA 是 MHA 的可靠替代方案。它在保持效能的同時在推理時更有效率。一些最近的模型採用了 MLA 以實現更大的 KV cache 壓縮,儘管它還沒有被廣泛採用。我們沒有消融 MLA,因為它在消融實驗時尚未在 nanotron 中實作。對於 SmolLM3,我們使用了 4 組的 GQA。

除了注意力架構本身,我們在訓練期間使用的注意力模式也很重要。讓我們看看注意力遮罩。

**文件遮罩**

我們如何在訓練序列中應用注意力會影響算力效率和模型效能。這帶我們來到 _文件遮罩_ 以及如何在 dataloader 中組織訓練樣本的更廣泛問題。

在預訓練期間,我們使用固定的序列長度進行訓練,但我們的文件具有可變長度。一篇研究論文可能有 10k token,而一個簡短的程式碼片段可能只有幾百個 token。我們如何將可變長度的文件放入固定長度的訓練序列中?填充較短的文件以達到我們的目標長度會在無意義的填充 token 上浪費算力。相反,我們使用 **packing**:打亂並連接帶有序列結束(EOS) token 的文件,然後將結果分割成與序列大小匹配的固定長度塊。

> 💡 我們也可以在文件開頭添加 BOS token。在這種情況下,你會注意到模型/tokenizer 配置中存在不同的 `bos_token_id`。

這在實踐中看起來像這樣:

```
File 1: "Recipe for granola bars..." (400 tokens) <EOS>
File 2: "def hello_world()..." (300 tokens) <EOS>
File 3: "Climate change impacts..." (1000 tokens) <EOS>
File 4: "import numpy as np..." (3000 tokens) <EOS>
...
連接並分塊成 4k 序列後:
Sequence 1: [File 1] + [File 2] + [File 3] + [partial File 4]
Sequence 2: [rest of File 4] + [File 5] + [File 6] + ...
```

如果訓練序列足夠長以填充我們的 4k 上下文,它可能包含一個完整的文件,但在大多數情況下文件很短,因此序列包含多個隨機文件的連接。

使用標準因果遮罩,token 可以關注打包序列中的所有先前 token。在上面的範例中,文件 4 的 Python 函數中的 token 可以關注燕麥棒食譜、氣候變遷文章以及任何其他恰好打包在一起的內容。讓我們快速看一下典型的 4k 預訓練上下文會包含什麼。快速[分析](https://www.harmdevries.com/post/context-length/)顯示,CommonCrawl 和 GitHub 中大部分(約 80-90%)的文件短於 2k token。

下圖檢查了本部落格中使用的較新資料集的 token 分布:

_0-1k | 1-2k | 2-4k | 4-8k | 8-16k | 16k+_
_文件 (%)_
_資料集: FineWeb-Edu | DCLM | FineMath | Python-Edu_

FineWeb-Edu、DCLM、FineMath 和 Python-Edu 中超過 80% 的文件包含少於 2k token。這意味著使用 2k 或 4k 訓練序列和標準因果遮罩,絕大多數 token 會花費算力關注打包在一起的無關文件。

> **PDF 中的較長文件**
>
> 雖然大多數基於網路的資料集由短文件組成,但基於 PDF 的資料集包含更長的內容。[FinePDFs](https://huggingface.co/datasets/HuggingFaceFW/fineweb-2) 文件平均比網路文字長 2 倍,當與 FineWeb-Edu 和 DCLM 混合時,它們可以提高效能。

除了算力效率低下,[Zhao et al. (2024)](https://doi.org/10.18653/v1/2024.acl-long.427) 發現這種方法會引入來自無關內容的雜訊,可能會降低效能。他們建議使用 _intra-document masking_(文件內遮罩),我們修改注意力遮罩,使 token 只能關注同一文件內的先前 token。下面的視覺化說明了這種差異:

_訓練序列 = 連接的文件_
`Mix oats with honey<|end_of_text|>def hello():print<|end_of_text|>Climate change affects the planet<|end_of_text|>`

**Causal Masking**
Recipe | Code | Climate
       | Recipe | Code | Climate

**Intra-Document Masking**
Recipe | Code | Climate
       | Recipe | Code | Climate

_可以關注 | 無法關注_

[Zhu et al. (2025)](https://arxiv.org/abs/2503.15450) 在 SkyLadder 中發現了文件內遮罩的類似好處,但提供了不同的解釋。他們發現較短的上下文長度在訓練中效果更好,而文件內遮罩有效地減少了平均上下文長度。

![Image](https://huggingfacetb-smol-training-playbook.hf.space/_astro/image_27c1384e-bcac-807c-807b-fac08be1d884.C286JbWA_AmD26.webp)

來自 SkyLadder 的這些圖表展示了多個發現:(a) 較短的上下文在預訓練期間通常表現更好(較低的驗證困惑度),(b) 文件內遮罩(IntraDoc)實現了比隨機打包(Random)和語義分組(BM25)更低的困惑度,(c) 即使沒有位置編碼,較短上下文優勢也成立,(d) IntraDoc 創建了一個傾向於較短有效上下文長度的分布。

Llama3 ([Grattafiori et al., 2024](https://arxiv.org/abs/2407.21783)) 也使用文件內遮罩進行訓練,他們發現在短上下文預訓練期間影響有限,但對於長上下文擴展有顯著好處,那裡注意力開銷變得更加顯著。此外,ProLong 論文([Gao et al., 2025](https://arxiv.org/abs/2410.02660))表明,使用文件遮罩在持續預訓練中擴展 Llama3 8B 的上下文,對長上下文和短上下文基準測試都有好處。

我們決定在 1B 基準線模型上執行消融實驗,測試文件遮罩是否影響短上下文效能。你可以在[這裡](https://huggingface.co/datasets/HuggingFaceTB/training-guide-nanotron-configs/blob/main/doc_masking/doc_masking.yaml)找到配置。結果顯示與標準因果遮罩相比,損失曲線和下游評估分數相同,如下圖所示。

要在 nanotron 中啟用文件遮罩,只需在模型配置中將此旗標設置為 `true`:

```yaml
model_config:
  _attn_implementation: flash_attention_2
  _fused_rms_norm: true
  _fused_rotary_emb: true
- _use_doc_masking: false
+ _use_doc_masking: true
```

_重置視圖_
_圖例_
HellaSwag | MMLU | ARC | PIQA | OpenBookQA | WinoGrande

與 Llama3 類似,我們沒有觀察到對短上下文任務的明顯影響,除了 PIQA 上的小幅改進。然而,在擴展到長序列以加速訓練時,文件遮罩變得至關重要。這對於我們的長上下文擴展特別重要,我們從 4k 擴展到 64k token(在[訓練馬拉松](https://huggingfacetb-smol-training-playbook.hf.space/#the-training-marathon)章節中詳細說明)。因此,我們在 SmolLM3 的整個訓練執行中採用了它。

我們在本節中介紹了注意力如何處理序列。現在讓我們看看 transformer 中的另一個主要參數塊:embedding。

#### **Embedding 共享**

如果你查看我們基準線消融實驗模型的[配置](https://huggingface.co/datasets/HuggingFaceTB/training-guide-nanotron-configs/blob/main/baseline_config_1B.yaml),與標準 transformer 不同的一件事是透過 `tie_word_embeddings` 旗標啟用的 embedding 共享。

LLM 有兩個 embedding 組件:輸入 embedding 作為 token 到向量的查找表(大小為 vocab_size × hidden_dim)和輸出 embedding,這是將隱藏狀態映射到詞彙表 logits 的最終線性層(hidden_dim × vocab_size)。在這些是獨立矩陣的經典情況下,總 embedding 參數為 2 × vocab_size × hidden_dim。因此,在小型語言模型中,embedding 可以構成總參數數量的很大一部分,特別是在詞彙表大小很大的情況下。這使得 embedding 共享(在輸出中重用輸入 embedding)成為小型模型的自然優化。

_詞彙表大小_ 128k
_隱藏大小_ 2048
_總模型大小 (M)_ 1500 M
_總 Embedding 參數_ 524.3 M
_模型百分比_ 35%
_權重綁定節省_ 262.1 M (17%)

**獨立 Embeddings** 1.5 B
輸入 Embedding: 128k × 2k
Layer 1 → Layer N
輸出投影: 2k × 128k

**綁定 Embeddings** 1.2 B
輸入 Embedding: 128k × 2k
Layer 1 → Layer N
輸出投影(共享): 2k × 128k (轉置)

較大的模型通常不使用這種技術,因為 embedding 佔其參數預算的比例較小。例如,沒有共享的總 embedding 在 Llama3.2 8B 中僅佔 13%,在 Llama3.1 70B 中佔 3%,如下面的餅圖所示。

### 選擇模型

_Llama 3.2 1B | SmolLM3 3B | Llama 3.1 8B | Llama 3.1 70B_

1.24 B 參數
層數: 16
隱藏大小: 2048
頭數 (Q/KV): 32/8
中間層: 8192
Embeddings: Tied

_21.3% | 13.6% | 65.2%_
_圖例: Embeddings | Attention | Feed Forward | Layer Norms_

**消融實驗 - 綁定 embedding 的模型匹配較大的非綁定變體**

現在我們將評估 embedding 共享對我們消融實驗模型的影響。我們從 [MobileLLM](https://arxiv.org/abs/2402.14905) 在 125M 規模上對這項技術的全面消融實驗中汲取見解,他們展示了共享將參數減少了 11.8%,準確性下降最小。

由於非綁定 embedding 將我們的參數數量從 1.2B 增加到 1.46B,我們將訓練另一個具有非綁定參數但層數較少的模型,使其與基準線 1.2B 的參數數量匹配。我們將比較兩個 1.2B 模型:我們的具有綁定 embedding 的基準線(16 層)與具有較少層(12 層)的非綁定版本以維持相同的參數預算,以及具有非綁定 embedding 和與我們基準線相同層數(16)的 1.46B 模型作為額外參考點。你可以在[這裡](https://huggingface.co/datasets/HuggingFaceTB/training-guide-nanotron-configs/blob/main/baseline_config_1B.yaml)找到 nanotron 配置。

_重置視圖_
_圖例_
HellaSwag | MMLU | ARC | PIQA | OpenBookQA | WinoGrande

損失和評估結果表明,我們的具有綁定 embedding 的基準線 1.2B 模型在所有基準測試上(除了 WinoGrande)實現了與 1.46B 非綁定等效模型相當的效能,儘管參數少了 18%。具有非綁定 embedding 和減少層數(12 vs 16)的 1.2B 模型在兩種配置下表現都較差,表現出更高的損失和更低的下游評估分數。這表明在相同參數預算下,增加模型深度比解除 embedding 綁定提供更大的好處。

基於這些結果,我們為 SmolLM3 3B 模型保留了綁定 embedding。

我們現在已經探索了 embedding 共享策略及其權衡。但僅靠 embedding 無法捕獲序列中 token 的順序;提供這些資訊是位置編碼的作用。在下一節中,我們將看看位置編碼策略如何演變,從標準 RoPE 到像 NoPE(No Position Embedding)這樣的新方法,這些方法能夠更有效地建模長上下文。

#### **位置編碼與長上下文**

當 transformer 處理文字時,它們面臨一個基本挑戰:它們天生沒有詞序的概念,因為它們透過平行注意力操作同時消費整個序列。這實現了高效訓練,但創造了一個問題。如果沒有明確的位置資訊,從模型的角度來看,"Adam beats Muon" 看起來類似於 "Muon beats Adam"。

解決方案是位置 embedding:數學編碼,為序列中的每個 token 提供唯一的"地址"。但隨著我們推向越來越長的上下文——從早期 BERT 的 512 token 到今天的百萬 token 模型——位置編碼的選擇對效能和算力效率都變得越來越關鍵。

**位置編碼的演變**

早期的 transformer 使用簡單的 **Absolute Position Embeddings (APE)** ([Vaswani et al., 2023](https://arxiv.org/abs/1706.03762)),本質上是將每個位置(1、2、3...)映射到添加到 token embedding 的向量的學習查找表。這對短序列效果很好,但有一個主要限制:模型的最大輸入序列長度僅限於它們訓練時的最大輸入序列長度。它們沒有開箱即用的能力泛化到更長的序列。

該領域演變為 **相對位置編碼**,捕獲 token 之間的距離而不是它們的絕對位置。這很直觀,兩個詞相距 3 個位置比它們是在位置 (5,8) 還是 (105,108) 更重要。

> 💡 要深入了解位置編碼,[這篇部落格](https://huggingface.co/blog/designing-positional-encoding)逐步介紹了從基本定位到旋轉編碼的發展。

**ALiBi** (Attention with Linear Biases) ([Press et al., 2022](https://arxiv.org/abs/2108.12409)) 特別根據 token 距離修改注意力分數。兩個 token 相距越遠,它們的注意力透過應用於注意力權重的簡單線性偏差受到的懲罰就越多。有關 ALiBi 的詳細實作,請查看此[資源](https://nn.labml.ai/transformers/alibi/index.html)。

但主導近期大型語言模型的技術是 Rotary Position Embedding (RoPE) ([Su et al., 2023](https://arxiv.org/abs/2104.09864))。

**RoPE:位置即旋轉**

RoPE 的核心見解是將位置資訊編碼為高維空間中的旋轉角度。RoPE 不是將位置向量添加到 token embedding,而是根據 query 和 key 向量的絕對位置旋轉它們。

直覺是,我們將 embedding 中的每對維度視為圓上的座標,並透過以下因素確定的角度旋轉它們:
- token 在序列中的位置
- 我們處理的是哪個維度對(不同的對以不同的頻率旋轉,這些頻率是基礎/參考頻率的指數)

```python
import torch
def apply_rope_simplified(x, pos, dim=64, base=10000):
    """
    Rotary Position Embedding (RoPE)
    想法:
    - 每個 token 都有一個位置索引 p (0, 1, 2, ...)。
    - 每對向量維度都有一個索引 k (0 .. dim/2 - 1)。
    - RoPE 透過角度 θ_{p,k} 旋轉每對 [x[2k], x[2k+1]]。
    公式:
      θ_{p,k} = p * base^(-k / (dim/2))
    - 小 k (早期維度對) → 慢振盪 → 捕獲長程資訊。
    - 大 k (後期維度對) → 快振盪 → 捕獲細節。
    """
    rotated = []
    for i in range(0, dim, 2):
        k = i // 2  # 這個維度對的索引

        # 頻率項:更高的 k → 更快的振盪

        inv_freq = 1.0 / (base ** (k / (dim // 2)))
        theta = pos * inv_freq  # 位置 p 和對 k 的旋轉角度

        cos_t = torch.cos(torch.tensor(theta, dtype=x.dtype, device=x.device))
        sin_t = torch.sin(torch.tensor(theta, dtype=x.dtype, device=x.device))
        x1, x2 = x[i], x[i+1]
        # 應用 2D 旋轉

        rotated.extend([x1 * cos_t - x2 * sin_t,
                        x1 * sin_t + x2 * cos_t])
    return torch.stack(rotated)

## Q, K: [batch, heads, seq, d_head]

Q = torch.randn(1, 2, 4, 8)
K = torch.randn(1, 2, 4, 8)

## 👉 在點積*之前*將 RoPE 應用於 Q 和 K

Q_rope = torch.stack([apply_rope(Q[0,0,p], p) for p in range(Q.size(2))])
K_rope = torch.stack([apply_rope(K[0,0,p], p) for p in range(K.size(2))])
scores = (Q_rope @ K_rope.T) / math.sqrt(Q.size(-1))
attn_weights = torch.softmax(scores, dim=-1)
```

這段程式碼可能看起來很複雜,所以讓我們用一個具體的例子來分解它。考慮句子 _"The quick brown fox"_ 中的單詞 _"fox"_。在我們的基準線 1B 模型中,每個注意力頭使用 64 維的 query/key 向量。RoPE 將這個向量分組為 32 對:(x₁, x₂)、(x₃, x₄)、(x₅, x₆) 等等。我們處理對是因為我們在 2D 空間中圍繞圓旋轉。為簡單起見,讓我們關注第一對 (x₁, x₂)。單詞 "fox" 在我們的句子中出現在位置 3,因此 RoPE 將透過以下方式旋轉這第一個維度對:

```
rotation_angle = position × θ₀
               = 3 × (1/10000^(0/32))
               = 3 × 1.0
               = 3.0 radians
               = 172° degrees
```

我們的基礎頻率是 10000,但對於第一個維度對 (k=0),我們的指數為零,所以基礎頻率不影響計算(我們提升到 0 的冪)。下面的視覺化說明了這一點:

_基於 token 位置的 Q/K 向量中第一對 (x₁, x₂) 的 RoPE 旋轉_
_The | quick | brown | fox | jumps ..._

位置 0 的 The 被旋轉
θ = 0 rad (0°)

**RoPE 公式:** θ (theta) = position × 1 / base^(2 × pair_index/h_dim) (這裡 pair_index=0)

**關鍵見解:** 第一個維度對獲得最大的旋轉,詞之間的相對角度僅取決於它們相距的距離。

現在,當兩個 token 透過注意力互動時,魔法就發生了。它們旋轉表示之間的點積透過旋轉角度之間的相位差直接編碼它們的相對距離(其中 `m` 和 `n` 是 token 位置)

```
dot_product(RoPE(x, m), RoPE(y, n)) = Σₖ [xₖ * yₖ * cos((m-n) * θₖ)]
```

注意力模式僅取決於 (m-n),因此相距 5 個位置的 token 將始終具有相同的角度關係,無論它們在序列中的絕對位置如何。因此,模型學習基於距離的模式,這些模式在序列中的任何絕對位置都有效,並且可以推斷到更長的序列。

**如何設置 RoPE 頻率?**

在實踐中,大多數 LLM 預訓練從相對較短的上下文長度(2K-4K token)開始,使用幾萬的 RoPE 基礎頻率,如 10K 或 50K。由於注意力隨序列長度的二次縮放以及長上下文資料(樣本 > 4K 上下文長度)的有限可用性,從一開始使用非常長的序列進行訓練在算力上會很昂貴,正如我們之前在 [Attention](https://huggingfacetb-smol-training-playbook.hf.space/#attention) 的文件遮罩部分中看到的那樣。研究還表明,它可能會損害短上下文效能([Zhu et al., 2025](https://arxiv.org/abs/2503.15450))。模型通常從學習詞之間的短程相關性開始,所以長序列幫助不大。典型的方法是用較短的序列進行大部分預訓練,然後進行持續預訓練或在較長的序列上花費最後幾千億 token。然而,隨著序列長度增長,與 token 位置成比例的旋轉角度增長,並可能導致遠距離 token 的注意力分數衰減過快([Rozière et al., 2024](https://arxiv.org/abs/2308.12950); [Xiong et al., 2023](https://arxiv.org/abs/2309.16039)):

```
θ = position x 1 / (base^(k/(dim/2)))
```

解決方案是隨著序列長度增加而增加基礎頻率,以防止這種衰減,使用 ABF 和 YaRN 等方法。

**RoPE ABF (RoPE with Adjusted Base Frequency)** ([Xiong et al., 2023b](https://arxiv.org/abs/2309.16039)):透過增加 RoPE 公式中的基礎頻率來解決長上下文中的注意力衰減問題。這種調整減慢了 token 位置之間的旋轉角度,防止遠距離 token 的注意力分數衰減過快。ABF 可以在單階段(直接頻率提升)或多階段(隨著上下文增長逐漸增加)中應用。該方法實作起來很簡單,以增加的粒度分布嵌入向量,使模型更容易區分遠距離位置。雖然簡單有效,但 ABF 跨所有維度的統一縮放對於極長的上下文可能不是最優的。

**YaRN (Yet another RoPE extensioN)** ([Peng et al., 2023](https://arxiv.org/abs/2309.00071)):採用更複雜的方法,使用斜坡或縮放函數在 RoPE 維度上不均勻地插值頻率。與 ABF 的統一調整不同,YaRN 對不同的頻率組件應用不同的縮放因子,優化擴展的上下文視窗。它包括額外的技術,如注意力 logits 中的動態注意力縮放和溫度調整,這有助於在非常大的上下文大小下保持效能。YaRN 實現高效的"訓練短,測試長"策略,需要更少的 token 和更少的微調以實現穩健的推斷。雖然比 ABF 更複雜,但 YaRN 通常透過提供更平滑的縮放和減輕災難性注意力損失,為極長的上下文提供更好的經驗效能。它也可以僅在推理中利用,無需任何微調。

這些頻率調整方法減慢了注意力分數衰減效應並維持遠距離 token 的貢獻。例如,Qwen3 的訓練涉及使用 ABF 將頻率從 10k 增加到 1M,因為序列長度從 4k 擴展到 32k 上下文(然後團隊應用 YaRN 達到 131k,4 倍推斷)。請注意,對於最優值沒有強烈的共識,在上下文擴展階段嘗試不同的 RoPE 值以找到最適合你特定設置和評估基準的值通常是好的。

今天大多數主要模型使用 RoPE:Llama、Qwen、Gemma 等。該技術已被證明在不同的模型大小和架構(dense、MoE、Hybrid)中都很穩健。讓我們看看最近出現的幾種 RoPE 變體。

**混合位置編碼方法**

然而,隨著模型推向越來越大的上下文([Meta AI, 2025](https://ai.meta.com/blog/llama-4-multimodal-intelligence/); [Yang et al., 2025](https://arxiv.org/abs/2501.15383)),即使 RoPE 也開始遇到效能挑戰。在比 Needle in the Haystack (NIAH) ([Kamradt, 2023](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)) 更具挑戰性的長上下文基準測試(如 Ruler 和 HELMET ([Hsieh et al., 2024](https://arxiv.org/abs/2404.06654); [Yen et al., 2025](https://arxiv.org/abs/2410.02694)))上評估時,在長上下文擴展期間增加 RoPE 頻率的標準方法存在限制。已經引入了新技術來提供幫助。

我們在本節開始時說 transformer 需要位置資訊來理解 token 順序,但最近的研究挑戰了這一假設。如果根本不需要明確的位置編碼會怎樣?

**NoPE (No Position Embedding)** ([Kazemnejad et al., 2023](https://arxiv.org/abs/2305.19466)) 訓練沒有任何明確位置編碼的 transformer,允許模型透過因果遮罩和注意力模式隱式學習位置資訊。作者表明,與 ALiBi 和 RoPE 相比,這種方法展示了更好的長度泛化。沒有明確的位置編碼來推斷超出訓練長度,NoPE 自然地處理更長的上下文。然而在實踐中,與 RoPE 相比,NoPE 模型在短上下文推理和知識任務上往往表現較弱([Yang et al](https://arxiv.org/pdf/2501.18795))。這表明,雖然明確的位置編碼可能限制推斷,但它們為訓練上下文長度內的任務提供了有用的歸納偏差。

**RNoPE 混合方法:** 鑑於這些權衡,[B. Yang et al. (2025)](https://arxiv.org/abs/2501.18795) 建議結合不同的位置編碼策略可能很有趣。他們引入了 RNoPE,在整個模型中在 RoPE 和 NoPE 層之間交替。RoPE 層提供明確的位置資訊並處理具有最近偏差的局部上下文,而 NoPE 層改善了跨長距離的資訊檢索。這項技術最近在 Llama4、Command A 和 **SmolLM3** 中使用。

> **命名約定**
>
> 為了簡單起見,我們將在本部落格的其餘部分將 RNoPE 稱為 "NoPE"。(你會經常看到人們在討論中使用 "NoPE" 來表示 RNoPE)。

**消融實驗 - NoPE 在短上下文上匹配 RoPE**

讓我們測試混合 NoPE 方法。我們將純 RoPE 1B 消融實驗基準線與每 4 層移除位置編碼的 NoPE 變體,以及結合 NoPE 與文件遮罩的第三種配置進行比較,以測試這些技術之間的互動。我們的基本問題是:我們能否在獲得長上下文能力的同時保持強大的短上下文效能?

_重置視圖_
_圖例_
HellaSwag | MMLU | ARC | PIQA | OpenBookQA | WinoGrande

損失和評估結果顯示所有三種配置的效能相似,表明 NoPE 保持強大的短上下文能力,同時為更好的長上下文處理提供基礎。鑑於這些結果,我們為 SmolLM3 採用了 NoPE + 文件遮罩組合。

**Partial/Fractional RoPE:** 另一個互補的想法是只在模型維度的一個子集上應用 RoPE。與在 RoPE 和 NoPE 層之間交替整個層的 RNoPE 不同,Partial RoPE 在同一層內混合它們。最近的模型如 GLM‑4.5 ([5 Team et al., 2025](https://arxiv.org/abs/2508.06471)) 或 Minimax-01 ([MiniMax et al., 2025](https://arxiv.org/abs/2501.08313)) 採用了這種策略,但這在較舊的模型如 gpt-j ([Wang & Komatsuzaki, 2021](https://github.com/kingoflolz/mesh-transformer-jax)) 中也存在。你還會在每個使用 MLA 的模型中看到這一點,因為它是擁有合理推理成本的必備條件。

> **技術解釋:為什麼 Partial RoPE 對 MLA 至關重要**
>
> MLA 透過投影吸收使推理高效:它不是儲存每個頭的 keys $k_i^{(h)}$,而是快取一個小的共享潛在 $c_i = x_i W_c \in \mathbb{R}^{d_c}$ 並合併頭的 query/key 映射,因此每個分數都很便宜。使用 $q_t^{(h)} = x_t W_q^{(h)}$ 和 $k_i^{(h)} = c_i E^{(h)}$,定義 $U^{(h)} = W_q^{(h)} E^{(h)}$ 得到:
>
> $$s_{t,i}^{(h)} \;=\; \tfrac{1}{\sqrt{d_k}}\,\big(q_t^{(h)}\big)^\top k_i^{(h)}\;=\; \tfrac{1}{\sqrt{d_k}}\,\big(x_t U^{(h)}\big)^\top c_i$$
>
> 因此你使用 $\tilde q_t^{(h)} = x_t U^{(h)} \in \mathbb{R}^{d_c}$ 對微小的 cache $c_i$ 進行計算(沒有儲存每個頭的 k)。RoPE 破壞了這一點,因為它在兩個映射之間插入了依賴於對的旋轉:使用全維 RoPE,
>
> $$s_{t,i}^{(h)} \;=\; \tfrac{1}{\sqrt{d_k}}\,\big(x_t W_q^{(h)}\big)^\top \underbrace{R_{t-i}}_{\text{depends on } t-i}\,\big(c_i E^{(h)}\big)$$
>
> 因此你無法將 $W_q^{(h)}$ 和 $E^{(h)}$ 預先合併為固定的 $U^{(h)}$。修復:partial RoPE。分割頭維度 $d_k = d_{\text{nope}} + d_{\text{rope}}$,在大塊上不應用旋轉(像之前一樣吸收:$(x_t U_{\text{nope}}^{(h)})^\top c_i$),僅在小塊上應用 RoPE。

**限制長上下文的注意力範圍**

到目前為止,我們已經探索了如何處理長上下文的位置資訊:啟動 RoPE、禁用它(NoPE)、在某些層上部分應用它(RNoPE)或在某些隱藏維度上(Partial RoPE),或調整其頻率(ABF、YaRN)。這些方法修改了模型如何編碼位置以處理比訓練期間看到的序列更長的序列。但還有一個互補策略:我們可以限制哪些 token 可以相互關注,而不是調整位置編碼。

為了理解為什麼這很重要,考慮一個使用 8 個 token 序列預訓練的模型。在推理時,我們想處理 16 個 token(超過訓練長度)。位置 8-15 對於模型的位置編碼來說是分布外的。雖然像 RoPE ABF 這樣的技術透過調整位置頻率來解決這個問題,但注意力範圍方法採取不同的方法:它們策略性地限制哪些 token 可以相互關注,在仍然處理完整序列的同時將注意力模式保持在熟悉的範圍內。這減少了算力成本和記憶體需求。下圖比較了五種處理我們 16 token 序列的策略,預訓練視窗為 8:

_單個文件 (16 tokens)_
The (0) | history (1) | of (2) | artificial (3) | intelligence (4) | began (5) | in (6) | the (7) | twentieth (8) | century (9) | with (10) | fundamental (11) | research (12) | into (13) | computation (14) | and (15)

_預訓練視窗 = 8_

_注意力模式: Causal Masking (window=8) | RoPE ABF | Chunked Attention | Sliding Window Attention | DCA (Dual Chunk Attention)_

_模型只能關注過去 8 個 token_

_0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15_

**Chunked Attention** 將序列劃分為固定大小的塊,token 只能在其塊內關注。在我們的範例中,16 個 token 被分成兩個 8 token 塊(0 到 7 和 8 到 15),每個 token 只能看到其所在塊內的其他 token。注意 token 8 到 15 根本無法關注回早期的塊。這創建了在塊邊界處重置的孤立注意力視窗。Llama 4 ([Meta AI, 2025](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)) 在 RoPE 層(四個解碼器層中的三個)中使用 8192 token 塊的 chunked attention,而 NoPE 層保持完全的上下文存取。這透過限制每層的 KV cache 大小來減少記憶體需求,儘管這意味著 token 無法關注先前的塊,這可能會影響某些長上下文任務。

**Sliding Window Attention (SWA)** 由 Mistral 7B 推廣 ([Child et al., 2019](https://huggingfacetb-smol-training-playbook.hf.space/#bib-child2019generating); [Jiang et al., 2023](https://arxiv.org/abs/2310.06825)),基於最近 token 最相關的直覺採取了不同的方法。每個 token 不是硬塊邊界,而是只關注最近的 N 個 token。在圖中,每個 token 可以看到最多 8 個位置,創建一個在序列中連續移動的滑動視窗。注意 token 15 如何關注位置 8 到 15,而 token 10 關注位置 3 到 10。視窗向前滑動,在沒有分塊的人工障礙的情況下,在整個序列中保持局部上下文。Gemma 3 在交替層中結合 SWA 與完全注意力,類似於混合位置編碼方法如何混合不同策略。

**Dual Chunk Attention (DCA)** ([An et al., 2024](https://arxiv.org/abs/2402.17463)) 是一種無需訓練的方法,擴展了 **chunked attention** 同時保持跨塊資訊流。在我們的範例中,我們使用塊大小 s=4,將 16 個 token 劃分為 4 個塊(視覺化沿對角線的 4×4 方塊)。DCA 結合三種機制:(1) 塊內注意力,token 在其塊內正常關注(對角線模式)。(2) 塊間注意力,queries 使用位置索引 c−1=7 來關注先前的塊,創建上限為 7 的相對位置。(3) 連續塊注意力,具有局部視窗 w=3,保留相鄰塊之間的局部性。這將所有相對位置保持在訓練分布內(0 到 7),同時在塊邊界處保持平滑轉換。DCA 使 Qwen2.5 等模型能夠在推理時支援高達 100 萬 token 的超長上下文視窗,而無需在百萬 token 序列上進行持續訓練。

**Attention Sinks**

transformer 模型在長上下文中出現了一個有趣的現象:模型將異常高的注意力分數分配給序列中的初始 token,即使這些 token 在語義上並不重要。這種行為稱為 **attention sinks** ([Xiao et al.](https://arxiv.org/abs/2309.17453))。這些初始 token 充當注意力分布的穩定機制,作為可以累積注意力的「池」。

實際的洞見是,僅保留初始幾個 token 的 KV cache 以及最近 token 的滑動視窗,在上下文超過 cache 大小時很大程度上恢復了效能。這個簡單的修改使模型能夠處理更長的序列,而無需微調或效能下降。

現代實作以不同方式利用 attention sinks。原始研究建議在預訓練期間添加專用的佔位符 token 作為明確的 attention sink。最近,像 **gpt-oss** 這樣的模型將 attention sinks 實作為**學習的每頭偏差 logits**,附加到注意力分數而不是輸入序列中的實際 token。這種方法在不修改 tokenized 輸入的情況下實現相同的穩定效果。

有趣的是,gpt-oss 也在注意力層本身使用偏差單元,這是自 GPT-2 以來很少見到的設計選擇。雖然這些偏差單元通常被認為對標準注意力操作是多餘的([Dehghani et al.](https://arxiv.org/pdf/2302.08626) 的實證結果顯示對測試損失的影響很小),但它們可以服務於實作 attention sinks 的特殊功能。關鍵洞見:無論是實作為特殊 token、學習的偏差還是每頭 logits,attention sinks 在長上下文場景中為注意力分布提供穩定的「錨點」,允許模型儲存關於整個序列的一般有用資訊,即使上下文任意增長。

我們現在已經涵蓋了注意力的核心組件:平衡記憶體和計算的不同頭配置(MHA、GQA、MLA)、幫助模型理解 token 順序的位置編碼策略(RoPE、NoPE 及其變體),以及使長上下文可處理的注意力範圍技術(滑動視窗、分塊和 attention sinks)。我們還檢查了如何配置和初始化 embedding 層。這些架構選擇定義了你的模型如何處理和表示序列。

但擁有正確的架構只是戰鬥的一半。即使是設計良好的模型也可能遭受訓練不穩定性,尤其是在規模上。讓我們看看有助於保持訓練穩定的技術。

#### [提高穩定性](https://huggingfacetb-smol-training-playbook.hf.space/#improving-stability)

現在讓我們轉向 LLM 預訓練中最大的挑戰之一:不穩定性。通常表現為損失尖峰或訓練損失的突然跳躍,這些問題在規模上變得特別常見。

雖然我們將在[訓練馬拉松](https://huggingfacetb-smol-training-playbook.hf.space/#the-training-marathon)章節中更深入地研究不同類型的尖峰以及如何處理它們(深入研究浮點精度、優化器和學習率),但某些架構和訓練技術也可以幫助我們減少不穩定性,所以讓我們花點時間在這裡研究它們。我們將涵蓋最近大規模訓練執行(例如 Olmo2 ([OLMo et al., 2025](https://arxiv.org/abs/2501.00656)) 和 Qwen3 ([A. Yang, Li, et al., 2025](https://arxiv.org/abs/2505.09388)))中使用的幾種簡單技術來提高穩定性:Z-loss、從 embeddings 移除 weight decay,以及 QK-norm。

**Z-loss**

Z-loss ([Chowdhery et al., 2022](https://arxiv.org/abs/2204.02311)) 是一種正則化技術,透過向損失函數添加懲罰項來防止最終輸出 logits 變得太大。正則化鼓勵 logits 上的 softmax 的分母保持在合理範圍內,這有助於在訓練期間保持數值穩定性。

$$\mathcal{L}_{\text{z-loss}} = \lambda \cdot \log^2(Z)$$

_重置視圖_
_圖例_
HellaSwag | MMLU | ARC | PIQA | OpenBookQA | WinoGrande

下面我們 1B 模型的消融實驗結果顯示,添加 Z-loss 不會影響訓練損失或下游效能。對於 SmolLM3,我們最終沒有使用它,因為我們的 Z-loss 實作引入了一些訓練開銷,在我們開始訓練時沒有優化。

**從 embeddings 移除 weight decay**

Weight decay 通常作為正則化技術應用於所有模型參數,但 [OLMo et al. (2025)](https://arxiv.org/abs/2501.00656) 發現從 weight decay 中排除 embeddings 可以提高訓練穩定性。原因是 weight decay 導致 embedding norms 在訓練期間逐漸降低,這可能導致早期層中的梯度更大,因為 layer normalization 的 Jacobian 與輸入 norm 成反比 ([Takase et al., 2025](https://arxiv.org/abs/2312.16903))。

我們透過訓練三種配置來測試這種方法:我們的基準線使用標準 weight decay,一個在 embeddings 上沒有 weight decay 的變體,以及結合我們所有採用的變更(embeddings 上沒有 weight decay + NoPE + 文件遮罩)的第三種配置,以確保技術之間沒有負面互動。三種配置的損失曲線和評估結果幾乎相同。因此我們在 SmolLM3 訓練中採用了所有 3 項變更。

_重置視圖_
_圖例_
HellaSwag | MMLU | ARC | PIQA | OpenBookQA | WinoGrande

**QK-norm**

QK-norm ([Dehghani et al., 2023](https://arxiv.org/abs/2302.05442)) 在計算注意力之前對 query 和 key 向量都應用 layer normalization。這種技術有助於防止注意力 logits 變得太大,並被許多最近的模型用來提高穩定性。

然而,[B. Yang et al. (2025)](https://arxiv.org/abs/2501.18795) 發現 QK-norm 損害長上下文任務。他們的分析顯示,QK-norm 導致相關 token(needles)的注意力質量較低,無關上下文的注意力質量較高。他們認為這是因為正則化操作從 query-key 點積中移除了幅度資訊,這使得注意力 logits 在幅度方面更接近。由於這個原因,我們在 SmolLM3 中沒有使用 QK-norm。此外,作為一個小型 3B 參數模型,與 QK-norm 已被證明最有益的大型模型相比,它面臨的訓練不穩定性風險較小。

#### [其他核心組件](https://huggingfacetb-smol-training-playbook.hf.space/#other-core-components)

除了我們已經涵蓋的組件之外,還有其他一些值得注意的架構決策以求完整性。

為了初始化參數,現代模型通常使用截斷正態初始化(mean=0, std=0.02 或 std=0.006)或像 muP 這樣的初始化方案 ([G. Yang & Hu, 2022](https://arxiv.org/abs/2011.14522)),例如 Cohere 的 Command A ([Cohere et al., 2025](https://arxiv.org/abs/2504.00698))。這可能是另一個消融實驗主題。

在啟動函數方面,SwiGLU 已成為現代 LLM 中的事實標準(除了 Gemma2 使用 GeGLU 和 nvidia 使用 relu^2 ([Nvidia et al., 2024](https://arxiv.org/abs/2406.11704); [NVIDIA et al., 2025](https://arxiv.org/abs/2508.14444))),取代了像 ReLU 或 GELU 這樣的舊選擇。

在更廣泛的規模上,架構佈局選擇也在塑造模型行為方面發揮作用。儘管總參數數量在很大程度上決定了語言模型的容量,但這些參數如何在深度和寬度上分布也很重要。[Petty et al. (2024)](https://arxiv.org/abs/2310.19956) 發現,在語言建模和組合任務上,較深的模型優於同等大小的較寬模型,直到益處飽和。這種「深而窄」策略在 MobileLLM 消融實驗中對十億以下參數的 LLM 效果很好 ([Z. Liu et al., 2024](https://arxiv.org/abs/2402.14905)),而較寬的模型由於更大的平行性往往提供更快的推理。正如這篇[部落格文章](https://sebastianraschka.com/blog/2025/the-big-llm-architecture-comparison.html)所述,現代架構以不同方式反映了這種權衡。

我們現在已經涵蓋了值得為你的訓練執行優化的密集 transformer 架構的最重要方面。然而,最近出現了涉及整個模型的其他架構介入,即 MoE 和混合模型。讓我們看看它們能提供什麼,從 MoE 開始。

#### [走向稀疏:MoE](https://huggingfacetb-smol-training-playbook.hf.space/#going-sparse-moe)

_Mixture-of-Experts (MoEs)_ 的直覺是,我們不需要完整模型來進行每個 token 預測,類似於我們的大腦根據手頭的任務啟動不同區域(例如視覺或運動皮層)。對於 LLM,這可能意味著學習編碼語法的部分在模型執行翻譯任務時不需要使用。如果我們能做好這一點,這意味著我們可以節省大量計算,因為我們只需要在推理時執行完整模型的部分。

在技術層面上,MoE 有一個簡單的目標:增加總參數而不增加每個 token 的「活躍」參數數量。總參數在某種程度上簡化影響模型的總學習容量,而活躍參數決定訓練成本和推理速度。這就是為什麼你看到許多前沿系統(例如 DeepSeek V3、K2,以及像 Gemini、Grok 這樣的閉源模型...)這些天使用 MoE 架構。這張來自 Ling 1.5 論文的圖 ([L. Team et al., 2025](https://arxiv.org/abs/2503.05139)) 比較了 MoE 和密集模型的擴展定律:

![Image](https://huggingfacetb-smol-training-playbook.hf.space/_astro/image_2931384e-bcac-80c4-ab02-f22c53e6fdee.dhphF60f_Z1Pe0ji.webp)

如果這是你第一次接觸 MoE,別擔心,機制並不複雜。讓我們從標準密集架構開始,看看 MoE 的必要變更(圖來自 [Sebastian Raschka](https://github.com/rasbt/LLMs-from-scratch/tree/main/ch04/07_moe)):

![Image](https://huggingfacetb-smol-training-playbook.hf.space/_astro/image_2931384e-bcac-8062-bc11-d1ee3706d996.D6CeK-45_10lKF5.webp)

使用 MoE,我們將單個 MLP 替換為多個 MLP(「experts」)並在 MLP 之前添加一個可學習的 router。對於每個 token,router 選擇一小部分 experts 來執行。這就是總參數和活躍參數之間的區別來源:模型有許多 experts,但任何給定 token 只使用少數幾個。

設計 MoE 層引發了幾個核心問題:
  - Expert 形狀和稀疏性:你應該使用許多小 experts 還是較少的大 experts?每個 token 應該啟動多少個 experts,總共需要多少個 experts(即稀疏性或「top-k」)?某些 experts 應該是通用的並因此始終啟動嗎?
  - 利用率和專業化:如何選擇路由的 experts 並保持它們被充分使用(避免閒置容量)同時仍鼓勵它們專業化?實際上這是一個負載平衡問題,對訓練和推理效率有重大影響。

在這裡,我們專注於一個目標:在固定的計算預算下,我們如何選擇一個使損失最小化的 MoE 配置?這與純系統效率(吞吐量/延遲)是不同的問題,我們稍後會回到這一點。本節的大部分內容遵循 Ant Group 的 MoE 擴展定律論文 ([Tian et al., 2025](https://arxiv.org/abs/2507.17702)) 中的分析。

我們將使用他們的 _Efficiency Leverage (EL)_ 概念。簡單來說,EL 衡量你需要多少密集計算來匹配 MoE 設計達到的損失,其中測量單位是 FLOPs。更高的 EL 意味著與密集訓練相比,MoE 配置每單位計算提供更多損失改進。

_損失擴展曲線_
Ling-Dense | Ling-MoE

_EL (𝒳 MoE | 𝒳 Dense) = CDense / CMoE_
_Efficiency Leverage (EL) 的定義_

讓我們更仔細地看看如何設置 MoE 的稀疏性以提高效率槓桿。

**稀疏性 / 啟動比率**

> **TL;DR:** 更多稀疏性 → 更好的 FLOPs 效率 → 在非常高的稀疏性下收益遞減 → 最佳點取決於你的計算預算。

在本節中,我們想找出哪種 MoE 設置最好。漸近地,很容易看出兩個極端不是理想設置。一方面,始終啟動所有 experts 使我們回到密集設置,其中所有參數始終被使用。另一方面,如果活躍參數非常低(作為極端,想想只有 1 個參數是活躍的),顯然即使在狹窄領域也不足以解決任務。所以顯然我們需要找到一些中間地帶。在深入尋找最佳設置之前,定義兩個量是有用的:_**啟動比率**_ 及其倒數 _**稀疏性**_:

$$\text{activation ratio} \;=\; \frac{\#\text{activated experts}}{\#\text{total experts}}$$

$$\text{sparsity} \;=\; \frac{\#\text{total experts}}{\#\text{activated experts}} \;=\; \frac{1}{\text{activation ratio}}$$

從計算角度來看,成本僅由活躍參數驅動。如果你保持啟動的 experts 的數量(和大小)固定並增加 experts 的總數,你的推理/訓練 FLOPs 預算保持大致相同,但你正在添加模型容量,因此模型應該普遍更好,只要你訓練足夠長的時間。

如果你調查最近的 MoE 論文,有一些有趣的實證要點:保持活躍 experts 的數量和大小固定,增加 experts 的總數(即降低啟動比率/增加稀疏性)改善損失,一旦稀疏性變得非常高就會出現收益遞減。

兩個範例:
  - Kimi K2 圖 ([K. Team et al., 2025](https://arxiv.org/abs/2507.20534)):顯示兩種效果:更高的稀疏性提高效能,但隨著稀疏性增長,收益逐漸減少。
  - Ant Group 圖 ([Tian et al., 2025](https://arxiv.org/abs/2507.17702)):與 K2 結論相同,額外的結果是更高稀疏性的 MoE 從增加計算中受益更多。

![Image](https://huggingfacetb-smol-training-playbook.hf.space/_astro/Capture_decran_2025-10-20_a_13_25_47_2921384e-bcac-8087-83e5-fa7a40c1f342.asYkEXKU_1s8wtB.webp)

![Image](https://huggingfacetb-smol-training-playbook.hf.space/_astro/Capture_decran_2025-10-20_a_13_26_08_2921384e-bcac-80b5-ac36-fb73d6374208.D-BBIjb7_Zs7nQa.webp)

這裡是一些 MoE 模型的稀疏性表格:

| 模型 | 總 experts | 每個 token 啟動(包括共享) | 稀疏性 |
|---|---|---|---|
| Mixtral-8×7B | 8 | 2 | 4.0 |
| Grok-1 | 8 | 2 | 4.0 |
| Grok-2 | 8 | 2 | 4.0 |
| OLMoE-1B-7B-0924 | 64 | 8 | 8.0 |
| gpt-oss 20b | 32 | 4 | 8 |
| Step-3 | 48 routed + 1 shared = 49 | 3 routed + 1 shared = 4 | 12.25 |
| GLM-4.5-Air | 128 routed + 1 shared = 129 | 8 routed + 1 shared = 9 | 14.3 |
| Qwen3-30B-A3B | 128 | 8 | 16.0 |
| Qwen3-235B-A22B | 128 | 8 | 16.0 |
| GLM-4.5 | 160 routed + 1 shared = 161 | 8 routed + 1 shared = 9 | 17.8 |
| DeepSeek-V2 | 160 routed + 2 shared = 162 | 6 routed + 2 shared = 8 | 20.25 |
| DeepSeek-V3 | 256 routed + 1 shared = 257 | 8 routed + 1 shared = 9 | 28.6 |
| gpt-oss 120b | 128 | 4 | 32 |
| Kimi K2 | 384 routed + 1 shared = 385 | 8 routed + 1 shared = 9 | 42.8 |
| Qwen3-Next-80B-A3B-Instruct | 512 routed + 1 shared = 513 | 10 total active + 1 shared = 11 | 46.6 |

最近的趨勢很明顯:MoE 模型變得越來越稀疏。話雖如此,最佳稀疏性仍然取決於硬體和端到端效率。例如,Step-3 以峰值效率為目標,故意不將稀疏性最大化以適應其特定的硬體和頻寬約束,而 gpt-oss-20b 由於裝置記憶體約束而具有低稀疏性(被動 expert 仍然佔用一些記憶體)。

**粒度(Granularity)**

除了稀疏性,我們還需要決定每個 expert 應該有多大。這由粒度捕獲,這是 Ant Group 引入的一個指標。讓我們明確我們對這個術語的含義。不同論文的術語有所不同,有些使用稍微不同的公式。在這裡,我們將使用與我們參考的圖匹配的定義:

$$G = \frac{\alpha*d_{model}}{d_{expert}} \text{ with } \alpha = 2 \text{ or } 4$$

更高的粒度值對應於具有更小維度的更多 experts(給定固定的參數數量)。這個指標是 expert 維度($d_{expert}$)和模型維度($d_{model}$)之間的比率。

在密集模型中,一個常見的經驗法則是將 MLP 的維度設置為 $d_{intermediate} = 4 * d_{model}$。如果 $\alpha = 4$(像 [Krajewski et al. (2024)](https://arxiv.org/abs/2402.07871))。你可以粗略地將粒度視為**需要多少個 experts 來匹配密集 MLP 寬度**($4\, d_{\text{model}} = d_{\text{intermediate}} = G\, d_{\text{expert}}$)。

這種解釋只是一個粗略的啟發式:現代 MoE 設計通常分配比單個密集 MLP 大得多的總容量,因此一對一匹配在實踐中會失效。Ant 團隊設置選擇 $\alpha = 2$,這只是一個不同的正則化選擇。為了一致性,我們將選擇這個約定並堅持它。

因為 G 隨 $d_{\text{model}}$ 縮放,當模型寬度不同時,跨模型比較可能很棘手。

這裡仍然是一個包含一些 MoE 發布的不同值的表格:

| 模型 | $d_\text{model}$ | $d_\text{expert}$ | $G = 2 d_\text{model} / d_\text{expert}$ | 年份 |
|---|---|---|---|---|
| Mixtral-8×7B | 4,096 | 14,336 | 0.571 | 2023 |
| gpt-oss-120b | 2880 | 2880 | 0.5 | 2025 |
| gpt-oss-20b | 2880 | 2880 | 0.5 | 2025 |
| Grok 2 | 8,192 | 16,384 | 1.0 | 2024 |
| StepFun Step-3 | 7,168 | 5,120 | 2.8 | 2025 |
| OLMoE-1B-7B | 2,048 | 1,024 | 4.0 | 2025 |
| Qwen3-30B-A3B | 2,048 | 768 | 5.3 | 2025 |
| Qwen3-235B-A22B | 4,096 | 1,536 | 5.3 | 2025 |
| GLM-4.5-Air | 4,096 | 1,408 | 5.8 | 2025 |
| DeepSeek V2 | 5,120 | 1,536 | 6.6 | 2024 |
| GLM-4.5 | 5,120 | 1,536 | 6.6 | 2025 |
| Kimi K2 | 7,168 | 2,048 | 7.0 | 2025 |
| DeepSeek V3 | 7168 | 2048 | 7.0 | 2024 |
| Qwen3-Next-80B-A3B | 2048 | 512 | 8.0 | 2025 |

讓我們談談粒度如何塑造行為(來自 [Ant Group 的論文](https://arxiv.org/pdf/2507.17702)):

![Image](https://huggingfacetb-smol-training-playbook.hf.space/_astro/75ae60ff-50be-48e1-aad2-a8fc56120d3d_2921384e-bcac-80c2-984b-d81404e4bb7c.8nxVv-OC_25NHa.webp)

粒度看起來不像是 EL 的主要驅動因素 — 它有幫助,特別是超過 2,但它不是決定損失的主導因素。不過有一個最佳點:將粒度推高有助於達到一定程度,然後收益趨於平緩。因此粒度是一個有用的調整旋鈕,在最近的發布中有明確的趨勢走向更高的值,但它不應該單獨優化。

另一種廣泛用於改善 MoE 的方法是共享 experts 的概念。讓我們看看!

**共享 experts**

共享 expert 設置將每個 token 路由到一小組始終啟動的 experts。這些共享 experts 吸收資料中的基本、重複模式,因此其餘 experts 可以更積極地專業化。實際上,你通常不需要很多;模型設計師通常選擇一個,最多兩個。隨著粒度增加(例如從 Qwen3 風格設置轉向更接近 Qwen3-Next 的東西),共享 experts 往往變得更有用。查看以下圖,整體影響是適度的,它不會顯著改變 EL。一個簡單的經驗法則在大多數情況下效果很好:只使用一個共享 expert,這與 DeepSeek V3、K2 和 Qwen3-Next 等模型的選擇相匹配,並且往往在不增加不必要的複雜性的情況下最大化效率。圖來自 [Tian et al. (2025)](https://arxiv.org/abs/2507.17702)。

![Image](https://huggingfacetb-smol-training-playbook.hf.space/_astro/Capture_decran_2025-10-21_a_11_11_38_2931384e-bcac-8008-ad8d-d5ab0c539d3a.ZeQ4Aq4n_Z3EACl.webp)

所以共享 expert 是一個總是路由某些 token 的 expert。其他 experts 呢?我們如何學習何時路由到每個 expert 並確保我們不只是使用少數 experts?接下來我們將討論負載平衡,它正好解決這個問題。

**負載平衡**

負載平衡是 MoE 中的關鍵部分。如果設置不當,它可以破壞所有其他設計選擇。我們可以從以下範例看到為什麼糟糕的負載平衡會給我們帶來很多痛苦。考慮一個非常簡單的分散式訓練設置,我們有 4 個 GPU,我們將模型的 4 個 experts 均勻分配到 GPU 上。如果路由崩潰,所有 token 都路由到 expert 1,這意味著只有 1/4 的 GPU 被利用,這對訓練和推理效率非常不利。除此之外,這意味著我們模型的有效學習容量也降低了,因為並非所有 experts 都被啟動。

為了解決這個問題,我們可以向 router 添加一個額外的損失項。下面你可以看到標準的基於輔助損失的負載平衡(LBL):

$$\mathcal{L}_{\text{Bal}} \;=\; \alpha \sum_{i=1}^{N_{r}} f_{i}\, P_{i}$$

這個簡單的公式只使用三個因素:係數 $\alpha$ 決定損失的強度,$f_i$ 是流量分數,即通過 expert $i$ 的 token 分數,最後 $P_i$ 是機率質量,簡單地總結通過 expert 的 token 的機率。它們都是必要的,$f_i$ 對應實際平衡,而 $P_i$ 是平滑和可微分的,允許梯度流動。如果我們實現完美的負載平衡,我們得到 $f_i=P_i=1/N_r$,但是我們需要小心如何調整 $\alpha$,因為值太小我們不夠引導路由,如果太大路由均勻性變得比主要語言模型損失更重要。

**無損失的負載平衡**

也可以在沒有明確損失項的情況下實現平衡。DeepSeek v3 ([DeepSeek-AI et al., 2025](https://arxiv.org/abs/2412.19437)) 引入了一個簡單的偏差項,添加到進入路由 softmax 的親和力分數。如果一個 router 過載,他們將分數降低一點(一個常數因子 $\gamma$),從而使其不太可能被選中,如果 expert 利用不足則增加 $\gamma$。透過這個簡單的自適應規則,他們也實現了負載平衡。

一個關鍵細節是你計算路由統計資訊的範圍:$f_i$ 和 $P_i$ 是按本地批次(每個 worker 的 mini-batch)計算還是全域計算(跨 workers/devices 聚合)?Qwen 團隊的分析 ([Qiu et al., 2025](https://arxiv.org/abs/2501.11873)) 顯示,當每個本地批次中沒有足夠的 token 多樣性時,本地計算可能會損害 expert 專業化(路由健康的良好代理)和整體模型效能。Expert 專業化是一種現象,其中一個或多個 experts 在特定領域中比其他 experts 更頻繁地被啟動。換句話說,如果本地批次很窄,其路由統計資訊變得嘈雜/有偏差,並且不會導致良好的平衡。這意味著我們應該盡可能使用全域統計資訊(或至少跨裝置聚合)。值得注意的是,在該論文發表時,許多框架 — 包括 Megatron — 預設在本地計算這些統計資訊。

Qwen 論文中的以下圖說明了 micro-batch 與 global batch 聚合的差異及其對效能和專業化的影響:

![Image](https://huggingfacetb-smol-training-playbook.hf.space/_astro/Capture_decran_2025-10-21_a_15_34_27_2931384e-bcac-8066-834b-c485ae8d1fa5.Cix986wE_ZLIDik.webp)

一般來說,圍繞 MoE 消融架構選擇很棘手,因為存在許多方面的相互作用。例如共享 expert 的有用性可能取決於模型的粒度。因此值得花一些時間確保你有一套好的實驗,以真正獲得你正在尋找的洞見!

我們現在已經涵蓋了 MoE 的基礎知識,但還有更多需要發現。一個非詳盡的進一步研究項目清單:
  - Zero-computation experts、MoE 層重新縮放和訓練監控(LongCat-Flash 論文)。
  - 正交損失負載平衡(如 ERNIE 4.5 中)。
  - 在訓練過程中排程負載平衡係數。
  - MoE 的架構/優化互動,例如:
    - 優化器排名是否對 MoE 有變化。
    - 如何將 MuP 應用於 MoE。
    - 如何調整 MoE 的學習率(因為它們每批次看不到相同數量的 token)。
  - 開始時的密集層數量。
  - 更多...

我們把它留給你,熱切的讀者,進一步深入兔子洞,而我們現在繼續最後一個主要架構選擇:混合模型!

#### [遊覽:混合模型](https://huggingfacetb-smol-training-playbook.hf.space/#excursion-hybrid-models)

最近的趨勢是用狀態空間模型(SSM)或線性注意力機制增強標準密集或 MoE 架構 ([MiniMax et al., 2025](https://arxiv.org/abs/2501.08313); [Zuo et al., 2025](https://arxiv.org/abs/2507.22448))。這些新類別的模型試圖解決 transformer 的一些根本弱點:有效處理非常長的上下文。它們在遞迴模型和 transformer 之間取中間立場,遞迴模型可以有效地處理任意長度的上下文並線性縮放,但可能難以利用上下文中的資訊,而 transformer 在長上下文中變得非常昂貴,但可以很好地利用上下文中的模式。

已經有一些研究 ([Waleffe et al., 2024](https://arxiv.org/abs/2406.07887)) 來理解例如 Mamba 模型(SSM 的一種形式)的弱點,發現此類模型在許多基準測試上表現良好,但例如在 MMLU 上表現不佳,並假設是缺乏上下文學習導致了差距。這就是為什麼它們與密集或 MoE 模型的塊結合以獲得兩全其美,因此得名混合模型。

這些線性注意力方法背後的核心思想是重新排序計算,使注意力不再花費 O(n^2d),這在長上下文中變得難以處理。這是如何工作的?首先,回憶推理時的注意力公式。為 token t 產生輸出看起來像:

$$\mathbf{o}_{t} \;=\; \sum_{j=1}^{t} \frac{\exp\\!\big(\mathbf{q}_{t}^{\top}\mathbf{k}_{j}\big) \mathbf{v}_{j}}{\sum_{l=1}^{t} \exp\\!\big(\mathbf{q}_{t}^{\top}\mathbf{k}_{l}\big)}$$

現在去掉 softmax:

$$o_t = \sum_{j=1}^{t} (q_t^\top k_j)\, v_j$$

重新排序給出:

$$\quad\sum_{j=1}^{t}(q_t^\top k_j)\,v_j = \Big(\sum_{j=1}^{t} v_j k_j^\top\Big) q_t.$$

定義運行狀態:

$$S_t \triangleq \sum_{j=1}^{t} k_j v_j^\top = K_{1:t}^\top V_{1:t} \in \mathbb{R}^{d\times d}$$

使用簡單的更新:

$$S_t = S_{t-1} + k_t v_t^\top$$

所以我們可以寫:

$$o_t = S_t q_t = S_{t-1} q_t + v_t (k_t^\top q_t)$$

為什麼重新排序很重要:左形式 $\sum_{j\le t}(q_t^\top k_j)v_j$ 意味著「對於每個過去的 token j,取一個點積 $q_t^\top k_j$(一個標量),用它來縮放 $v_j$,並將這些 t 個向量加起來」 — 這在步驟 t 大約是 O(td) 的工作。右形式將其重寫為 $\big(\sum_{j\le t} v_j k_j^\top\big) q_t$:你保留一個單一的運行狀態矩陣 $S_t=\sum_{j\le t} v_j k_j^\top\in\mathbb{R}^{d\times d}$,它已經總結了所有過去的 $(k_j,v_j)$。每個新 token 用一個外積 $v_t k_t^\top$ 更新它,成本 O(d^2),然後輸出只是一個矩陣-向量乘法 $S_t q_t$(另一個 O(d^2))。因此從頭開始透過左形式生成 T 個 token 是 O(T^2 d),而維護 $S_t$ 並使用右形式是 O(T d^2)。直覺上:左 = 「每步許多小的點-縮放-加法」;右 = 「一個預先總結的矩陣乘以 query」,用對維度的依賴來交換對序列長度的依賴。我們這裡關注推理和遞迴形式,但它在訓練中也更高效,重新排序就像以下方程一樣簡單:

$$\underset{n\times n}{(QK^\top)}\,V \;=\; Q\,\underset{d\times d}{(K^\top V)}$$

所以我們可以看到這現在看起來非常類似於 RNN 類結構。這解決了我們的問題,對吧?幾乎。實際上,softmax 發揮著重要的穩定作用,沒有某種正則化,天真的線性形式可能不穩定。這激發了一個實用的變體,稱為 lightning 或 norm attention!

**Lightning 和 norm attention**

這個家族出現在 Minimax01 ([MiniMax et al., 2025](https://arxiv.org/abs/2501.08313)) 和最近的 Ring-linear ([L. Team, Han, et al., 2025](https://arxiv.org/abs/2510.19338)) 中。基於 Norm Attention 想法 ([Qin et al., 2022](https://arxiv.org/abs/2210.10340))。關鍵步驟很簡單:**正則化輸出**。「Lightning」變體專注於使實作快速和高效,並使公式有點不同。這是兩者的公式:

NormAttention:
$$\text{RMSNorm}(Q(K^TV))$$

LightningAttention:
$$Q= \text{Silu(Q)}, \; K = \text{Silu(K)}, \; V = \text{Silu(V)}$$
$$O = \text{SRMSNorm}(Q(KV^T))$$

根據 Minimax01,混合模型與 Norm attention 在大多數任務上與 softmax 匹配。

_注意力方法_
Softmax Attention | Lightning Attention | Hybrid-lightning

_410M 基準測試效能_
_PIQA | H5 | WG | ARC-E | ARC-C | OBQA | CSR-AVG | NIAH | SCR_

_1B 基準測試效能_
_PIQA | H5 | WG | ARC-E | ARC-C | OBQA | CSR-AVG | NIAH | SCR_

_3B 基準測試效能_
_PIQA | H5 | WG | ARC-E | ARC-C | OBQA | CSR-AVG | NIAH | SCR_

_7B 基準測試效能_
_PIQA | H5 | WG | ARC-E | ARC-C | OBQA | CSR-AVG | NIAH | SCR_

這裡有趣的是,在像 Needle in a Haystack (NIAH) 這樣的檢索任務上,它可以比完整 softmax attention 做得好得多,這似乎令人驚訝,但可能表明當 softmax 和線性層一起工作時存在一些協同作用!

**MiniMax M2**

令人驚訝的是,最近發布的 MiniMax M2 沒有使用混合或線性注意力。根據他們的[預訓練負責人](https://x.com/zpysky1125/status/1983383094607347992),雖然他們早期的 MiniMax M1 使用 Lightning Attention 的實驗在較小規模上在當時流行的基準測試(MMLU、BBH、MATH)上看起來很有希望,但他們發現它在更大規模上在「複雜、多跳推理任務」中有「明顯的缺陷」。他們還引用了 RL 訓練期間的數值精度問題和基礎設施成熟度作為關鍵阻礙因素。他們得出結論,在規模上製作架構是一個多變數問題,由於對其他參數(如資料分布、優化器...)的敏感性,很難且計算密集。

然而,他們承認「隨著 GPU 計算增長放緩而資料長度持續增加,線性和稀疏注意力的好處將逐漸出現」。這突顯了架構消融實驗的複雜性以及研究與生產現實之間的差距。

現在讓我們看看其中一些方法以及如何用統一框架理解它們。

**進階線性注意力**

從遞迴模型中獲得的一個有用教訓是讓狀態偶爾放開過去。實際上,這意味著為先前狀態引入一個 gate $\mathbf{G}_t$:

$$\mathbf{S}_t \;=\; \mathbf{G}_t \odot \mathbf{S}_{t-1} \;+\; \mathbf{v}_t \mathbf{k}_t^{\top}$$

幾乎所有最近的線性注意力方法都有這個 gating 組件,只是 $\mathbf{G}_t$ 的實作不同。這是來自[這篇論文](https://arxiv.org/abs/2312.06635)的 gate 的不同變體和相應架構的清單:

| 模型 | 參數化 | 可學習參數 |
|---|---|---|
| Mamba ([A. Gu & Dao, 2024](https://arxiv.org/abs/2312.00752)) | $\mathbf{G}_t = \exp(-(\mathbf{1}^\top \boldsymbol{\alpha}_t) \odot \exp(\mathbf{A})), \quad \boldsymbol{\alpha}t = \text{softplus}(\mathbf{x}t \mathbf{W}{\alpha_1} \mathbf{W}{\alpha_2})$ | $\mathbf{A} \in \mathbb{R}^{d_k \times d_v}, \quad \mathbf{W}{\alpha_1} \in \mathbb{R}^{d \times \frac{d}{16}}, \quad \mathbf{W}{\alpha_2} \in \mathbb{R}^{\frac{d}{16} \times d_v}$ |
| Mamba-2 ([Dao & Gu, 2024](https://arxiv.org/abs/2405.21060)) | $\mathbf{G}_t = \gamma_t \mathbf{1}^\top \mathbf{1}, \quad \gamma_t = \exp(-\text{softplus}(\mathbf{x}t \mathbf{W}{\gamma})\exp(a))$ | $\mathbf{W}_{\gamma} \in \mathbb{R}^{d \times 1}, \quad a \in \mathbb{R}$ |
| mLSTM ([Beck et al., 2025](https://arxiv.org/abs/2503.14376); H. [Peng et al., 2021](https://arxiv.org/abs/2103.02143)) | $\mathbf{G}_t = \gamma_t \mathbf{1}^\top \mathbf{1}, \quad \gamma_t = \sigma(\mathbf{x}t \mathbf{W}{\gamma})$ | $\mathbf{W}_{\gamma} \in \mathbb{R}^{d \times 1}$ |
| Gated Retention ([Sun et al., 2024](https://arxiv.org/abs/2405.05254)) | $\mathbf{G}_t = \gamma_t \mathbf{1}^\top \mathbf{1}, \quad \gamma_t = \sigma(\mathbf{x}t \mathbf{W}{\gamma})^{\frac{1}{\tau}}$ | $\mathbf{W}_{\gamma} \in \mathbb{R}^{d \times 1}$ |
| DFW (Mao, 2022; Pramanik et al., 2023) ([Mao, 2022](https://arxiv.org/abs/2210.04243)) | $\mathbf{G}_t = \boldsymbol{\alpha}_t^\top \boldsymbol{\beta}_t, \quad \boldsymbol{\alpha}_t = \sigma(\mathbf{x}t \mathbf{W}{\alpha}), \quad \boldsymbol{\beta}_t = \sigma(\mathbf{x}t \mathbf{W}{\beta})$ | $\mathbf{W}{\alpha} \in \mathbb{R}^{d \times d_k}, \quad \mathbf{W}{\beta} \in \mathbb{R}^{d \times d_v}$ |
| GateLoop ([Katsch, 2024](https://arxiv.org/abs/2311.01927)) | $\mathbf{G}_t = \boldsymbol{\alpha}_t^\top \mathbf{1}, \quad \boldsymbol{\alpha}_t = \sigma(\mathbf{x}t \mathbf{W}{\alpha_1})\exp(\mathbf{x}t \mathbf{W}{\alpha_2} \mathrm{i})$ | $\mathbf{W}{\alpha_1} \in \mathbb{R}^{d \times d_k}, \quad \mathbf{W}{\alpha_2} \in \mathbb{R}^{d \times d_k}$ |
| HGRN-2 ([Qin et al., 2024](https://arxiv.org/abs/2404.07904)) | $\mathbf{G}_t = \boldsymbol{\alpha}_t^\top \mathbf{1}, \quad \boldsymbol{\alpha}_t = \gamma + (1-\gamma)\sigma(\mathbf{x}t \mathbf{W}{\alpha})$ | $\mathbf{W}_{\alpha} \in \mathbb{R}^{d \times d_k}, \quad \gamma \in (0,1)^{d_k}$ |
| RWKV-6 ([B. Peng et al., 2024](https://arxiv.org/abs/2404.05892)) | $\mathbf{G}_t = \boldsymbol{\alpha}_t^\top \mathbf{1}, \quad \boldsymbol{\alpha}_t = \exp(-\exp(\mathbf{x}t \mathbf{W}{\alpha}))$ | $\mathbf{W}_{\alpha} \in \mathbb{R}^{d \times d_k}$ |
| Gated Linear Attention (GLA) | $\mathbf{G}_t = \boldsymbol{\alpha}_t^\top \mathbf{1}, \quad \boldsymbol{\alpha}t = \sigma(\mathbf{x}t \mathbf{W}{\alpha_1} \mathbf{W}{\alpha_2})^{\frac{1}{\tau}}$ | $\mathbf{W}{\alpha_1} \in \mathbb{R}^{d \times 16}, \quad \mathbf{W}{\alpha_2} \in \mathbb{R}^{16 \times d_k}$ |

_最近模型的 Gated linear attention 公式,它們在 $\mathbf{G}_t$ 的參數化上有所不同。偏差項被省略。_

清單上的一個值得注意的變體是 Mamba-2 ([Dao & Gu, 2024](https://arxiv.org/abs/2405.21060))。它用於許多混合模型,如 Nemotron-H ([NVIDIA, :, Blakeman, et al., 2025](https://arxiv.org/abs/2504.03624))、Falcon H1 ([Zuo et al., 2025](https://arxiv.org/abs/2507.22448)) 和 Granite-4.0-h ([IBM Research, 2025](https://www.ibm.com/new/announcements/ibm-granite-4-0-hyper-efficient-high-performance-hybrid-models))。

然而,這仍然是早期階段,在擴展到大型混合模型時需要考慮重要的細微差別。雖然它們顯示出希望,但 MiniMax 對 [M2](https://x.com/zpysky1125/status/1983383094607347992) 的經驗突顯了小規模的好處並不總是轉化為大規模生產系統,特別是對於複雜推理任務、RL 訓練穩定性和基礎設施成熟度。話雖如此,混合模型正在快速發展,並且仍然是前沿訓練的可靠選擇。Qwen3-Next(使用 gated DeltaNet 更新)([Qwen Team, 2025](https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&from=research.latest-advancements-list)) 報告說他們在長上下文推理時更快,訓練更快,在常規基準測試上更強。我們也期待 Kimi 的下一個模型,它很可能會使用他們新的 [「Kimi Delta Attention」](https://github.com/fla-org/flash-linear-attention/pull/621)。讓我們也提一下 Sparse Attention,它透過選擇要計算注意力的塊或 query 來解決與線性注意力相同的長上下文問題。一些範例是 Native Sparse Attention ([Yuan et al., 2025](https://arxiv.org/abs/2502.11089))、DeepSeek Sparse Attention ([DeepSeek-AI, 2025](https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf)) 和 InfLLM v2 ([M. Team, Xiao, et al., 2025](https://arxiv.org/abs/2506.07900))。

在移動到 tokenizer 之前,我們將透過建立一個小決策樹來確定是訓練密集、MoE 還是混合模型來結束架構選擇。

#### [**MoE 或不 MoE:選擇基礎架構**](https://huggingfacetb-smol-training-playbook.hf.space/#to-moe-or-not-moe-choosing-a-base-architecture)

我們現在已經看到了密集、MoE 和混合模型,所以你可能自然會好奇知道你應該使用哪一個?你的架構選擇通常取決於你將在哪裡部署模型、你團隊的專業知識以及你的時間表。讓我們簡要回顧每種架構的優缺點,並想出一個簡單的指導過程來為你找到正確的架構。

**密集 transformers** 是基礎標準的僅解碼器 transformer,其中每個參數對每個 token 都啟動。請參見 [The Annotated Transformers](https://nlp.seas.harvard.edu/2018/04/03/attention.html) 獲取數學和 [The Illustrated Transformers](https://jalammar.github.io/illustrated-transformer/) 建立你的直覺。

優點:廣泛支援,充分理解,穩定訓練,每參數效能良好。
缺點:計算隨大小線性縮放,70B 模型的成本約為 3B 的 23 倍。

這通常是記憶體受限用例或新 LLM 訓練者的預設選擇。

**Mixture of Experts (MoE)** 將 transformer 中的前饋層替換為多個「experts」。對於每個 token,一個 gating 網路僅將其路由到少數幾個 experts。結果是以一小部分計算獲得大型網路的容量。例如 [Kimi K2](https://huggingface.co/moonshotai/Kimi-K2-Instruct) 有 1T 總參數,但每個 token 只有 32B 啟動。問題是所有 experts 必須載入記憶體。有關視覺指南和提醒,請查看[此部落格](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts)。

優點:訓練和推理的每計算效能更好。
缺點:高記憶體(必須載入所有 experts)。比密集 transformer 更複雜的訓練。框架支援正在改善,但不如密集模型成熟。分散式訓練是一場噩夢,涉及 expert 放置、負載平衡和 all-to-all 通訊挑戰。

當你不受記憶體限制並希望每計算獲得最大效能時使用。

**混合模型** 將 transformers 與狀態空間模型(SSM)如 Mamba 結合,為某些操作提供線性複雜度,而 attention 的二次縮放。([Mathy blog](https://srush.github.io/annotated-mamba/hard.html) | [Visual guide](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state))

優點:潛在更好的長上下文處理。對於非常長的序列更高效。
缺點:不如密集和 MoE 成熟,已驗證的訓練配方較少。框架支援有限。

如果你想擴展到非常大的上下文同時減少標準 transformer 的推理開銷,請使用。

我們現在還看到一些團隊探索用於文字的 diffusion 模型,但這些模型(和其他實驗性替代方案)處於非常早期階段,我們認為它們超出了本文件的範圍。

因此總結一下,首先問你的模型將在哪裡部署。然後考慮你團隊的專業知識和你的訓練時間表,以評估你能負擔多少探索:

_決策樹圖示_
_這個模型將在哪裡執行? → Edge/手機 → 記憶體受限環境 → 密集(大多數情況)_
_其他 → 更多記憶體可用 → 混合或其他(對於有經驗的團隊)_
_你團隊的專業知識? → 第一次 LLM 訓練 → 密集(專注於基礎)_
_有經驗 → 熟悉密集 → 密集_
_非常有經驗 → 你的時間表? → 緊張 → 需要已驗證的路徑 → 密集_
_靈活 → 願意探索 → MoE 或 MoE + 混合:更好的效能/計算_

對於 SmolLM3,我們想建立一個用於裝置部署的強大小型模型,我們有大約 3 個月的時間表,並且過去主要訓練密集模型。這排除了 MoE(記憶體限制)和混合(短時間表來探索新架構,密集模型可以獲得我們的目標長上下文,最大 128k tokens),所以我們選擇了類似 llama 的密集模型。

現在我們已經研究了模型架構的內部,讓我們看看 tokenizer,它在資料和我們的模型之間形成橋樑。

#### [Tokenizer](https://huggingfacetb-smol-training-playbook.hf.space/#the-tokenizer)

雖然它很少從架構創新中搶走風頭,但 tokenization 方案很可能是任何語言模型中最被低估的組件之一。將它想像成人類語言和我們模型生活的數學世界之間的翻譯器,就像任何翻譯器一樣,翻譯的品質非常重要。那麼我們如何建立或選擇適合我們需求的正確 tokenizer?

**Tokenizer 基礎**

在核心,tokenizer 將原始文字轉換為我們模型可以處理的數字序列,透過將運行文字分段為稱為 tokens 的單獨可處理單元。在深入技術細節之前,我們應該首先回答一些將指導我們 tokenizer 設計的基本問題:
  - **我們想支援哪些語言?** 如果我們正在建立一個多語言模型,但我們的 tokenizer 只看過英語,那麼模型在遇到非英語文字時將效率低下,這會被分割成比必要多得多的 tokens。這直接影響效能、訓練成本和推理速度。
  - **哪些領域對我們很重要?** 除了語言,像數學和程式碼這樣的領域需要仔細表示數字。
  - **我們知道我們的目標資料混合嗎?** 如果我們計劃從頭開始訓練我們的 tokenizer,理想情況下,我們應該在反映我們最終訓練混合的樣本上訓練它。

一旦我們回答了這些問題,我們就可以檢查主要設計決策:

有關 tokenization 基礎的深入探討,Andrej Karpathy 的 [「Let's build the GPT Tokenizer」](https://www.youtube.com/watch?v=zduSFxRajkE) 是一個出色的實踐教程。你還可以查看這個[資源](https://github.com/huggingface/evaluation-guidebook/blob/main/contents/general-knowledge/tokenization.md),它提供了 tokenizers 的介紹和許多外部資源。

**詞彙表大小**

詞彙表本質上是一個字典,列出我們模型識別的所有 tokens(最小文字單元,如單詞、子詞或符號)。

更大的詞彙表更有效地壓縮文字,因為我們每句話生成更少的 tokens,但存在計算權衡。詞彙表大小直接影響我們 embedding 矩陣的大小。如果我們有詞彙表大小 V 和隱藏維度 h,輸入 embeddings 有 V × h 參數,輸出層有另外 V × h 參數。對於較小的模型,這成為總參數的重要部分,正如我們在「Embedding Sharing」部分中看到的,但相對成本隨著模型擴大而縮小。

最佳點取決於我們的目標覆蓋範圍和模型大小。對於僅英語模型,大約 50k tokens 通常足夠,但多語言模型通常需要 100k+ 來有效處理多樣化的書寫系統和語言。像 Llama3 這樣的現代最先進模型已經採用了 128k+ 範圍的詞彙表,以提高跨多種語言的 token 效率。同一家族中的較小模型應用 embedding sharing 來減少 embedding 參數的百分比,同時仍受益於更大的詞彙表。[Dagan et al. (2024)](https://arxiv.org/abs/2402.01035) 分析了詞彙表大小對壓縮、推理和記憶體的影響。他們觀察到來自更大詞彙表的壓縮收益呈指數遞減,表明存在最佳大小。對於推理,更大的模型受益於更大的詞彙表,因為壓縮在前向傳遞上節省的比 softmax 中額外 embedding tokens 的成本更多。對於記憶體,最佳大小取決於序列長度和批次大小:更長的上下文和大批次由於從擁有更少 tokens 的 KV cache 節省而受益於更大的詞彙表。

將詞彙表大小設置為 128 的倍數(例如 50,304 而不是 50,000)以優化吞吐量。現代 GPU 在維度可以被 2 的更高次方整除時執行矩陣運算效果更好,因為這確保了高效的記憶體對齊並減少了未對齊記憶體存取的開銷。更多細節在這個[部落格](https://www.thonking.ai/p/what-shapes-do-matrix-multiplications)。

**Tokenization 演算法**

BPE (Byte-Pair Encoding) ([Sennrich et al., 2016](https://arxiv.org/abs/1508.07909)) 仍然是最受歡迎的選擇,其他演算法如 WordPiece 或 SentencePiece 存在但較少採用。對於直接在位元組或字元上工作的無 tokenizer 方法的研究興趣也在增長,可能完全消除 tokenization。

現在我們已經看到了定義 tokenizer 的關鍵參數,我們面臨一個實際決定:我們應該使用現有 tokenizer 還是從頭開始訓練?答案取決於覆蓋範圍:具有我們目標詞彙表大小的現有 tokenizers 是否很好地處理我們的語言和領域。

下圖比較了 GPT-2 的僅英語 tokenizer ([Radford et al., 2019](https://huggingfacetb-smol-training-playbook.hf.space/#bib-gpt2)) 和 Gemma 3 的多語言 tokenizer ([G. Team, Kamath, et al., 2025](https://arxiv.org/abs/2503.19786)) 如何分段相同的英語和阿拉伯語句子。

_GPT-2_
_English_
_Text: The development of large language models has revolutionized artificial intelligence research and applications across multiple domains especially education._
_Tokens: 20_
_The | development | of | large | language | models | has | revolution | ized | artificial | intelligence | research..._

_Arabic_
_Text:_ [阿拉伯文文字範例]
_Tokens: 122_

_Gemma 3_
_English_
_Text: The development of large language models has revolutionized artificial intelligence research and applications across multiple domains especially education._
_Tokens: 19_
_The | development | of | large | language | models | has | revolutionized | artificial | intelligence | research | and | applications | across | multiple | domains | especially | education | ._

_Arabic_
_Text:_ [阿拉伯文文字範例]
_Tokens: 44_

雖然兩個 tokenizers 在英語上似乎表現相似,但阿拉伯語的差異變得驚人:GPT2 將文字分解為一百多個片段,而 Gemma3 由於其多語言訓練資料和更大、更包容的詞彙表產生的 tokens 要少得多。

但是要衡量 tokenizer 的品質,我們不能只是看幾個 tokenization 範例然後說它很好,就像我們不能在沒有執行消融實驗的情況下根據直覺進行架構變更一樣。我們需要具體的指標來評估 tokenizer 品質。

**衡量 Tokenizer 品質**

為了評估 tokenizer 的表現如何,我們可以使用 FineWeb2 ([Penedo et al., 2025](https://arxiv.org/abs/2506.20920)) 中使用的兩個關鍵指標。

**Fertility(生育率):**

它衡量編碼一個單詞所需的平均 token 數量。較低的 fertility 意味著更好的壓縮,這轉化為更快的訓練和推理。這樣想:如果一個 tokenizer 需要一個或兩個以上的 tokens 來編碼大多數單詞,而另一個用更少的 tokens 完成,第二個顯然更有效。

衡量 fertility 的標準方法是計算**單詞-token 比率**(word fertility),它衡量平均每個單詞需要多少 tokens。這個指標是圍繞單詞的概念定義的,因為當有適當的單詞 tokenizers 可用時,它提供了有意義的跨語言比較,例如在 [Spacy](https://spacy.io/) 和 [Stanza](https://stanfordnlp.github.io/stanza) 中 ([Penedo et al., 2025](https://arxiv.org/abs/2506.20920))。

在為單一語言比較 tokenizers 時,你也可以使用字元或位元組的數量而不是單詞來獲得字元-token 比率或位元組-token 比率 ([Dagan et al., 2024](https://arxiv.org/abs/2402.01035))。然而,這些指標對於跨語言比較有限制。位元組可能會傾斜,因為不同書寫系統中的字元需要不同的位元組表示(例如中文字元在 UTF-8 中使用三個位元組,而拉丁字元使用一到兩個)。類似地,使用字元數量不能解釋單詞在不同語言中長度差異巨大的事實。例如,中文單詞往往比德語複合詞短得多。

**連續單詞的比例:**

這個指標告訴我們有多少百分比的單詞被分割成多個片段。較低的百分比更好,因為這意味著更少的單詞被分段,導致更高效的 tokenization。

讓我們實作這些指標:

```python
import numpy as np

def compute_tokenizer_metrics(tokenizer, word_tokenizer, text):
    """
    Computes fertility and proportion of continued words.
    Returns:
        tuple: (fertility, proportion_continued_words)
            - fertility: average tokens per word (lower is better)
            - proportion_continued_words: percentage of words split into 2+ tokens (lower is better)
    """
    words = word_tokenizer.word_tokenize(text)
    tokens = tokenizer.batch_encode_plus(words, add_special_tokens=False)
    tokens_per_word = np.array(list(map(len, tokens["input_ids"])))
    fertility = np.mean(tokens_per_word).item()
    proportion_continued_words = (tokens_per_word >= 2).sum() / len(tokens_per_word)
    return fertility, proportion_continued_words
```

對於程式碼和數學等專業領域,除了 fertility,我們需要更深入地研究 tokenizer 如何處理特定領域的模式。大多數現代 tokenizers 進行單位數分割(因此「123」變成 ["1", "2", "3"]) ([Chowdhery et al., 2022](https://arxiv.org/abs/2204.02311); [DeepSeek-AI et al., 2024](https://arxiv.org/abs/2405.04434))。將數字拆開似乎違反直覺,但實際上它幫助模型更有效地學習算術模式。如果「342792」被編碼為一個不可分割的 token,模型必須記住當你將該特定 token 與每個其他數字 token 相加、減去或相乘時會發生什麼。但當它被分割時,模型學習數字級別的操作如何工作。一些 tokenizers 如 Llama3 ([Grattafiori et al., 2024](https://arxiv.org/abs/2407.21783)) 將 1 到 999 的數字編碼為唯一 tokens,其餘由這些 tokens 組成。

有關 tokenization 如何影響算術效能的更深入探討,請參見 [From Digits to Decisions: How Tokenization Impacts Arithmetic in LLMs](https://huggingface.co/spaces/huggingface/number-tokenization-blog),它比較了不同 tokenization 方案在數學任務上的表現。

因此我們可以在目標領域衡量 fertility 來評估 tokenizer 的弱點和優點。下表比較了流行 tokenizers 在不同語言和領域的 fertility。

**評估 tokenizers**

為了跨不同語言比較 tokenizers,我們將使用 [FineWeb2](https://arxiv.org/abs/2506.20920) tokenizer 分析的設置,使用維基百科文章作為我們的評估語料庫。對於每種語言,我們將抽樣 100 篇文章以獲得有意義的樣本,同時保持計算可管理。

首先,讓我們安裝依賴項並定義我們想要比較的 tokenizers 和語言:

```bash
pip install transformers datasets sentencepiece 'datatrove[multilingual]'
# we need datatrove to load word tokenizers
```

```python
tokenizers = [("Llama3", "meta-llama/Llama-3.2-1B"),
    ("Gemma3", "google/gemma-3-1b-pt"),
    ("Mistral (S)", "mistralai/Mistral-Small-24B-Instruct-2501"),
    ("Qwen3", "Qwen/Qwen3-4B")]

languages = [("English", "eng_Latn", "en"),
    ("Chinese", "cmn_Hani", "zh"),
    ("French", "fra_Latn", "fr"),
    ("Arabic", "arb_Arab", "ar"),]
```

現在讓我們載入我們的維基百科樣本,我們使用串流來避免下載整個資料集:

```python
from datasets import load_dataset

wikis = {}
for lang_name, lang_code, short_lang_code in languages:
    wiki_ds = load_dataset("wikimedia/wikipedia", f"20231101.{short_lang_code}", streaming=True, split="train")
    wiki_ds = wiki_ds.shuffle(seed=42, buffer_size=10_000)
    # Sample 100 articles per language
    ds_iter = iter(wiki_ds)
    wikis[lang_code] = "\n".join([next(ds_iter)["text"] for _ in range(100)])
```

準備好資料後,我們現在可以在每種語言上評估每個 tokenizer。對於每個組合,我們從 [datatrove](https://github.com/huggingface/datatrove) 載入適當的單詞 tokenizer 並計算兩個指標:

```python
from transformers import AutoTokenizer
from datatrove.utils.word_tokenizers import load_word_tokenizer
import pandas as pd

results = []
for tokenizer_name, tokenizer_path in tokenizers:
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, trust_remote_code=True)
    for lang_name, lang_code, short_lang_code in languages:
        word_tokenizer = load_word_tokenizer(lang_code)
        # Compute metrics on Wikipedia
        fertility, pcw = compute_tokenizer_metrics(tokenizer, word_tokenizer, wikis[lang_code])
        results.append({
            "tokenizer": tokenizer_name,
            "language": lang_name,
            "fertility": fertility,
            "pcw": pcw
        })

df = pd.DataFrame(results)
print(df)
```

```
      tokenizer    language  fertility       pcw
0        Llama3     English   1.481715  0.322058
1        Llama3     Chinese   1.601615  0.425918
2        Llama3      French   1.728040  0.482036
3        Llama3     Spanish   1.721480  0.463431
4        Llama3  Portuguese   1.865398  0.491938
5        Llama3     Italian   1.811955  0.541326
6        Llama3      Arabic   2.349994  0.718284
7        Gemma3     English   1.412533  0.260423
8        Gemma3     Chinese   1.470705  0.330617
9        Gemma3      French   1.562824  0.399101
10       Gemma3     Spanish   1.586070  0.407092
11       Gemma3  Portuguese   1.905458  0.460791
12       Gemma3     Italian   1.696459  0.484186
13       Gemma3      Arabic   2.253702  0.700607
14  Mistral (S)     English   1.590875  0.367867
15  Mistral (S)     Chinese   1.782379  0.471219
16  Mistral (S)      French   1.686307  0.465154
17  Mistral (S)     Spanish   1.702656  0.456864
18  Mistral (S)  Portuguese   2.013821  0.496445
19  Mistral (S)     Italian   1.816314  0.534061
20  Mistral (S)      Arabic   2.148934  0.659853
21        Qwen3     English   1.543511  0.328073
22        Qwen3     Chinese   1.454369  0.307489
23        Qwen3      French   1.749418  0.477866
24        Qwen3     Spanish   1.757938  0.468954
25        Qwen3  Portuguese   2.064296  0.500651
26        Qwen3     Italian   1.883456  0.549402
27        Qwen3      Arabic   2.255253  0.660318
```

結果顯示了一些贏家和權衡,取決於你的優先事項:

##### Fertility(每單詞 tokens 數)

_越低越好_

| Tokenizer (詞彙表大小) | English | Chinese | French | Arabic |
|---|---|---|---|---|
| Llama3 (128k) | 1.48 | 1.60 | 1.73 | 2.35 |
| Mistral Small (131k) | 1.59 | 1.78 | 1.69 | 2.15 |
| Qwen3 (151k) | 1.54 | 1.45 | 1.75 | 2.26 |
| Gemma3 (262k) | 1.41 | 1.47 | 1.56 | 2.25 |

##### 連續單詞比例(%)

_越低越好_

| Tokenizer (詞彙表大小) | English | Chinese | French | Arabic |
|---|---|---|---|---|
| Llama3 (128k) | 32.2% | 42.6% | 48.2% | 71.8% |
| Mistral Small (131k) | 36.8% | 47.1% | 46.5% | 66.0% |
| Qwen3 (151k) | 32.8% | 30.7% | 47.8% | 66.0% |
| Gemma3 (262k) | 26.0% | 33.1% | 39.9% | 70.1% |

Gemma3 tokenizer 在多種語言上實現了低 fertilities 和單詞分割率,特別是在英語、法語和西班牙語上,這可以透過其 tokenizer 訓練資料和非常大的 262k 詞彙表大小來解釋,大約是 Llama3 的 128k 的 2 倍。Qwen3 tokenizer 在中文上表現出色,但在英語、法語和西班牙語上落後於 Llama3 的 tokenizer。Mistral Small 的 tokenizer ([Mistral AI, 2025](https://mistral.ai/news/mistral-small-3-1)) 在阿拉伯語上表現最佳,但在英語和中文上不如其他 tokenizers。

**在現有和自訂 Tokenizers 之間選擇**

目前,有很好的強大 tokenizers 可供選擇。許多最近的模型從 GPT4 的 tokenizer ([OpenAI et al., 2024](https://arxiv.org/abs/2303.08774)) 開始,並用額外的多語言 tokens 增強它。正如我們在上表中看到的,Llama 3 的 tokenizer 在多語言文字和程式碼上平均表現良好,而 Qwen 2.5 在中文和一些低資源語言上特別出色。

  - **何時使用現有 tokenizers:** 如果我們的目標用例與上述最佳 tokenizers(Llama、Qwen、Gemma)的語言或領域覆蓋範圍匹配,那麼它們是經過實戰測試的可靠選擇。對於 SmolLM3 訓練,我們選擇了 Llama3 的 tokenizer:它在我們的目標語言(英語、法語、西班牙語、葡萄牙語、義大利語)上提供了有競爭力的 tokenization 品質,詞彙表大小適中,對我們的小型模型大小有意義。對於 embeddings 占總參數較小比例的較大模型,Gemma3 的效率收益變得更有吸引力。
  - **何時訓練我們自己的:** 如果我們正在為低資源語言訓練或有非常不同的資料混合,我們可能需要訓練自己的 tokenizer 以確保良好的覆蓋範圍。在這種情況下,重要的是我們在接近我們相信最終訓練混合的資料集上訓練 tokenizer。這創建了一個有點雞生蛋蛋生雞的問題,因為我們需要一個 tokenizer 來執行資料消融實驗並找到混合。但我們可以在啟動最終執行之前重新訓練 tokenizer,並驗證下游效能改善和 fertilities 仍然良好。

你的 tokenizer 選擇可能看起來像一個技術細節,但它貫穿模型效能的每個方面。所以不要害怕投入時間把它做對。

#### [SmolLM3](https://huggingfacetb-smol-training-playbook.hf.space/#smollm3)

現在我們已經探索了架構景觀並執行了系統化的消融實驗,讓我們看看這一切如何在 SmolLM3 這樣的模型中實際結合在一起。

SmolLM 家族是關於推動小型模型可能性的邊界。SmolLM2 在 135M、360M 和 1.7B 參數上提供了三個有能力的模型,所有這些都設計為在裝置上有效執行。對於 SmolLM3,我們想擴大效能,同時保持足夠小以適應手機,並解決 SmolLM2 的弱點:多語言性、非常長的上下文處理和強大的推理能力。我們選擇 3B 參數作為這種平衡的最佳點。

由於我們正在擴大一個已驗證的配方,我們自然傾向於密集 transformers。MoE 尚未在 nanotron 中實作,我們已經擁有訓練強大小型密集模型的專業知識和基礎設施。更重要的是,對於邊緣裝置部署,我們受記憶體限制,即使只有少數 experts 是活躍的,具有許多參數的 MoE 也會受到限制,因為我們仍然需要將所有 experts 載入記憶體,使密集模型對我們的邊緣部署目標更實用。

**消融實驗:** 我們從 SmolLM2 1.7B 的架構作為我們的基礎開始,然後使用 Qwen2.5-3B 佈局在 100B tokens 上訓練了一個 3B 消融實驗模型。這給了我們一個可靠的基準線來單獨測試每個修改。每個架構變更需要改善損失和英語基準測試的下游效能,或提供可衡量的好處,如推理速度,而沒有品質下降。

這是我們在啟動最終執行之前測試的內容:

**Tokenizer:** 在深入架構修改之前,我們需要選擇一個 tokenizer。我們找到了一組覆蓋我們目標語言和領域的好 tokenizers。根據我們的 fertility 分析,Llama3.2 的 tokenizer 在我們的 6 種目標語言之間給了我們最好的權衡,同時將詞彙表保持在 128k,足夠大以實現多語言效率,但又不太大以至於用 embedding 權重膨脹我們的 3B 參數數量。

**Grouped Query Attention (GQA)** : 我們重新確認了我們早期的發現,即具有 4 個群組的 GQA 匹配 Multi-Head Attention 效能,但這次在 3B 規模上使用 100B tokens。KV cache 效率收益太好了,無法放棄,特別是對於記憶體珍貴的裝置部署。

**NoPE 用於長上下文** : 我們實作了 NoPE,透過每 4 層移除 RoPE。我們的 3B 消融實驗證實了上面部分的發現。NoPE 改善了長上下文處理,而沒有犧牲短上下文效能。

**文件內注意力遮罩** : 我們在訓練期間防止跨文件注意力,以幫助在非常大的序列上訓練時的訓練速度和穩定性,我們再次發現這不會影響下游效能。

**模型佈局優化** : 我們比較了文獻中最近 3B 模型的佈局,一些優先考慮深度,其他考慮寬度。我們在訓練設置上測試了 Qwen2.5-3B (3.1B)、Llama3.2-3B (3.2B) 和 Falcon3-H1-3B (3.1B) 佈局,其中深度和寬度有所不同。結果很有趣:所有佈局實現了幾乎相同的損失和下游效能,儘管 Qwen2.5-3B 實際上參數更少。但 Qwen2.5-3B 的更深架構與研究一致,研究顯示網路深度有益於泛化 ([Petty et al., 2024](https://arxiv.org/abs/2310.19956))。因此,我們選擇了更深的佈局,押注它會隨著訓練的進展而有所幫助。

**穩定性改進** : 我們保留了 SmolLM2 的 tied embeddings,但添加了一個受 OLMo2 啟發的新技巧,從 embeddings 移除 weight decay。我們的消融實驗顯示這不會損害效能,同時降低 embedding norms,這可以幫助防止訓練發散。

這種系統化消融實驗方法的美妙之處在於,我們可以自信地結合所有這些修改,知道每一個都已經過驗證。

**在消融實驗中組合變更**

實際上我們逐步測試變更:一旦一個功能被驗證,它就成為測試下一個功能的基準線的一部分。測試順序很重要:首先從經過實戰測試的功能開始(tie embeddings → GQA → 文件遮罩 → NoPE → 移除 weight decay)。

#### [參與規則](https://huggingfacetb-smol-training-playbook.hf.space/#rules-of-engagement-1)

> TL;DR:你的用例驅動你的選擇。

**讓你的部署目標指導架構決策。** 在評估新的架構創新時,考慮你的模型實際將如何以及在哪裡執行。

**在創新和實用主義之間取得正確的平衡。** 我們不能忽視主要的架構進步 — 當 GQA 和更好的替代方案存在時,今天使用 Multi-Head Attention 將是一個糟糕的技術選擇。了解最新研究並採用提供明確、已驗證好處的技術。但抵制追逐每篇承諾邊際收益的新論文的誘惑(除非你有資源這樣做或你的目標是架構研究)。

**系統化勝過直覺。** 驗證每個架構變更,無論它在紙上看起來多麼有前途。然後在組合它們之前單獨測試修改以了解它們的影響。

**規模效應是真實的 — 盡可能在目標大小重新消融實驗。** 不要假設你的小規模消融實驗會在目標模型大小上完美保持。如果你有計算資源,嘗試重新確認它們。

**在你的實際領域上驗證 tokenizer 效率。** 跨目標語言和領域的 Fertility 指標比遵循最新模型使用的內容更重要。50k 英語 tokenizer 不適合嚴肅的多語言工作,但如果你沒有覆蓋那麼多語言,你也不需要 256k 詞彙表。
