### [優化器和訓練超參數](https://huggingfacetb-smol-training-playbook.hf.space/#optimiser-and-training-hyperparameters)

各個部分正在就位。我們已經執行了消融實驗,確定了架構,並選擇了 tokenizer。但在我們真正啟動訓練之前,還有一些關鍵的缺失部分:我們應該使用哪個優化器?什麼學習率和批次大小?我們應該如何在訓練過程中排程學習率?

這裡誘人的方法是直接從文獻中的另一個強大模型借用值。畢竟,如果它對大型實驗室有效,它應該對我們有效,對吧?對於許多情況,如果我們從類似的架構和模型大小取值,這將工作得很好。

然而,我們冒著因不為我們的特定設置調整這些值而留下效能的風險。文獻超參數是為特定資料和約束優化的,有時這些約束甚至不是關於效能的。也許那個學習率在開發早期被選中,從未重新審視。即使模型作者進行了徹底的超參數掃描,這些最佳值也是為他們的確切架構、資料和訓練方案組合找到的,而不是我們的。文獻值總是一個很好的起點,但探索我們是否能在附近找到更好的值是個好主意。

在本章中,我們將探索最新的優化器(看看值得信賴的老 AdamW ([Kingma, 2014](https://huggingfacetb-smol-training-playbook.hf.space/#bib-kingma2014adam)) 是否仍經得起[時間的考驗](https://blog.iclr.cc/2025/04/14/announcing-the-test-of-time-award-winners-from-iclr-2015/) 🎉),深入探討超越標準 cosine decay 的學習率排程,並弄清楚如何在給定模型和資料大小的情況下調整學習率和批次大小。

讓我們從優化器之戰開始。

#### [優化器:AdamW 及其他](https://huggingfacetb-smol-training-playbook.hf.space/#optimizers-adamw-and-beyond)

優化器是整個 LLM 訓練操作的核心。它根據過去的更新、當前權重和從損失導出的梯度決定每個參數的實際更新步驟。同時它也是一個[記憶體和計算飢渴的野獸](https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=profiling_the_memory_usage),因此可以影響你需要多少 GPU 以及你的訓練速度有多快。

如果你不確定什麼是優化器以及它有什麼用,請查看 Ruder 的[關於梯度下降和優化器的部落格](https://www.ruder.io/optimizing-gradient-descent/),它特別比較了很酷的優化器。

我們不遺餘力地總結了用於 LLM 預訓練的優化器的當前景觀:

| 模型 | 優化器 |
|---|---|
| Kimi K2, GLM 4.5 | Muon |
| 其他所有人 | AdamW |

所以,你可能想知道為什麼每個人都在使用 AdamW?

撰寫這部分部落格文章的人認為這是因為「人們很懶」(嗨,我是 [Elie](https://x.com/eliebakouch)),但其他人可能更現實地說 AdamW 在不同規模上長期工作得很好/更好,並且改變這樣一個核心組件總是有點可怕,特別是如果很難(即昂貴)測試它在非常長的訓練中表現如何。

此外,公平比較優化器比看起來更難。規模以難以在小型消融實驗中模擬的方式改變動態,因此超參數調整很複雜。你可能會說:_「沒關係,我已經調整我的 AdamW 好幾週了,我可以只重用相同的超參數來比較!」_ 我們非常希望這是真的。但不幸的是,對於每個優化器,你需要進行適當的超參數搜尋(1D?2D?3D?),這使得優化器研究變得困難和昂貴。

通常,基準線調整得不是很好,所以新優化器與弱 AdamW 設置進行比較。最近的一項研究 ([Wen et al., 2025](https://arxiv.org/abs/2509.02046)) 顯示僅此一項就扭曲了報告的收益有多少。

所以讓我們從經典和 Durk Kingma 可怕的 [Google scholar ](https://scholar.google.com/citations?user=yyIoQu4AAAAJ&hl=en)統治的基礎開始:AdamW。

**AdamW**

Adam(Adaptive Momentum Estimation)是一種一階優化技術。這意味著除了單獨查看梯度之外,我們還考慮權重在前幾步中改變了多少。這使得每個參數的學習率根據動量進行調整。

仔細的讀者可能想知道:嘿,你不是缺少一個 W 嗎?確實!我們特別添加 W(=weight decay)的原因如下。在標準 SGD 中,我們可以簡單地向損失添加 $\lambda \theta^2$(其中 $\theta$ 是權重)來應用 L2 正則化。然而,如果我們對 Adam 做同樣的事情,自適應學習率也會影響 L2 正則化。這意味著正則化強度取決於梯度幅度,削弱了其效果。這不是我們想要的,這就是為什麼 AdamW 將其與主優化迴圈解耦應用以解決這個問題。

有趣的是,在過去幾年中,AdamW 超參數幾乎沒有移動:
  - β₁ = 0.9, β₂ = 0.95
  - grad norm clipping = 1.0
  - weight decay = 0.1(Llama-3-405B 將此降低到 0.01)

從 Llama 1,2,3 到 DeepSeek-V1,2,3 671B,幾乎重用相同的三元組,沒有變化。Durk Kingma 一直都是對的,還是我們可以做得更好?

**一行中的 Muon**

Adam 是一階方法,因為它只使用梯度。Muon 是一個二階優化器,作用於參數 tensor 的_矩陣_視圖。

$$\begin{aligned} G_t &= \nabla_{\theta}\mathcal{L}_t(\theta_{t-1}) \\\ B_t &= \mu\, B_{t-1} + G_t \\\ O_t &= \mathrm{NewtonSchulz5}(B_t) \ \approx\ U V^\top \quad \text{if } B_t = U\Sigma V^\top \text{ (SVD)} \\\ \theta_t &= \theta_{t-1} - \eta\, O_t \end{aligned}$$

看著這些方程,你可能想知道為什麼這是一個二階方法,我只看到梯度,沒有更高階項。二階優化實際上發生在 Newton Schulz 步驟內部,但我們不會在這裡深入更多細節。已經有高品質的部落格深入解釋 Muon,所以這裡我們只列出 Muon 的三個關鍵思想:

要了解更多關於 Muon 的資訊,我們建議查看 Keller Jordan 的這個[部落格](https://kellerjordan.github.io/posts/muon/),Jeremy Bernstein 的這個[部落格](https://jeremybernste.in/writing/deriving-muon)和 Jia-Bin Huang 的這個[影片](https://www.youtube.com/watch?v=bO5nvE289ec),這是一個很好的起點。

  1. **矩陣級幾何 vs. 參數級更新:** AdamW _按參數_預處理(對角二階矩)。Muon 將每個權重**矩陣**視為單個物件,並沿 $G=UV^{\top}$ 更新,這捕獲了行/列子空間結構。
  2. **透過正交化的各向同性步驟:** 用奇異值分解(SVD)分解 $G=U\Sigma V^{\top}$ 將幅度($\Sigma$)與方向(左/右子空間 $U,V$)分離。用 $UV^{\top}$ 替換 $G$ 丟棄奇異值並使步驟在活躍子空間中_各向同性_。這起初有點違反直覺 — 因為丟棄 $\Sigma$ 看起來像是丟失資訊 — 但它減少了軸對齊偏差,並鼓勵探索否則會被非常小的奇異值抑制的方向。關於這種探索是否將不明顯的能力烘焙到模型中,如果你只看損失,仍然存在一個開放問題。
  3. **對較大批次大小的實證容忍度:** 在實踐中,Muon 通常容忍更高的批次大小。我們將在批次大小部分更深入地討論這一點,但這可能是 Muon 採用的一個關鍵點!

多年來,社群主要settled on AdamW,前沿實驗室的優化器配方通常是保密的(例如 Qwen 不談論他們的),但最近 Muon 在高調發布中得到了採用(例如 Kimi K2、GLM-4.5)。希望我們會看到更多開放和穩健的配方來使用。

有一個瘋狂的優化器動物園,研究人員在結合所有可能的動量和導數方面比為它們命名更有創造力:Shampoo、SOAP、PSGD、CASPR、DION、Sophia、Lion... 甚至 AdamW 也有自己的變體,如 NAdamW、StableAdamW 等。深入所有這些優化器值得有自己的部落格,但我們將留待下次。同時,我們推薦 stanford/marin 團隊的這篇令人驚嘆的論文 ([Wen et al., 2025](https://arxiv.org/abs/2509.02046)),他們對許多不同的優化器進行了基準測試,以顯示在進行比較時超參數調整有多重要。

幾乎每個優化器都伴隨著關於我們應該多大程度更新由學習率確定的權重的問題,學習率通常在優化器方程中顯示為標量值。讓我們看看這個看似簡單的主題仍然有許多方面。

#### [學習率](https://huggingfacetb-smol-training-playbook.hf.space/#learning-rate)

學習率是我們必須設置的最重要的超參數之一。在每個訓練步驟,它控制我們根據計算的梯度調整模型權重的程度。選擇太低的學習率,我們的訓練變得痛苦緩慢,我們可能陷入糟糕的局部最小值。損失曲線看起來會很平,我們會燒掉計算預算而沒有取得有意義的進展。另一方面,如果我們將學習率設置得太高,我們會導致優化器採取過大的步驟,超過最佳解決方案,永遠不會收斂,或者難以想像的事情發生,損失發散,損失飆升到月球。

但最佳學習率甚至不是恆定的,因為學習動態在訓練期間會改變。當我們遠離好的解決方案時,高學習率在早期有效,但在接近收斂時會導致不穩定。這就是學習率排程的作用:從零開始 warmup 以避免早期混亂,然後 decay 以穩定到一個好的最小值。這些模式(例如 warmup + cosine decay)已經為神經網路訓練驗證了多年。

**Warmup 步數**

大多數現代 LLM 使用固定數量的 warmup 步數(例如 2000),無論模型大小和訓練長度如何,如[表 1](https://huggingfacetb-smol-training-playbook.hf.space/#llms-landscape-pretrain) 所示。我們發現對於長訓練,增加 warmup 步數的數量不會對效能產生影響,但對於非常短的訓練,人們通常使用訓練步數的 1% 到 5%。

讓我們看看常見的排程,然後討論如何選擇峰值。

**學習率排程:超越 Cosine Decay**

多年來已知改變學習率有助於收斂 ([Smith & Topin, 2018](https://arxiv.org/abs/1708.07120)),cosine decay ([Loshchilov & Hutter, 2017](https://arxiv.org/abs/1608.03983)) 是訓練 LLM 的首選排程:在 warmup 後從峰值學習率開始,然後遵循 cosine 曲線平滑降低。它簡單且效果很好。但它的主要缺點是不靈活;我們需要預先知道我們的總訓練步數,因為 cosine 週期長度必須匹配你的總訓練持續時間。這在常見場景中成為問題:你的模型尚未達到平台期,或者你獲得更多計算資源並想訓練更長時間,或者你正在執行擴展定律並需要在不同 token 數量上訓練相同模型。Cosine decay 迫使你從頭開始重新啟動。

有趣的是,這種不靈活性扭曲了早期的擴展定律研究 ([Kaplan et al., 2020](https://arxiv.org/abs/2001.08361)),因為他們在不同 token 數量上訓練模型時使用了固定的 cosine 排程長度,低估了資料大小的影響。Chinchilla 研究 ([Hoffmann et al., 2022](https://arxiv.org/abs/2203.15556)) 糾正了這一點,並將排程長度與每個模型的實際訓練持續時間匹配。

許多團隊現在使用的排程在 warmup 後不需要立即開始 decay。這是 **Warmup-Stable-Decay (WSD)** ([Hu et al., 2024](https://arxiv.org/abs/2404.06395)) 和 **Multi-Step** ([DeepSeek-AI, :, et al., 2024](https://arxiv.org/abs/2401.02954)) 變體的情況,如下圖所示。你在大部分訓練中保持恆定的高學習率,然後在最後階段急劇 decay(通常是最後 10-20% 的 tokens)用於 WSD,或者進行離散下降(steps)來降低學習率,例如在訓練的 80% 後,然後在 90% 後,如 [DeepSeek LLM](https://arxiv.org/abs/2401.02954) 的 Multi-Step 排程所做的那樣。

_重置視圖_
_圖例_

這些排程比 cosine decay 提供了實際優勢。我們可以在執行中期擴展訓練而無需重新啟動,無論我們想訓練比最初計劃的更長時間,提前 decay 以獲得訓練進度的更多衡量,我們可以用一次主要訓練執行在不同 token 數量上執行擴展定律實驗。此外,研究表明 WSD 和 Multi-Step 都匹配 cosine decay ([DeepSeek-AI, :, et al., 2024](https://arxiv.org/abs/2401.02954); [Hägele et al., 2024](https://arxiv.org/abs/2405.18392)),同時對實際訓練場景更實用。

最近 GLM 4.5 提到 WSD 在一般基準測試(SimpleQA、MMLU)上表現較差,但他們沒有提供任何結果。

但你可能注意到這些排程與 cosine 相比引入了新的超參數:WSD 中的 decay 階段應該持續多長時間?Multi-Step 變體中的每個步驟應該持續多長時間?

  - 對於 WSD:匹配 cosine 效能所需的 cooldown 持續時間隨著更長的訓練執行而減少,建議將總 tokens 的 10-20% 分配給 decay 階段 ([Hägele et al., 2024](https://arxiv.org/abs/2405.18392))。我們將在下面的消融實驗中確認此設置匹配 cosine。
  - 對於 Multi-Step:DeepSeek LLM 的消融實驗發現,雖然他們的基準線 80/10/10 分割(穩定到 80%,第一步從 80-90%,第二步從 90-100%)匹配 cosine,但調整這些比例甚至可以超越它,例如當使用 70/15/15 和 60/20/20 分割時。

但我們可以對這些排程更有創意。讓我們看看 DeepSeek 模型每個家族中使用的排程:

_重置視圖_
_圖例_

DeepSeek LLM 使用基準線 Multi-Step 排程(80/10/10)。[DeepSeek V2](https://arxiv.org/abs/2405.04434) 調整比例為 60/30/10,給第一個 decay 步驟更多時間。[DeepSeek V3](https://arxiv.org/abs/2412.19437) 採取了最有創意的方法:他們不是保持恆定學習率然後兩個急劇步驟,而是從恆定階段過渡到 cosine decay(從訓練的 67% 到 97%),然後在最後急劇步驟之前應用簡短的恆定階段。

**DeepSeek 排程變更**

DeepSeek-V2 和 V3 的技術報告不包括對這些排程變更的消融實驗。對於你的設置,從簡單的 WSD 或 Multi-Step 排程開始,然後考慮透過消融實驗調整參數。

讓我們在這裡停止我們對異國學習率排程的調查,並燒掉一些 GPU 小時來確定實踐中有效的方法!

**消融實驗 - WSD 匹配 Cosine**

現在是消融實驗的時候了!讓我們測試 WSD 在實踐中是否真的匹配 cosine 的效能。我們不會在這裡顯示 Multi-Step 消融實驗,但我們推薦 DeepSeek LLM 的消融實驗,他們顯示 Multi-Step 與不同階段分割的 cosine 匹配。在本節中,我們將比較 cosine decay 與兩個 decay 視窗的 WSD:10% 和 20%。

_重置視圖_
_圖例_
HellaSwag | MMLU | ARC | PIQA | OpenBookQA | WinoGrande

評估結果顯示所有三種配置的最終效能相似。查看損失和評估曲線(特別是 HellaSwag),我們看到一個有趣的模式:cosine 在穩定階段(在 WSD 的 decay 開始之前)實現更好的損失和評估分數。然而,一旦 WSD 進入其 decay 階段,損失和下游指標幾乎呈線性改善,允許 WSD 在訓練結束時趕上 cosine。

這證實了 WSD 的 10-20% decay 視窗足以匹配 cosine 的最終效能,同時保持在執行中期擴展訓練的靈活性。我們為 SmolLM3 選擇了具有 10% decay 的 WSD。

**在執行中期比較使用不同排程器訓練的模型**

如果你在穩定階段期間比較 cosine 和 WSD 之間的中間檢查點,請確保對 WSD 檢查點應用 decay 以進行公平比較。

現在我們對流行的學習率排程有了很好的概述,下一個問題是:峰值學習率實際應該是多少?

**找到最佳學習率**

我們如何為我們特定的學習率排程器和訓練設置選擇正確的學習率?

我們可以像對架構選擇那樣在短消融實驗上執行學習率掃描。但最佳學習率取決於訓練持續時間:在短消融實驗中收斂最快的學習率可能不是完整執行的最佳學習率。我們無法負擔多次執行昂貴的多週訓練只是為了測試不同的學習率。

讓我們首先看看我們可以快速執行的簡單掃描,這些掃描幫助我們排除太高或太低的學習率,然後我們將討論超參數的擴展定律。

**消融實驗 - LR 掃描**

為了說明不同學習率的影響,讓我們看看我們在 45B tokens 上訓練的 1B 消融實驗模型的掃描。我們在相同設置下訓練相同模型,使用 4 種不同的學習率:1e-4、5e-4、5e-3、5e-2。結果清楚地顯示了兩個極端的危險:

_重置視圖_
_圖例_
HellaSwag | MMLU | ARC | PIQA | OpenBookQA | WinoGrande

LR 5e-2 幾乎立即發散,損失早期尖峰,從未恢復,使模型無法使用。LR 1e-4 太保守,雖然它訓練穩定,但它比其他學習率收斂得慢得多。5e-4 和 5e-3 的中間地帶顯示更好的收斂和可比的效能。但為每個模型大小執行掃描很快變得昂貴,更重要的是,它不考慮計劃的訓練 tokens 數量,如我們之前所述。這就是擴展定律變得無價的地方。

對於 SmolLM3,我們使用 WSD 排程在 100B tokens 上用 AdamW 訓練 3B 模型,比較幾個學習率。我們發現 2e-4 在損失和下游效能方面都比 1e-4 收斂得快得多,而 3e-4 只比 2e-4 稍好一點。3e-4 的邊際收益伴隨著長訓練執行期間不穩定性的風險增加,因此我們選擇 2e-4 作為我們的最佳點。

這些掃描幫助我們排除明顯太高(發散)或太低(收斂慢)的學習率,但為每個模型大小執行掃描很快變得昂貴,更重要的是,它不考慮計劃的訓練 tokens 數量,如我們之前所述。這就是擴展定律變得無價的地方。

但在我們深入超參數的擴展定律之前,讓我們討論與學習率互動的另一個關鍵超參數:批次大小。

#### [批次大小](https://huggingfacetb-smol-training-playbook.hf.space/#batch-size)

批次大小是在更新模型權重之前處理的樣本數量。它直接影響訓練效率和最終模型效能。如果你的硬體和訓練堆疊在裝置間擴展良好,增加批次大小可以提高吞吐量。但超過某個點,更大的批次開始損害資料效率:模型需要更多總 tokens 才能達到相同的損失。這種情況發生的斷點被稱為**臨界批次大小** ([McCandlish et al., 2018](https://arxiv.org/abs/1812.06162))。

**吞吐量**是訓練期間每秒處理的 tokens 數量。

  - **在保持低於臨界的同時增加批次大小:** 增加批次大小並重新調整學習率後,你用相同數量的 tokens 達到相同的損失,沒有浪費資料。
  - **在保持高於臨界的同時增加批次大小:** 更大的批次開始犧牲資料效率;現在達到相同的損失需要更多總 tokens(因此更多錢),即使牆上時鐘時間下降,因為更多晶片在忙碌。

讓我們嘗試給出一些關於為什麼我們需要重新調整學習率以及如何計算臨界批次大小應該是什麼的估計的直覺。

當批次大小增長時,每個 mini-batch 梯度是真實梯度的更好估計,因此你可以安全地採取更大的步驟(即增加學習率)並在更少的更新中達到目標損失。問題是如何擴展它。

**對 B 個樣本進行平均**

  - 批次梯度:$\tilde{g}_{B} \;=\; \frac{1}{B}\sum_{i=1}^{B} \tilde{g}^{(i)}$
  - 平均值保持不變:$\mathbb{E}\\!\left[\tilde{g}_{B}\right] \;=\; g$
  - 但協方差縮小:$\mathrm{Cov}\\!\left(\tilde{g}_{B}\right) \;=\; \frac{\Sigma}{B}$

SGD 參數更新是:
  - $\Delta w \;=\; -\,\eta \,\tilde{g}_{B}$

這個更新的方差正比於:
  - $\mathrm{Var}(\Delta w) \;\propto\; \eta^{2}\,\frac{\Sigma}{B}$

所以為了保持更新方差大致恆定,如果你將批次大小縮放 k 倍,你想將學習率縮放 $\sqrt k$ 倍。所以假設你已經計算了你的最佳批次大小和學習率,你發現增加到臨界批次大小是可能的並增加吞吐量,你也需要調整最佳學習率。

$$B_{\text{critical}} \;\rightarrow\; kB_{\text{optimal}} \quad\Rightarrow\quad \eta_{\text{critical}} \;\rightarrow\; \sqrt{k}\eta_{\text{optimal}}$$

查看(令人驚嘆的)Jianlin Su 的系列以獲取更多數學:<https://kexue.fm/archives/11260>

對於像 AdamW 或 Muon 這樣的優化器,一個有用的經驗法則是_**平方根**_ _**LR 縮放**_,隨著批次大小增長,但這也取決於優化器。例如使用 AdamW,與 `beta1` / `beta2` 的互動可能引入非常不同的行為。一個實用的替代方案是為短暫視窗分支訓練:在原始批次保持一次執行,以更大的批次和重新縮放的 LR 開始第二次,僅在重新縮放後兩個損失曲線對齊時才採用更大的批次 ([Merrill et al., 2025](https://arxiv.org/abs/2505.23971))。在論文中,他們在切換批次大小時重新 warm up 學習率並重置優化器狀態。他們還設置了容忍度和時間視窗來決定損失是否「匹配」,兩個旋鈕都是經驗性選擇的。他們發現 $B_{simple}$ 估計 - 也是嘈雜的 - 低估了「實際」臨界批次大小。這給你一個快速、低風險的檢查,新的批次/LR 對保持訓練動態。

臨界批次大小不是固定的,它隨著訓練的進展而增長。在訓練早期,模型正在採取大梯度步驟,因此 $\lVert g\rVert^2$ 很大,這意味著 $B_\text{simple}$ 很小,因此模型有較小的臨界批次大小。後來,隨著模型更新穩定,更大的批次變得更有效。這就是為什麼一些大規模訓練不保持批次大小恆定,而使用我們可以稱為批次大小 warmup 的東西。例如,DeepSeek-V3 在前 ~469 B tokens 開始時使用 12.6 M 批次,然後在訓練的剩餘部分增加到 62.9M。像這樣的批次大小 warmup 排程與學習率 warmup 具有相同的目的:它隨著梯度噪聲規模的增長,使模型保持在有效邊界上,在整個過程中保持穩定和高效的優化。

另一個有趣的方法是將損失視為臨界批次大小的代理。Minimax01 使用了這個,在最後階段他們用 128M 批次大小訓練!這有點不同,因為他們不增加學習率,所以他們的批次大小排程充當學習率 decay 排程。

**調整批次大小和學習率**

在實踐中,你可以這樣選擇批次大小和學習率:
  - 你首先選擇你認為最佳的批次大小和學習率,要麼從擴展定律(稍後見!),要麼從文獻。
  - 然後,你可以調整批次大小以查看是否可以提高訓練吞吐量。

關鍵洞見是,在你的起始批次大小和臨界批次大小之間通常有一個範圍,你可以增加它以提高硬體利用率而不犧牲資料效率,但你必須相應地重新調整學習率。如果吞吐量增益不顯著,或者如果測試更大的批次大小(使用重新縮放的學習率)顯示較差的資料效率,請堅持初始值。

如上述注釋中所述,為批次大小和學習率選擇起點的一種方法是透過擴展定律。讓我們看看這些擴展定律如何工作,以及它們如何預測這兩個超參數作為你的計算預算的函數。

#### [**超參數的擴展定律**](https://huggingfacetb-smol-training-playbook.hf.space/#scaling-laws-for-hyperparameters)

最佳學習率和批次大小不僅關於模型架構和大小,它們還取決於我們的計算預算,它結合了模型參數數量和訓練 tokens 數量。實際上,這兩個因素互動以確定我們的更新應該多麼激進或保守。這就是擴展定律的作用。

擴展定律建立描述模型效能如何隨著我們增加訓練規模而演變的實證關係,無論是透過更大的模型還是更多的訓練資料(請參見本章末尾的「擴展定律」部分以獲取完整歷史)。但擴展定律也可以幫助我們預測如何在擴大訓練時調整關鍵超參數,如學習率和批次大小,如 [DeepSeek](https://arxiv.org/abs/2401.02954) 和 [Qwen2.5](https://arxiv.org/abs/2412.15115) 最近的工作所做的那樣。這給我們原則性的預設值,而不是完全依賴超參數掃描。

要在這種情況下應用擴展定律,我們需要一種量化訓練規模的方法。標準指標是計算預算,表示為 C 並以 FLOPs 衡量,可以近似為:

$$C\approx 6×N×D$$

N 是模型參數數量(例如 1B = 1e9),D 是訓練 tokens 數量。這通常以 FLOPs(浮點運算)衡量,這是一種硬體不可知的量化實際完成多少計算的方式。但如果 FLOPs 感覺太抽象,只需這樣想:在 100B tokens 上訓練 1B 參數模型消耗大約 2× 更少的 FLOPs,比在 100B tokens 上訓練 2B 模型,或在 200B tokens 上訓練 1B 模型。

常數 6 來自訓練 Transformer 需要多少浮點運算的實證估計,每參數每 token 大約 6 FLOPs。

如果你想要一個更精確的衡量,考慮 MoE 層和混合層,你可以查看 Megatron-LM 中的 [num_floating_point_operations](https://github.com/NVIDIA/Megatron-LM/blob/f34fa11af6f5dc65f5342f2a785c3227446cebfd/megatron/training/training.py#L158) 函數。

現在,這與學習率有什麼關係?我們可以導出擴展定律,將最佳學習率和批次大小預測為總計算預算(C)的函數。它們幫助回答這樣的問題:
  - 當我從 1B 擴展到 7B 參數時,學習率應該如何變化?
  - 如果我將訓練資料加倍,我應該調整學習率嗎?

讓我們透過走過 DeepSeek 使用的方法來看看這是如何工作的:首先,我們選擇我們的學習率排程,理想情況下是 WSD 以獲得靈活性。然後,我們在一系列計算預算(例如 1e17、5e17、1e18、5e18、1e19、2e19 FLOPs)上訓練模型,使用批次大小和學習率的不同組合。用更簡單的術語:我們為不同數量的 tokens 訓練不同的模型大小,測試不同的超參數設置。這就是 WSD 排程的亮點,我們可以將相同的訓練執行擴展到不同的 token 數量,而無需重新啟動。

對於每個設置,我們對學習率和批次大小進行掃描,並識別導致接近最佳效能的配置,通常定義為在最佳驗證損失的小幅度內(例如 0.25%)(在獨立驗證集上計算,與訓練集具有相似的分布)。每個接近最佳的配置給我們一個資料點 — 一個(計算預算 C,最佳學習率 η)或(C,最佳批次大小 B)的元組。當在對數-對數尺度上繪製時,這些關係通常遵循冪律行為,顯示為大約直線(如上圖所示)。透過擬合這些資料點,我們可以提取擴展定律,描述最佳超參數如何隨計算演變。

這個過程的一個重要發現是,對於固定的模型大小和計算預算,效能在廣泛的超參數範圍內保持穩定。這意味著有一個廣泛的最佳點而不是狹窄的最優。我們不需要找到完美的值,只需要一個足夠接近的值,這使整個過程更加實用。

這裡你可以看到 DeepSeek 導出的擴展定律的結果,其中每個點代表一個接近最佳的設置:

![Image](https://huggingfacetb-smol-training-playbook.hf.space/_astro/image_2471384e-bcac-8059-84a4-d4ce5ae3847c.2mT4C1Dr_1a8Vp7.webp)

這些結果背後的核心直覺是,隨著訓練變得更大和更長,我們想要**更穩定的更新**(因此學習率更小)和**更高效的梯度估計**(因此批次大小更大)。

這些擴展定律給我們學習率和批次大小的起點。但目標不是「每梯度的最佳樣本」,而是「在我們的時間和 GPU 數量約束內可達到的較低損失」,同時仍然從每個 token 提取完整訊號。

在實踐中,你可能能夠將批次大小增加到超過預測的最佳批次大小,以顯著提高吞吐量而不會有意義地損害資料效率,直到我們之前討論的臨界批次大小。

#### [SmolLM3](https://huggingfacetb-smol-training-playbook.hf.space/#smollm3-1)

那麼我們最終為 SmolLM3 使用了什麼?在啟動 SmolLM3 之前的消融實驗時,我們在 100B tokens 上訓練的 1B 模型上比較了 AdamW、AdEMAMix 和 Muon。Muon 在適當調整時可以超越 AdamW,但對學習率敏感並且容易發散。AdeMaMix 不太敏感,並實現了與 Muon 類似的損失。AdamW 是最穩定的,但達到的最終損失高於調整後的替代方案。

然而,當我們擴展到 3B 時,我們遇到了 Muon 和 AdeMaMix 更頻繁的發散。這可能是由於我們在完成消融實驗後發現的並行性錯誤(請參見訓練馬拉松章節),儘管我們還沒有確認這一點。我們決定使用 AdamW(beta1: 0.9, beta2: 0.95),weight decay 0.1 和 gradient clipping 1。畢竟是一個非常 vanilla 的設置。

對於學習率排程,我們選擇了 WSD。我們在 SmolLM2 中成功使用了它,它被證明是我們在易用性和總訓練持續時間靈活性方面以及執行中期訓練 decay 實驗的能力方面的最佳決策之一。我們執行了學習率掃描並確定了 2e-4。對於全域批次大小,我們測試了從 2M 到 4M tokens 的值,但發現對損失或下游效能的影響很小,因此我們選擇了 2.36M tokens - 給我們最佳吞吐量的大小。

#### [參與規則](https://huggingfacetb-smol-training-playbook.hf.space/#rules-of-engagement-2)

> **TL;DR:** 平衡探索和執行,完成比完美更好。

我們談論了很多「什麼」(優化器、學習率、批次大小),但同樣重要的是**如何**。我們如何決定什麼值得實驗?我們如何組織我們的時間?我們什麼時候停止探索,只是訓練?

**明智地分配你在探索和執行之間的時間。** 花費數週完善來自新方法的小改進比將相同計算投資於更好的資料策劃或更徹底的架構消融實驗更沒有價值。根據我們的經驗,儘管這可能讓架構愛好者失望,但最大的效能收益通常來自資料策劃。

**當有疑問時,選擇靈活性和穩定性而不是峰值效能。** 如果兩種方法表現相同好,選擇提供更多靈活性或具有更好實作成熟度和穩定性的方法。像 WSD 這樣的學習率排程讓我們擴展訓練或執行中期訓練實驗,比可能收斂稍好的剛性排程更有價值。

**知道何時停止優化並開始訓練。** 總是有一個更多的超參數要調整或一個更多的優化器要嘗試。為探索設定一個截止日期並堅持它 - 我們實際完成訓練的模型總是會擊敗我們從未開始的完美模型。

![Image](https://huggingfacetb-smol-training-playbook.hf.space/_astro/Capture_decran_2025-10-27_a_22_10_05_2991384e-bcac-802a-a6e6-dab0f9f410ec.CKOEFjcT_b6hhb.webp)

再多一個消融實驗不會傷害(劇透:它確實傷害了)。來自 [sea_snell](https://x.com/sea_snell/status/1905163154596012238)

完美是好的敵人,特別是當我們使用有限的計算預算和截止日期時。
