### [後訓練羅盤:為什麼 → 什麼 → 如何](https://huggingfacetb-smol-training-playbook.hf.space/#post-training-compass-why--what--how)

就像預訓練一樣,後訓練受益於清晰的羅盤,以避免浪費研究和工程週期。以下是如何構建它:
  1. **為什麼要後訓練?** 我們在[預訓練羅盤](https://huggingfacetb-smol-training-playbook.hf.space/#training-compass-why--what--how)中概述的訓練的三個動機 — 研究、生產和策略性開源 — 同樣適用於後訓練。例如,你可能正在探索 RL 是否可以在現有模型中解鎖新的推理能力(研究),或者你可能出於延遲原因需要將大型模型蒸餾成較小的模型(生產),或者你可能已經識別出一個差距,在特定使用案例中沒有強大的開放模型存在(策略性開源)。區別在於後訓練建立在現有能力之上,而不是從頭開始創造它們。然而,在拿起你的 GPU 之前,問問自己:
    - 你真的需要後訓練嗎?許多開放權重模型現在在廣泛的任務上與專有模型競爭。有些甚至可以透過量化和適度的計算在本機執行。如果你想要一個通用助理,[Hugging Face Hub](https://huggingface.co/models) 上的現成模型可能已經滿足你的需求。
    - 你是否可以接觸到高品質、特定領域的資料?當你針對通用模型效能不佳的特定任務或領域時,後訓練最有意義。有了正確的資料,你可以調整模型以為你最關心的應用程式產生更準確的輸出。
    - 你能衡量成功嗎?沒有明確的評估標準,你不會知道後訓練是否真的有幫助。
  2. **後訓練應該達到什麼目標?** 這取決於你的優先事項:
    - 你想要一個很少偏離主題的清晰指令跟隨器嗎?
    - 一個可以根據需要切換語氣和角色的多功能助理?
    - 一個可以處理數學、程式碼或代理問題的推理引擎?
    - 一個可以用多種語言交談的模型?
  3. **你將如何到達那裡?** 這就是配方重要的地方。我們將涵蓋:
    - **監督式微調(SFT)** 以灌輸核心能力。
    - **偏好優化(PO)** 直接從人類或 AI 偏好中學習。
    - **強化學習(RL)** 以超越監督資料完善可靠性和推理。
    - **資料策劃** 以在多樣性和品質之間取得正確的平衡。
    - **評估** 以追蹤進展並及早捕獲退化。
這個羅盤使後訓練的混亂保持基礎。_為什麼_給出方向,_什麼_設定優先事項,_如何_將抱負變成實際的訓練迴圈。
讓我們走過我們如何為 SmolLM3 回答這些問題:
  - **為什麼?** 對我們來說,「為什麼」很簡單,因為我們有一個需要在發布前進行後訓練的基礎模型。同時,像 Qwen3 這樣的混合推理模型變得越來越流行,但顯示如何訓練它們的開放配方很少。SmolLM3 給了我們一個機會來解決兩者:為真實世界使用準備一個模型,並貢獻一個完全開放的配方,與 Qwen3 的 1.7B 和 4B 模型一起坐在 Pareto 前沿。
  - **什麼?** 我們著手訓練一個針對 SmolLM3 優勢量身訂做的混合推理模型,主要是推理品質應該在英語以外的語言中保持。由於真實世界的使用越來越涉及工具呼叫和長上下文工作流程,這些成為我們後訓練配方的核心要求。
  - **如何?** 這就是本章的其餘部分 😀。
就像預訓練一樣,我們從基礎開始:評估和基線,因為每個大型模型都從小型消融實驗開始。但在我們如何消融方面有一個關鍵區別。在預訓練中,「小型」通常意味著更小的模型和資料集。在後訓練中,「小型」意味著更小的資料集和_更簡單的_演算法_。我們幾乎從不使用不同的基礎模型進行消融實驗,因為行為太依賴於模型,並且執行足夠短,可以直接在目標模型上迭代。
這個規則的主要例外是當你從 Hugging Face Hub 使用現成的基礎模型時。在這種情況下,消融基礎模型可能是有意義的,因為在 1T tokens 上訓練的一個模型和在 10T 上訓練的另一個模型之間存在天壤之別,即使它們具有相同的大小。
讓我們從許多模型訓練者避免直到專案後期才開始的主題開始:評估。
