### [優化訓練輸送量](https://huggingfacetb-smol-training-playbook.hf.space/#optimizing-training-throughput)

#### [**我們需要多少 GPU?**](https://huggingfacetb-smol-training-playbook.hf.space/#how-many-gpus-do-we-need)

好問題!在所有這些關於規格和基準測試的討論之後,你仍然需要弄清楚實際問題:你實際上應該租用或購買多少個 GPU?

確定正確的 GPU 數量需要平衡訓練時間、成本和擴展效率。這是我們使用的框架:

**基本大小公式:**

$\text{GPU Count} = \frac{\text{Total FLOPs Required}}{\text{Per-GPU Throughput} \times \text{Target Training Time}}$

這個公式將問題分解為三個關鍵元件:
  - **Total FLOPs Required**:訓練模型所需的計算工作(取決於模型大小、訓練 tokens 和架構)
  - **Per-GPU Throughput**:每個 GPU 實際上可以提供多少 FLOPs per second(不是理論峰值!)
  - **Target Training Time**:你願意等待訓練完成多長時間

關鍵見解:你需要估計**實際輸送量**,而不是峰值規格。這意味著要考慮 Model FLOPs Utilization (MFU):你在實務中實際達到的理論峰值效能的百分比。

對於 SmolLM3,我們的計算如下:
  - **Model size**:3B 參數
  - **Training tokens**:11 兆 tokens
  - **Target training time**:~4 週
  - **Expected MFU**:30%(基於類似規模的實驗)

首先,我們使用標準 transformer 近似**每個 token 6N FLOPs**(其中 N = 參數)計算所需的總 FLOPs:

$\text{Total FLOPs} = 6 \times 3 \times 10^9 \text{ params} \times 11 \times 10^{12} \text{ tokens} = 1.98 \times 10^{23} \text{ FLOPs}$

以我們預期的 30% MFU,我們的有效每 GPU 輸送量變為:

$\text{Effective Throughput} = 720 \times 10^{12} \text{ FLOPs/sec} \times 0.30 = 216 \times 10^{12} \text{ FLOPs/sec}$

現在代入我們的大小公式:

$\text{GPU Count} = \frac{1.98 \times 10^{23} \text{ FLOPs}}{216 \times 10^{12} \text{ FLOPs/sec} \times 4 \text{ weeks} \times 604,800 \text{ sec/week}}$

$ = \frac{1.98 \times 10^{23}}{5.23 \times 10^{20}} \approx 379 \text{ GPUs}$

這個計算指向我們需要 375-400 個 H100,我們獲得了 384 個 H100,這個數字與我們的並行策略很好地對齊,並為我們提供了一個實際的 4 週時間表,還有一些緩衝來應對意外問題,如節點失敗和重新啟動。

**為什麼更多 GPU 並不總是更好:Amdahl 定律在行動中**

這是一個反直覺的真理:**增加更多 GPU 實際上可能使你的訓練變慢**。這就是 [Amdahl's Law](https://en.wikipedia.org/wiki/Amdahl%27s_law) 發揮作用的地方。

Amdahl 定律指出,並行化的加速從根本上受到工作負載的串行(不可並行化)部分的限制。在 LLM 訓練中,這個「串行」部分主要是**通訊開銷:**跨 GPU 同步梯度/權重/啟動所花費的時間,這無法被並行化(閱讀更多 [這裡](https://acenet-arc.github.io/ACENET_Summer_School_General/05-performance/index.html))。

公式是:**Maximum Speedup = 1 / (Serial Fraction + Parallel Fraction / Number of Processors)**

GPU Scaling in distributed LLM Training: Amdahl's Law in action

GPU Speedup vs Number of GPUs
10020030040050050100150200Number of GPUsSpeedup

GPU Efficiency vs Number of GPUs
100200300400500020406080100Number of GPUsEfficiency (%)

Legend
s = 0.00s = 0.01s = 0.02s = 0.05s = 0.10s = 0.20

對於 SmolLM3 的 3B 模型,如果通訊佔每個訓練步驟的 10%,那麼無論你添加多少 GPU,你永遠不會獲得超過 10 倍的加速。更糟糕的是,當你添加更多 GPU 時,通訊部分通常會_增加_,因為:
  - 更多 GPU = 更多 AllReduce 參與者 = 更長的同步
  - 網路延遲/頻寬成為瓶頸
  - 小型模型無法將通訊隱藏在計算後面

對於 SmolLM3,我們使用弱擴展原則:我們的全域批次大小隨著我們的 GPU 數量擴展,全域維持每個 GPU 大約 8K tokens。這使我們的通訊與計算比率保持合理,同時最大化輸送量。

#### [找到最佳並行配置](https://huggingfacetb-smol-training-playbook.hf.space/#finding-the-optimal-parallelism-configuration)

一旦你獲得了你的 GPU,下一個挑戰是配置它們以實際高效訓練。為此,並行策略變得至關重要。

我們遵循 [**Ultra-Scale Playbook**](https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=step_1%3A_fitting_a_training_step_in_memory) 的方法來找到最佳訓練配置。該手冊將問題分解為三個順序步驟:首先確保模型適合記憶體,然後達到你的目標批次大小,最後優化以獲得最大輸送量。讓我們看看我們如何將其應用於 SmolLM3。

有關不同並行策略(Data Parallelism、Tensor Parallelism、Pipeline Parallelism、ZeRO 等)的詳細解釋,我們再次敦促你查看 [Ultra-Scale Playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook) *插入 Bernie meme*

#### [步驟 1:將訓練步驟放入記憶體](https://huggingfacetb-smol-training-playbook.hf.space/#step-1-fitting-a-training-step-in-memory)

第一個問題很簡單:我們的 SmolLM3 3B 模型甚至適合單個 H100 的 80GB 記憶體嗎?為了回答這個問題,我們使用 [nanotron's ](https://huggingface.co/spaces/nanotron/predict_memory)[`predict_memory`](https://huggingface.co/spaces/nanotron/predict_memory)[ tool](https://huggingface.co/spaces/nanotron/predict_memory),它估計模型參數、優化器狀態、梯度和啟動的記憶體消耗。

Memory Component Breakdown
586511730117302346010421288Model BF16FP32 ParametersFP32 GradientsOptimizer StatesDDP Gradient BuffersZERO-3 BuffersOverheadActivations05,00010,00015,00020,000Memory (MiB)

Memory Timeline
58652932550613527857407352785Model InitGradient Accumulator InitFwd-Bwd PeakOptimizer Step2nd Fwd-Bwd Peak2nd Optimizer Step010,00020,00030,00040,00050,00060,00070,00080,000Memory (MiB)

Legend
Model BF16FP32 ParametersFP32 GradientsOptimizer StatesOverheadActivations

來自 [nanotron's predict_memory tool](https://huggingface.co/spaces/nanotron/predict_memory) 的記憶體時間線,顯示 SmolLM3 3B 峰值為 74GB,接近 H100 的 80GB 限制。

結果顯示我們接近 80GB 限制。這意味著我們需要某種形式的並行,以減少每個 GPU 的記憶體佔用,無論是 Tensor Parallelism(跨 GPU 分割模型層)、Pipeline Parallelism(跨 GPU 分割模型深度)還是 ZeRO 優化器分片(分佈優化器狀態)。沒有至少一種這些策略,我們將無法高效訓練或根本無法訓練。

#### [步驟 2:達到目標全域批次大小](https://huggingfacetb-smol-training-playbook.hf.space/#step-2-achieving-the-target-global-batch-size)

現在我們知道模型以某種形式的並行適合記憶體,我們需要確定如何達到我們約 200 萬 tokens 的目標全域批次大小(GBS)。這個約束為我們提供了第一個方程式:

$\text{GBS} = \text{DP} \times \text{MBS} \times \text{GRAD\_ACC} \times \text{SEQLEN} \approx 2\text{M tokens}$

其中:
  - **DP (Data Parallelism)**:資料並行副本數
  - **MBS (Micro Batch Size)**:每個 GPU 每個 micro-batch 處理的 tokens
  - **GRAD_ACC (Gradient Accumulation)**:優化器步驟前的前向-後向數
  - **SEQLEN (Sequence Length)**:每個序列的 tokens(第 1 個預訓練階段為 4096)

我們還有來自我們 384 個 H100 的硬體約束:

$\text{DP} \times \text{TP} \times \text{PP} = 384 = 2^7 \times 3$

其中:
  - **TP (Tensor Parallelism)**:每個模型層的 GPU(分割權重矩陣)
  - **PP (Pipeline Parallelism)**:每個模型深度的 GPU(垂直分割層)

這兩個方程式定義了我們的搜尋空間。我們需要找到滿足兩個約束同時最大化訓練輸送量的值。

#### [步驟 3:優化訓練輸送量](https://huggingfacetb-smol-training-playbook.hf.space/#step-3-optimizing-training-throughput)

建立了我們的約束後,我們需要找到最大化訓練輸送量的並行配置。搜尋空間由我們的硬體拓撲和模型架構定義。

正如我們在上面的部分看到的,我們的硬體設定呈現兩種不同類型的互連:用於節點內通訊的 NVLink (900 GB/s) 和用於節點間通訊的 EFA (~50 GB/s)。這種拓撲自然建議使用至少兩種形式的並行來匹配我們的網路特性。這些互連之間的顯著頻寬差異將嚴重影響哪些並行策略最有效。

從模型角度來看,SmolLM3 的架構限制了我們的選擇。由於我們不使用 Mixture-of-Experts 架構,我們不需要 **Expert Parallelism**。同樣,在第一階段使用 4096 序列長度訓練意味著不需要 **Context Parallelism**。這使我們剩下三個主要並行維度來探索:**Data Parallelism** (DP)、**Tensor Parallelism** (TP) 和 **Pipeline Parallelism** (PP)。

鑑於我們來自步驟 2 的約束,我們需要掃描幾個參數:
  - **DP with ZeRO variants** (ZeRO-0, ZeRO-1, ZeRO-3):從 1 到 384 的值,約束為 2 和/或 3 的倍數
  - **TP** (1, 2, 3, 4, 6, 8):保持在單個節點內以充分利用 NVLink 的高頻寬
  - **PP** (1..48):跨 GPU 分割模型深度
  - **MBS** (2, 3, 4, 5):根據並行的記憶體節省,我們可以增加 MBS 以更好地利用 Tensor Cores
  - **Activation checkpointing** (none, selective, full):用額外的計算換取減少的記憶體和通訊
  - **Kernel optimizations**:CUDA graphs 和優化的 kernels(如果可用)

雖然這可能看起來像一個壓倒性的組合數量,但一個實用的方法是首先獨立基準測試每個維度,然後消除顯著損害輸送量的配置。關鍵見解是並非所有並行策略都是平等的。有些引入的通訊開銷遠遠超過其好處,尤其是在我們的規模上。

在我們的案例中,**Pipeline Parallelism** 顯示出不良的效能特性。PP 需要跨節點頻繁的 pipeline bubble 同步,而對於我們相對較小的 3B 模型,通訊開銷主導了任何潛在的好處。此外,我們無法存取可以完全消除 pipeline bubble 的高效 PP 排程,這進一步限制了 PP 的可行性。同樣,**ZeRO** 高於 0 的級別引入了顯著的 all-gather 和 reduce-scatter 運算,這些運算對輸送量的傷害大於它們對記憶體的幫助。這些早期基準測試使我們能夠顯著縮小搜尋空間,專注於將 **Data Parallelism** 與適度的 **Tensor Parallelism** 結合的配置。

👉 為了評估每個配置,我們執行 5 次迭代的基準測試並記錄**每個 GPU 每秒 tokens (tok/s/gpu)**,這最終是我們關心的指標。我們使用 Weights & Biases 和 Trackio 記錄輸送量和配置,使比較不同並行策略變得容易。

在系統地基準測試 nanotron 中的可用選項後,我們確定了 **DP = 192**,它利用節點間 EFA 頻寬進行資料並行梯度同步。這意味著 192 個獨立的模型副本,每個處理不同批次的資料。對於 Tensor Parallelism,我們選擇了 **TP = 2**,將 tensor-parallel 通訊保持在單個節點內,以充分利用 NVLink 的高頻寬。這將每個層的權重矩陣分割到兩個 GPU 上,需要快速通訊進行前向和後向傳遞。

我們的 **Micro Batch Size = 3** 在記憶體使用和計算效率之間取得平衡。更大的批次大小會更好地利用 Tensor Cores,但我們已經接近記憶體限制。最後,我們選擇了 ZeRO-0,意味著沒有優化器狀態分片。雖然 ZeRO-1 或 ZeRO-3 可以減少記憶體佔用,但在我們的 384 個 GPU 上收集和分散優化器狀態的通訊開銷會顯著損害輸送量。

許多這些並行決策受到實驗時庫狀態的影響。例如,nanotron 還不支援 ZeRO-3,而且我們無法存取可以消除 pipeline bubbles 的高度優化的 Pipeline Parallelism 排程。隨著框架的演變,這些權衡中的一些可能會改變。始終歡迎貢獻!

這種配置在我們的 384 H100 叢集上實現了我們約 200 萬 tokens 的目標全域批次大小(192 × 3 × 1 × 4096 ≈ 2.3M),同時最大化了輸送量。你可以在我們的 [stage1_8T.yaml](https://github.com/huggingface/smollm/blob/main/text/pretraining/smollm3/stage1_8T.yaml) 中看到完整的訓練配置。
