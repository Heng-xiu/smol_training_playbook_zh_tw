### [首要之務:評估先於一切](https://huggingfacetb-smol-training-playbook.hf.space/#first-things-first-evals-before-everything-else)

後訓練的第一步 — 就像預訓練一樣 — 是決定正確的評估集。由於今天大多數 LLM 被用作助理,我們發現瞄準「運作良好」的模型是比追逐抽象的「智慧」基準測試(如 [ARC-AGI](https://arcprize.org/arc-agi))更好的目標。那麼一個好的助理需要做什麼?至少,它應該能夠:
  - 處理模糊的指令
  - 逐步規劃
  - 編寫程式碼
  - 在適當時呼叫工具
這些行為借鑑了推理、長上下文處理以及數學、程式碼和工具使用技能的混合。小到甚至小於 3B 參數的模型可以作為助理運作良好,儘管效能通常在 1B 以下急劇下降。
微小模型是否可以使用工具呼叫來抵消其有限容量,從而充當可行的助理,仍然是一個有趣但開放的問題。請參閱來自 [LiquidAI](https://huggingface.co/LiquidAI/LFM2-1.2B-Tool) 的模型,了解這個方向的最新工作。
在 Hugging Face,我們使用分層評估套件,呼應我們在預訓練的[消融實驗部分](https://huggingfacetb-smol-training-playbook.hf.space/#every-big-model-starts-with-a-small-ablation)中詳細說明的預訓練原則(單調性、低雜訊、高於隨機訊號、排名一致性)。
保持你的評估最新
要考慮的評估清單隨著模型的改善而不斷發展,下面討論的清單反映了我們在 2025 年中期的重點。有關後訓練評估的全面概述,請參閱[評估指南](https://github.com/huggingface/evaluation-guidebook/blob/main/yearly_dives/2025-evaluations-for-useful-models.md)。
以下是評估後訓練模型的多種方式:
  1. **能力評估**
這類評估針對基本技能,如推理和競爭性數學和編碼。
  - **知識。** 我們目前使用 GPQA Diamond ([Rein et al., 2024](https://huggingfacetb-smol-training-playbook.hf.space/#bib-gpqa)) 作為科學知識的主要評估。這個基準測試由研究生層級的多選題組成。對於小型模型,它遠未飽和,並且比 MMLU 及其同類提供更好的訊號,同時執行速度快得多。另一個良好的事實性測試是 SimpleQA ([Wei et al., 2024](https://huggingfacetb-smol-training-playbook.hf.space/#bib-simpleqa)),儘管小型模型由於其有限的知識而傾向於在這個基準測試上顯著掙扎。
  - **數學。** 為了衡量數學能力,今天大多數模型都在最新版本的 AIME(目前是 2025 版本)上進行評估。MATH-500 ([Lightman et al., 2023](https://arxiv.org/abs/2305.20050)) 對小型模型來說仍然是一個有用的健全性測試,但在很大程度上被推理模型飽和了。對於更全面的數學評估集,我們推薦來自 [MathArena](https://matharena.ai/) 的那些。
  - **程式碼。** 我們使用最新版本的 [LiveCodeBench](https://livecodebench.github.io/leaderboard.html) 來追蹤編碼能力。儘管針對競爭性程式設計問題,我們發現 LiveCodeBench 上的改善確實轉化為更好的編碼模型,儘管僅限於 Python。[SWE-bench Verified](https://openai.com/index/introducing-swe-bench-verified/) 是編碼技能的更複雜衡量標準,但對小型模型來說往往太難,因此不是我們通常考慮的。
  - **多語言性。** 不幸的是,在測試模型的多語言能力時沒有很多選擇。我們目前依賴 Global MMLU ([Singh et al., 2025](https://arxiv.org/abs/2412.03304)) 來針對我們的模型應該表現良好的主要語言,並包括 MGSM ([Shi et al., 2022](https://arxiv.org/abs/2210.03057)) 作為多語言數學能力的測試。
  1. **整合任務評估**
這些評估測試接近我們將交付的東西:多輪推理、長上下文使用以及半真實設置中的工具呼叫。
  - **長上下文。** 長上下文檢索最常用的測試是 Needle in a Haystack (NIAH) ([Kamradt, 2023](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)),其中隨機事實(「針」)被放置在長文件(「乾草堆」)的某處,模型必須檢索它。然而,這個基準測試太膚淺,無法區分長上下文理解,因此社群開發了更全面的評估,如 RULER ([Hsieh et al., 2024](https://arxiv.org/abs/2404.06654)) 和 HELMET ([Yen et al., 2025](https://arxiv.org/abs/2410.02694))。最近,OpenAI 發布了 [MRCR](https://huggingface.co/datasets/openai/mrcr) 和 [GraphWalks](https://huggingface.co/datasets/openai/graphwalks) 基準測試,擴展了長上下文評估的難度。
另請參閱這篇關於長上下文評估的局限性以及如何設計真實評估的[精彩部落格文章](https://nrehiew.github.io/blog/long_context/)。
  - **指令跟隨。** IFEval ([J. Zhou et al., 2023](https://arxiv.org/abs/2311.07911)) 目前是衡量指令跟隨的最流行評估,並針對「可驗證的指令」使用自動評分。IFBench ([Pyatkin et al., 2025](https://arxiv.org/abs/2507.02833)) 是來自 Ai2 的新擴展,其中包括比 IFEval 更多樣化的約束集,並減輕了最近模型發布中發生的一些基準測試最佳化。對於多輪指令跟隨,我們推薦 Multi-IF ([He et al., 2024](https://arxiv.org/abs/2410.15553)) 或 MultiChallenge ([Sirdeshmukh et al., 2025](https://arxiv.org/abs/2501.17399))。
  - **對齊。** 衡量模型與使用者意圖的對齊程度通常透過人類註釋者或公共排行榜(如 [LMArena](https://lmarena.ai/))完成。這是因為諸如自由形式生成、風格或整體幫助性等品質難以使用自動化指標定量衡量。然而,在所有情況下,執行這些評估都非常昂貴,這就是為什麼社群訴諸於使用 LLM 作為人類偏好的代理。這種風格最流行的基準測試包括 AlpacaEval ([Dubois et al., 2025](https://arxiv.org/abs/2404.04475))、ArenaHard ([T. Li et al., 2024](https://arxiv.org/abs/2406.11939)) 和 MixEval ([Ni et al., 2024](https://arxiv.org/abs/2406.06565)),後者與 LMArena 上的人類 Elo 評級具有最強的相關性。
  - **工具呼叫。** [BFCL](https://gorilla.cs.berkeley.edu/leaderboard.html) 提供了工具呼叫的全面測試,儘管通常很快就會飽和。TAU-Bench ([Barres et al., 2025](https://arxiv.org/abs/2506.07982)) 提供了模型在模擬客戶服務設置中使用工具和解決使用者問題的能力的測試,並且也成為了一個流行的報告基準測試。
  1. **防止過擬合評估**
為了測試我們的模型是否對特定技能過擬合,我們在我們的集合中包括一些穩健性或適應性評估,如 GSMPlus ([Q. Li et al., 2024](https://arxiv.org/abs/2402.19255)),它擾動來自 GSM8k ([Cobbe et al., 2021](https://arxiv.org/abs/2110.14168)) 的問題,以測試模型是否仍然可以解決類似難度的問題。
  1. **內部評估**
儘管公共基準測試可以在模型開發期間提供一些有用的訊號,但它們不能替代實作你自己的內部評估來針對特定能力,或要求內部專家與你的模型互動。
如果你正在建立 AI 產品,這一點尤其正確。有關此主題的具體建議,請參閱 Hamel Husain 的精彩[部落格文章](https://decodingml.substack.com/?utm_source=navbar&utm_medium=web)。
例如,對於 SmolLM3,我們需要一個基準測試來評估模型是否能夠進行_多輪推理_,因此我們實作了 Multi-IF 的變體來衡量這一點。
  1. **氛圍評估和競技場**
同樣,我們發現「氛圍測試」中間檢查點(即與你的模型互動)對於發現評估分數未捕獲的模型行為中的微妙怪癖至關重要。正如我們稍後討論的,氛圍測試發現了我們資料處理程式碼中的一個錯誤,其中所有系統訊息都從語料庫中刪除了!這也可以大規模進行以衡量人類偏好,就像在流行的 [LMArena](https://lmarena.ai/) 上一樣。然而,眾包人類評估往往是脆弱的(偏愛諂媚和華麗的言辭而不是實際的有用性),因此將其視為低訊號回饋很重要。
去污你的訓練資料
依賴公共基準測試的一個風險是模型很容易對它們過擬合,特別是當使用合成資料生成類似於目標基準測試的提示和回應時。因此,對你將用於指導模型開發的評估進行訓練資料去污至關重要。你可以使用 N-gram 匹配來執行此操作,使用 [Open-R1](https://github.com/huggingface/open-r1/blob/main/scripts/decontaminate.py) 中的腳本。
具體而言,對於 SmolLM3,我們希望有一個混合推理模型,可以可靠地遵循指令並在數學和程式碼等流行領域中進行良好推理。我們還希望確保我們保留了基礎模型的多語言性和長上下文檢索能力。
這導致我們得出以下評估集:
基準測試 | 類別 | 提示數量 | 指標
---|---|---|---
AIME25 | 競爭性數學 | 30 | avg@64
LiveCodeBench (v4 用於驗證,v5 用於最終發布) | 競爭性程式設計 | 100 (268) | avg@16
GPQA Diamond | 研究生層級推理 | 198 | avg@8
IFEval | 指令跟隨 | 541 | accuracy
MixEval Hard | 對齊 | 1000 | accuracy
BFCL v3 | 工具使用 | 4441 | mixed
Global MMLU (lite 用於驗證) | 多語言 Q&A | 590,000 (6,400) | accuracy
GSMPlus (mini 用於驗證) | 穩健性 | 10,000 (2,400) | accuracy
RULER | 長上下文 | 6,500 | accuracy

讓我們從每個中查看一些範例問題,以具體了解這些評估實際測試的內容:
瀏覽上面的範例以查看每個基準測試中的問題類型。注意領域的多樣性如何確保我們在整個消融實驗中測試模型能力的不同方面。
對於我們正在使用的 3B 模型規模,我們認為這些評估會給我們可操作的訊號,執行速度比訓練本身更快,並讓我們確信改進是真實的,而不僅僅是來自採樣的雜訊。我們還追蹤了我們的預訓練評估(完整清單請參閱[消融實驗部分](https://huggingfacetb-smol-training-playbook.hf.space/#every-big-model-starts-with-a-small-ablation)),以確保我們在基礎模型效能上沒有退化太多。
優先考慮你的評估
上面的故事表明我們作為一個團隊聚集在一起,在評估集上達成一致,並在訓練任何模型之前準備好它們。現實要混亂得多:我們有一個緊迫的截止日期,在許多上述評估實作之前就急於進行模型訓練(例如,RULER 直到模型發布前幾天才可用 🙈)。事後看來,這是一個**錯誤**,我們應該與預訓練團隊討論應該在後訓練中保留哪些核心評估,並優先在基礎模型完成訓練之前很久實作它們。換句話說,優先考慮你的評估,先於一切!

#### [參與規則](https://huggingfacetb-smol-training-playbook.hf.space/#rules-of-engagement-3)

讓我們用我們從評估數千個模型中獲得的一些來之不易的經驗教訓來總結這一節:
  - 在模型開發期間使用**小子集來加速評估**。例如,LiveCodeBench v4 與 v5 高度相關,但執行時間減半。或者,使用 tinyBenchmarks ([Polo et al., 2024](https://arxiv.org/abs/2402.14992)) 等方法,該方法尋求找到可靠匹配完整評估的最小提示子集。
  - 對於**推理模型**,從**評分**輸出中刪除思維鏈。這消除了誤報,也直接影響像 IFEval 這樣的基準測試,這些基準測試懲罰違反約束的回應,如「在 50 個字以下寫一首詩」。
  - 如果評估使用 **LLM judges,請固定 judge 和版本**,以便隨著時間的推移進行蘋果對蘋果的比較。更好的是,使用開放權重模型,以便即使提供者棄用 judge 模型,評估也是可重現的。
  - 警惕基礎模型中的**污染**。例如,在 AIME 2025 之前發布的大多數模型在 AIME 2024 上的效能要差得多,這表明存在一些基準測試最佳化。
  - 如果可能,將消融實驗期間使用的任何內容視為**驗證**,而不是**測試。** 這意味著為最終模型報告保留一組留出的基準測試,類似於 Tulu3 評估框架 ([Lambert et al., 2025](https://arxiv.org/abs/2411.15124))。
  - 始終包括一小組**「氛圍評估」**,在你自己的資料和任務上,以捕獲對公共套件的過擬合。
  - 對於問題數量較少的評估(通常少於 ~2k),採樣 `k` 次並報告 `avg@k` 準確度。這對於減輕雜訊很重要,雜訊可能導致開發期間的錯誤決定。
  - 在實作新評估時,確保你**可以複製幾個模型的已發布結果**(在某個誤差範圍內)。如果你需要修復實作並重新評估許多檢查點,未能做到這一點將浪費大量時間。
  - 如有疑問,請始終返回評估資料,特別是檢查你用模型提示的內容
有了評估在手,是時候訓練一些模型了!在這樣做之前,我們首先需要選擇一個後訓練框架。
