### [在策略訓練和超越監督標籤](https://huggingfacetb-smol-training-playbook.hf.space/#going-on-policy-and-beyond-supervised-labels)

如果你希望你的模型一致地解決數學問題、生成可執行程式碼或跨多個步驟規劃,你通常需要一個**獎勵訊號**而不僅僅是「A 優於 B」。
這就是 RL 開始有意義的地方。你不是用偏好監督模型,而是讓它與環境互動(可能是數學驗證器、程式碼執行器,甚至真實使用者回饋),並直接從結果中學習。RL 在以下情況下發光:
  - **你可以自動檢查正確性,** 例如,單元測試、數學證明、API 呼叫,或者可以存取高品質驗證器或獎勵模型。
  - **任務需要多步驟推理或規劃**,其中局部偏好可能無法捕獲長期成功。
  - **你想優化超越偏好標籤的目標**,如透過程式碼的單元測試或最大化某些目標。
當涉及到 LLM 時,有兩種主要的 RL 風格:
  - **Reinforcement Learning from Human Feedback (RLHF):** 這是 OpenAI 的 InstructGPT 論文 ([Ouyang et al., 2022](https://arxiv.org/abs/2203.02155)) 流行的方法,也是 gpt-3.5 和許多現代 LLM 的基礎。在這裡,人類註釋者比較模型輸出(例如「A 優於 B」),並訓練獎勵模型來預測這些偏好。然後使用 RL 微調策略以最大化學習的獎勵。
因為獎勵模型只近似人類偏好,它有時會鼓勵獎勵駭客,其中策略發出像「the the the the」這樣的分布外序列,這被賦予虛假的高獎勵,然後透過 RL 迴圈烘焙到模型中。
  - **Reinforcement Learning with Verifiable Rewards (RLVR):** 這是 DeepSeek-R1 流行的方法,涉及使用驗證器來檢查模型的輸出是否符合某些明確定義的正確性標準(例如,程式碼是否編譯並透過所有測試,或數學答案是否正確?)。然後使用 RL 微調策略以產生更多可驗證正確的輸出。
RLHF 和 RLVR 都定義了模型正在優化的_內容_,但它們沒有告訴我們應該如何_執行_該優化。在實踐中,基於 RL 的訓練的效率和穩定性在很大程度上取決於學習演算法是**在策略**還是**離策略**。
像 GRPO 這樣的方法通常屬於在策略優化演算法的類別,其中生成完成的模型(策略)與正在優化的模型相同。雖然 GRPO 廣義上是一個在策略演算法,但有一些注意事項。首先,為了優化生成步驟,可能會採樣幾個批次的生成,然後對模型進行 k 次更新,第一個批次與下幾個批次在策略上,後幾個批次略微離策略。
為了解決用於生成的模型與正在優化的當前模型之間的策略延遲,使用重要性採樣和裁剪來重新加權 token 機率並限制更新的大小。
我們在這裡提到離策略 RL,但有幾個真正的離策略 RL 演算法,如 Q-learning,其中用於生成軌跡的策略可以與正在優化的策略完全不同。當 GRPO 應用於 LLM 時,用於生成的策略可能落後於用於優化的策略,但通常兩者之間的步驟差異少於 16。
由於來自 LLM 的自回歸生成很慢,許多框架(如 [verl](https://github.com/volcengine/verl) 和 [PipelineRL](https://github.com/ServiceNow/PipelineRL))已新增非同步生成完成和模型權重的「飛行中」更新,以最大化訓練輸送量。這些方法需要更複雜和仔細的實作,但可以達到比同步訓練方法高 4-5 倍的訓練速度。正如我們稍後將看到的,這些訓練效率的改善對於推理模型特別明顯,推理模型具有長尾 token 分布。
對於 SmolLM3,我們完全跳過了 RL,主要是由於時間限制以及擁有一個已經使用離線偏好優化處於最佳狀態的模型。然而,自發布以來,我們重新審視了這個主題,並將透過分享我們將 RLVR 應用於混合推理模型的一些經驗教訓來結束後訓練章節。

#### [將 RLVR 應用於混合推理模型](https://huggingfacetb-smol-training-playbook.hf.space/#applying-rlvr-to-hybrid-reasoning-models)

混合推理模型為 RLVR 帶來了額外的複雜性,因為生成長度根據推理模式變化很大。例如,在下圖中,我們繪製了來自 SmolLM3 的 [final APO checkpoint](https://huggingface.co/HuggingFaceTB/SmolLM3-3B-checkpoints/tree/it-soup-APO) 在 AIME25 上的 token 長度分布:
如你所見,`/no_think` 模式生成中位數長度約為 2k tokens 的解決方案,而 `/think` 模式要大得多,有 16k tokens 和一個肥尾分布。理想情況下,我們希望使用 RLVR 改善兩種模式的整體效能,而不會太激進地改變它們各自的長度分布。
為了探索這一點,我們首先專注於優化 `/no_think` 模式,並從 [Big-Math](https://huggingface.co/datasets/SynthLabsAI/Big-Math-RL-Verified) 中取了一個提示子集,這是一個包含超過 250k 數學問題和驗證答案的資料集。
令我們驚訝的是,天真地應用 GRPO 會導致一種形式的_獎勵駭客_:儘管從未被提示發出長 CoT,模型學會利用其基礎推理能力來增加獎勵。如下圖所示,不僅獎勵上升,完成的長度也上升:
天真地將 RLVR 應用於 SmolLM3 會達到高獎勵和改善的下游效能,但代價是完成長度爆炸。
換句話說,使用 GRPO 的 RLVR 已經將我們推理模型的 `/no_think` 模式轉換為看起來與 `/think` 模式非常相似的模式!
當我們審查模型的完成時,我們看到模型現在生成長 CoT,甚至包括一些與推理模型相關的認知行為 ([Gandhi et al., 2025](https://arxiv.org/abs/2503.01307)),如「Wait, ...」。以下顯示了一個範例:

```

However, since the jogger and the train are moving in the same direction, the relative speed between them is the difference of their speeds:
\\( v_{\\text{rel}} = v_t - v_j = 12.7778 - 2.7778 = 10 \\) m/s
 **Wait,**  let me double-check the calculation:
 **Wait,**  46 km/hr to m/s:
46 * 1000 m / 3600 s = (46 * 10)/36 = 460/36 ≈ 12.7778 m/s (correct)
10 km/hr is:
10 * 1000 / 3600 = 10000 / 3600 ≈ 2.7778 m/s (correct)
Difference: 12.7778 - 2.7778 = 10 m/s (exact, since 46-10=36 km/hr → 36*(1000/3600)=10 m/s exactly)
The jogger is 340 meters ahead of the train's engine. When the train passes the jogger, the distance the train needs to cover relative to the jogger is 340 meters plus the length of the train? Wait, actually, let me think about the problem again.
 **Wait,**  actually, when the problem says the train takes 46 seconds to pass the jogger, that usually means the time from the front of the train to the end passing the jogger. The length of the train would then be equal to the relative speed multiplied by the time.
But first, the initial position: the jogger is 340 meters ahead of the engine. So the engine is 340 meters ahead of the jogger. Wait no, the wording says "340 m ahead of the engine", so the jogger is ahead by 340 meters. So when the jogger is running alongside, the engine of the train is 340 m away behind the jogger? Wait, maybe I need to clarify.
 **Wait**  the problem says: "A jogger running at 10 km/hr alongside a railway track is 340 m ahead of the engine of a train running at 46 km/hr in the same direction. The train takes 46 seconds to pass the jogger. How long is the train in meters?"
Hmm, so the jogger is 340 m ahead of the engine along the track. Since they're moving in the same direction, the train is behind the jogger by 340 meters. To pass the jogger, the train has to cover that 340 meters plus the length of the train itself?

```

**使用過長懲罰減輕獎勵駭客**
這個問題可以透過包含_過長完成懲罰_來減輕,該懲罰懲罰超過一定長度的完成。該懲罰由兩個參數參數化:最大完成長度 Lmax 和軟懲罰快取 Lcache。這種懲罰是 DAPO 論文 ([Yu et al., 2025](https://arxiv.org/abs/2503.14476)) 中提出的改進之一,相當於應用如下獎勵函式:
Rlength(y)={0,∣y∣≤Lmax−Lcache(Lmax−Lcache−∣y∣)Lcache,Lmax−Lcache<∣y∣≤Lmax−1,Lmax<∣y∣

使用這種懲罰,我們可以直接控制模型的輸出分布,並衡量增加回應長度和效能之間的權衡。下圖顯示了一個範例,其中我們以 512 tokens 的步長將過長懲罰從 1.5k 變化到 4k:
應用過長懲罰約束每個展開的長度,同時也降低平均獎勵。
當我們檢查 AIME25 上的改善時,回應長度和效能之間的權衡更清楚:
使用 RLVR 的 Smollm3 在 AIME25 上的下游效能。
現在我們可以清楚地看到過長懲罰如何影響下游效能,範圍在 2-4k 的懲罰產生顯著改善,同時保持 token 分布在控制之下。如下圖所示,如果我們從步驟 400 取檢查點,我們可以比較初始策略和最終模型之間跨一系列不同懲罰的輸出 token 分布:
**將所有內容整合在一起**
我們發現在 2.5-3k 範圍內應用長度懲罰在效能和回應長度之間提供了最佳權衡,下圖顯示 GRPO 幾乎將 AIME 2025 上的效能比 APO 等離線方法翻倍:
現在我們知道如何改善 `/no_think` 推理模式的效能,RL 訓練管道的下一步將是同時在兩種推理模式下對模型進行_聯合訓練_。然而,我們發現這是一個相當難啃的硬骨頭,因為每種模式都需要自己的長度懲罰,到目前為止,相互作用產生了不穩定的訓練。這突出了嘗試將 RL 應用於混合推理模型的主要挑戰,我們可以看到這反映在像 Qwen 這樣的模型開發者分別發布 [instruct](https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507) 和 [reasoning](https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507) 變體的新趨勢中。
我們的實驗表明,RLVR 可以有效地引導推理行為,但只有仔細的獎勵塑造和穩定性機制。鑑於這種複雜性,值得詢問強化學習是否是唯一可行的前進路徑。事實上,最近文獻中已經提出了幾種更輕量級的在策略優化策略,但仍然被開源社群驚人地探索不足。讓我們透過看一些來結束本章。

#### [RL 是唯一的遊戲嗎?](https://huggingfacetb-smol-training-playbook.hf.space/#is-rl-the-only-game-in-town)

其他在策略學習的方法將偏好優化和蒸餾擴展到迭代迴圈中,隨著模型發展刷新訓練訊號:
  - **Online DPO:** 不是在固定偏好資料集上訓練一次,模型持續採樣新回應,收集新鮮的偏好標籤(來自獎勵模型或 LLM 評分者),並更新自己。這使優化保持_在策略上_,並減少訓練資料與模型當前行為之間的漂移 ([Guo et al., 2024](https://arxiv.org/abs/2402.04792))。
  - **在策略蒸餾:** 訊號不是來自偏好,而是來自更強的教師模型。學生在每個訓練步驟採樣回應,這些樣本上學生和教師 logits 之間的 KL 散度提供學習訊號。這允許學生持續吸收教師的能力,而無需明確的偏好標籤或驗證器 ([Agarwal et al., 2024](https://arxiv.org/abs/2306.13649))。
這些方法模糊了靜態偏好優化和完整 RL 之間的界線:你仍然獲得適應模型當前分布的好處,但沒有設計和穩定強化學習迴圈的全部複雜性。

#### [我選擇哪種方法?](https://huggingfacetb-smol-training-playbook.hf.space/#which-method-do-i-pick)

儘管有無數研究論文關於哪種在策略方法是「最佳」的,但在實踐中,決定取決於下表中顯示的幾個因素:
演算法 | 何時使用 | 權衡 | 最佳模型大小
---|---|---|---
**Online DPO** | 你可以便宜地獲得偏好標籤。最適合將行為與不斷發展的分布對齊。 | 易於迭代擴展,比 RL 更穩定,但取決於標籤品質和覆蓋範圍。在少數訓練框架中支援。 | 任何大小,其中偏好捕獲超越模仿的改善。
**在策略蒸餾** | 你可以存取更強的教師模型,並希望有效地轉移能力。 | 實作簡單,執行便宜,繼承教師偏見,上限受教師限制。僅在 TRL 和 NemoRL 中支援 | 對小到中型模型(<30B)最有效。
**強化學習** | 當你有可驗證的獎勵或需要多步驟推理/規劃的任務時最好。可以與獎勵模型一起使用,但存在獎勵駭客等挑戰,其中模型利用獎勵模型的弱點。 | 靈活且強大,但成本高且更難穩定;需要仔細的獎勵塑造。在大多數後訓練框架中支援。 | 中型到大型模型(20B+),其中額外容量讓它們利用結構化獎勵訊號。

在開源生態系統中,像 GRPO 和 REINFORCE 這樣的強化學習方法往往是最廣泛使用的,儘管 Qwen3 技術報告 ([A. Yang, Li, et al., 2025](https://arxiv.org/abs/2505.09388)) 強調使用在策略蒸餾來訓練 32B 參數以下的模型:

```

flowchart LR
    subgraph Flagship ["Flagship Models"]
        Base1["Base Models"] --> Stage1["Stage 1:
Long-CoT Cold Start"]
        Stage1 --> Stage2["Stage 2:
Reasoning RL"]
        Stage2 --> Stage3["Stage 3:
Thinking Mode Fusion"]
        Stage3 --> Stage4["Stage 4:
General RL"]
        Stage4 --> FlagshipOut["Qwen3-235B-A22B
Qwen3-32B"]
    end
       subgraph Lightweight ["Lightweight Models"]
        Base2["Base Models"] --> Distillation["Strong-to-Weak
Distillation"]
        FlagshipOut --> Distillation
        Distillation --> LightweightOut["Qwen3-30B-A3B
14B/8B/4B/1.7B/0.6B"]
    end
       classDef flagshipStage fill:#ffd0c5
    classDef lightweightStage fill:#fef3c7
    classDef output fill:#f8d7da
       class Stage1,Stage2,Stage3,Stage4 flagshipStage
    class Distillation lightweightStage
    class FlagshipOut,LightweightOut output

```

小型模型的在策略蒸餾的一個有趣特性是,它通常以計算成本的一小部分優於基於 RL 的方法。這是因為我們不是為每個提示生成多個展開,而是只採樣一個,然後由教師在單個前向-後向傳遞中評分。正如 Qwen3 技術報告所示,比 GRPO 的收益可能是顯著的:
方法 | AIME'24 | AIME'25 | MATH500 | LiveCodeBench v5 | MMLU -Redux | GPQA -Diamond | GPU Hours
---|---|---|---|---|---|---|---
Off-policy Distillation | 55.0 | 42.8 | 92.4 | 42.0 | 86.4 | 55.6 | -
+ Reinforcement Learning | 67.6 | 55.5 | 94.8 | 52.9 | 86.9 | 61.3 | 17,920
+ On-policy Distillation | **74.4** | **65.5** | **97.0** | **60.3** | **88.3** | **63.3** | 1,800

最近,[Thinking Machines](https://thinkingmachines.ai/blog/on-policy-distillation/) 表明,在策略蒸餾在減輕_災難性遺忘_方面也很有效,其中後訓練模型在新領域上進一步訓練,其先前效能退化。在下表中,他們顯示,儘管 Qwen3-8b(IFEval)的聊天效能在內部資料上微調時崩潰,但行為可以透過便宜的蒸餾恢復:
![Image](https://huggingfacetb-smol-training-playbook.hf.space/_astro/Screenshot_2025-10-30_at_13_02_36_29c1384e-bcac-80d6-a72d-ff34bc221b60.CWwwpiVT_1VFoBG.webp)
我們自己對在策略蒸餾非常興奮,因為有大量能力強大的開放權重 LLM 可以蒸餾成更小的、特定任務的模型。然而,所有在策略蒸餾方法的一個弱點是教師和學生必須共享相同的 tokenizer。為了解決這個問題,我們開發了一種名為 General On-Policy Logit Distillation (GOLD) 的新方法,它允許任何教師蒸餾到任何學生。如果你對這些主題感興趣,我們建議查看我們的[技術文章](https://huggingface.co/spaces/HuggingFaceH4/on-policy-distillation)。
同樣,FAIR 的研究人員比較了 DPO 完全離策略與在策略的效果,並表明使用遠少的計算可以匹配 GRPO 的效能 ([Lanchantin et al., 2025](https://arxiv.org/abs/2506.21495)):
syncweights
Rewards
Reward/Verifier
Model
Rollouts
Trainer (T)
Generator (G)
Prompt
Offline (s=∞)
Step k+1 T G
Step k T G
Step 1 T G
Step 0 T sync ⟶ G
Semi-Online (s=k)
Step k+1 T G
Step k T sync ⟶ G
Step 1 T G
Step 0 T sync ⟶ G
Online (s=1)
Step k+1 T sync ⟶ G
Step k T sync ⟶ G
Step 1 T sync ⟶ G
Step 0 T sync ⟶ G
如他們的論文所示,在線 DPO 對數學任務效果很好,即使半在策略變體在離策略許多步驟的情況下也達到了可比的效能:
訓練方法 | Math500 | NuminaMath | AMC23
---|---|---|---
Seed (Llama-3.1-8B-Instruct) | 47.4 | 33.9 | 23.7
Offline DPO (s = ∞) | 53.7 | 36.4 | 28.8
Semi-online DPO (s = 100) | **58.9** | 39.3 | **35.1**
Semi-online DPO (s = 10) | 57.2 | 39.4 | 31.4
Online DPO (s = 1) | 58.7 | **39.6** | 32.9
GRPO | 58.1 | 38.8 | 33.6

總體而言,我們認為在有效擴展 RL ([Khatri et al., 2025](https://arxiv.org/abs/2510.13786)) 和探索其他計算效率方法方面仍有許多工作要做。確實是令人興奮的時代!
