## [超越基礎模型 — 2025 年的後訓練](https://huggingfacetb-smol-training-playbook.hf.space/#beyond-base-models--post-training-in-2025)

> 一旦預訓練完成,我們應該在一天內有一個 SFT 基線
> Lewis Tunstall,樂觀的 LLM 專家。
後訓練冒險
Base modelSFTORPODPO and friendsRL (here be dragons)KTO
選擇你自己的(後訓練)冒險。
預訓練給了我們 SmolLM3 的原始能力,但在 GPU 冷卻下來之前,我們進入了模型能力的下一個前沿:_後訓練_。這包括監督式微調、強化學習、模型合併等等 — 所有這些都旨在彌合「預測文本的模型」與「人們實際可以使用的模型」之間的差距。如果預訓練是關於將知識強行塞入權重,後訓練是關於將原始能力雕刻成可靠和可控的東西。就像預訓練一樣,精美的後訓練論文沒有捕捉到深夜的驚喜:GPU 崩潰、挑剔的資料混合,或者看似微小的聊天模板決定如何在下游基準測試中產生連鎖反應。在本節中,我們將展示我們如何在混亂的後訓練世界中導航,將 SmolLM3 從強大的基礎模型變成最先進的混合推理器。
什麼是混合推理模型?
混合推理模型在兩種不同的模式下運作:一種用於簡潔、直接的回應,另一種用於擴展的逐步推理。通常,操作模式由使用者在系統訊息中設定。遵循 Qwen3,我們使用輕量級命令明確表示這一點:「/think」呼叫擴展推理,而「/no_think」強制執行簡潔回答。這樣,使用者控制模型是優先考慮深度還是速度。

