### 選擇基準線

每個成功的模型都建立在經過驗證的基礎之上，並根據需求進行修改。當 Qwen 訓練他們的第一個模型家族時（[Bai et al., 2023](https://arxiv.org/abs/2309.16609)），他們從 Llama 的架構開始。當 Meta 訓練 Llama 3 時，他們從 Llama 2 開始。Kimi K2 從 DeepSeek-V3 的 MoE 架構開始。這不僅適用於架構，也適用於訓練超參數和優化器。

為什麼？好的架構和訓練設置設計需要多個組織多年的迭代。標準的 transformer 和像 Adam 這樣的優化器已經透過數千次實驗得到改進。人們已經發現了它們的失敗模式、除錯了不穩定性、優化了實作。從經過驗證的基礎開始意味著繼承所有這些累積的知識。從頭開始意味著自己重新發現每個問題。

以下是一個好的架構起點所具備的特質：
- **符合你的限制**：與你的部署目標和用例對齊
- **經過大規模驗證**：在相似或更大規模上進行了數萬億 token 的訓練
- **文件完善**：已知的超參數在開源模型中被證明有效
- **框架支援**：理想情況下應該在你考慮的訓練框架和計劃使用的推理框架中得到支援

以下是 2025 年各種架構和模型大小的強大基準選項的非詳盡清單：

| 架構類型 | 模型家族 | 大小 |
|---|---|---|
| **密集型** | [Llama 3.1](https://huggingface.co/collections/meta-llama/llama-31-669fc079a0c406a149a5738f) | 8B, 70B |
| **密集型** | [Llama 3.2](https://huggingface.co/collections/meta-llama/llama-32-66f448ffc8c32f949b04c8cf) | 1B, 3B |
| **密集型** | [Qwen3](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f) | 0.6B, 1.7B, 4B, 14B, 32B |
| **密集型** | [Gemma3](https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d) | 12B, 27B |
| **密集型** | [SmolLM2](https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9), [SmolLM3](https://huggingface.co/HuggingFaceTB/SmolLM3-3B) | 135M, 360M, 1.7B, 3B |
| **MoE** | [Qwen3 MoE](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f) | 30B-A3B, 235B-A122B |
| **MoE** | [GPT-OSS](https://huggingface.co/collections/openai/gpt-oss-68911959590a1634ba11c7a4) | 21B-A3B, 117B-A5B |
| **MoE** | [Kimi Moonlight](https://huggingface.co/moonshotai/Moonlight-16B-A3B-Instruct) | 16B-A3B |
| **MoE** | [Kimi-k2](https://huggingface.co/collections/moonshotai/kimi-k2-6871243b990f2af5ba60617d) | 1T-A32B |
| **MoE** | [DeepSeek V3](https://huggingface.co/deepseek-ai/DeepSeek-V3) | 671B-A37B |
| **混合型** | [Zamba2](https://huggingface.co/Zyphra/models?search=zamba2) | 1.2B, 2.7B, 7B |
| **混合型** | [Falcon-H1](https://huggingface.co/collections/tiiuae/falcon-h1-6819f2795bc406da60fab8df) | 0.5B, 1.5B, 3B, 7B, 34B |
| **MoE + 混合** | [Qwen3-Next](https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct) | 80B-A3B |
| **MoE + 混合** | [MiniMax-01](https://huggingface.co/MiniMaxAI/MiniMax-Text-01) | 456B-A46B |

所以去你的架構類型那裡，選擇一個接近你想要的模型參數數量的基準線。不要想太多，因為你開始的架構並非一成不變。在下一節中，我們將看到如何從基準線到最適合你的最終架構。

#### 修改基準線：降低風險的紀律

現在你有了一個有效且符合你用例的基準線。你可以在這裡停下來，在你的資料混合上訓練它（假設它很好），很可能會得到一個不錯的模型。許多成功的專案正是這樣做的。但基準線並未針對你的特定限制進行優化，它們是為構建它們的人的用例和部署目標而設計的。因此，很可能有值得做的修改來更好地與你的目標對齊。然而，每個架構變更都帶有風險：它可能提升效能、破壞效能，或者什麼都不做，同時浪費你的消融實驗算力。

讓你保持在正軌上的紀律是**降低風險**：除非你測試過有幫助，否則永遠不要改變任何東西。

什麼算是降低了風險？

當測試顯示一個變更要麼改善了你目標能力的效能，要麼提供了有意義的好處（例如更快的推理、更低的記憶體、更好的穩定性），而不會在你可接受的權衡之外損害效能時，這個變更就算降低了風險。

棘手的部分是，你的基準線和訓練設置有許多你可以修改的組件：注意力機制、位置編碼、啟動函數、優化器、訓練超參數、正規化方案、模型佈局等等。每個都代表一個潛在的實驗，而這些組件通常以非線性方式互動。你既沒有時間也沒有算力來測試所有東西或探索每個互動。

從針對當前基準線測試有前景的變更開始。當某個變更有效時，將其整合以建立新的基準線，然後針對該基準線測試下一個變更。如果你的算力預算允許，你可以單獨測試變更並執行留一法分析。

查看 ScaleRL 論文（[Khatri et al., 2025](https://arxiv.org/abs/2510.13786)）來看這個方法論在實踐中的例子。

> 💡 不要陷入對每個超參數進行詳盡網格搜尋或測試每個出現的架構變體的陷阱。

**戰略性實驗**

如果你不知道哪些實驗值得執行，知道如何執行實驗是不夠的。在測試任何修改之前問自己兩個問題：
- 這會幫助我的特定用例嗎？
- 這會優化我的訓練嗎？

如果一個修改沒有明確解決任何一個問題，就跳過它。

現在你知道如何透過戰略規劃識別什麼是有前景的，是時候進入**經驗驗證**了。在接下來的章節中，我們將向你展示_如何_在實踐中實際測試這些變更。我們將介紹如何設置可靠的實驗、解釋結果以及避免常見陷阱。然後在接下來的章節中，我們將透過具體例子來測試流行的架構、資料、基礎設施和訓練決策。

所以讓我們建立一個簡單的消融實驗設置，用於我們的實驗。首先，我們需要決定選擇哪個訓練框架。
