### 消融實驗設置

有了框架選擇，我們現在需要設計消融實驗設置。我們需要足夠快速的實驗來快速迭代,但也要足夠大,讓結果能給我們訊號並遷移到最終模型。讓我們看看如何設置。

#### 建立我們的消融實驗框架

消融實驗的目標是在小規模執行實驗,並獲得我們可以自信地推斷到最終生產執行的結果。

有兩種主要方法。首先,我們可以採用目標模型大小並用較少的 token 訓練它。對於 SmolLM3 消融實驗,我們在 100B token 上訓練完整的 3B 模型,而不是最終的 11T。其次,如果我們的目標模型太大,我們可以訓練一個較小的代理模型進行消融實驗。例如,當 Kimi 開發他們的 1T 參數 Kimi K2 模型(具有 32B 啟動參數)時,對所有消融實驗使用完整大小會過於昂貴,因此他們在 3B MoE 上執行了一些消融實驗,具有 0.5B 啟動參數([Team et al., 2025](https://arxiv.org/abs/2507.20534))。

一個關鍵問題是這些小規模發現是否真的能遷移。根據我們的經驗,如果某件事在小規模上損害了效能,你可以自信地在大規模上排除它。但如果某件事在小規模上有效,你仍然應該確保你已經在合理數量的 token 上訓練,以高機率得出這些發現將推斷到更大規模的結論。你訓練的時間越長,消融實驗模型越接近最終模型,效果越好。

在這篇部落格文章中,我們將對所有消融實驗使用基準線普通 transformer。我們的主要設置是一個 1B transformer,遵循 [Llama3.2 1B](https://huggingface.co/meta-llama/Llama-3.2-1B) 架構,在 45B token 上訓練。使用這個 nanotron [配置](https://huggingface.co/datasets/HuggingFaceTB/training-guide-nanotron-configs/blob/main/baseline_config_1B.yaml),在 8xH100s 節點上訓練大約需要 1.5 天(每個 GPU 每秒 42k token)。在 SmolLM3 訓練期間,我們在 3B 模型上執行這些消融實驗,在 100B token 上訓練(配置[在此](https://huggingface.co/datasets/HuggingFaceTB/training-guide-nanotron-configs))。我們將在每章結尾分享這些結果(你會看到結論是一致的)。

我們訓練 45B token 以確保我們獲得穩定的訊號,儘管約 35B 對於這個模型大小來說是 [Chinchilla 最佳](https://arxiv.org/abs/2203.15556)。

我們的基準線 1B 配置以結構化的 YAML 格式捕捉了所有重要的訓練細節。以下是關鍵部分:

```yaml
## 資料集和混合權重

data_stages:
- data:
    dataset:
      dataset_folder:
      - fineweb-edu
      - stack-edu-python
      - finemath-3plus
      dataset_weights:
      - 0.7
      - 0.2
      - 0.1

## 模型架構,Llama3.2 1B 配置

model:
  model_config:
    hidden_size: 2048
    num_hidden_layers: 16
    num_attention_heads: 32
    num_key_value_heads: 8
    intermediate_size: 8192
    max_position_embeddings: 4096
    rope_theta: 50000.0
    tie_word_embeddings: true

## 訓練超參數,AdamW 與 cosine 排程

optimizer:
  clip_grad: 1.0
  learning_rate_scheduler:
    learning_rate: 0.0005
    lr_decay_starting_step: 2000
    lr_decay_steps: 18000
    lr_decay_style: cosine
    lr_warmup_steps: 2000
    lr_warmup_style: linear
    min_decay_lr: 5.0e-05
  optimizer_factory:
    adam_beta1: 0.9
    adam_beta2: 0.95
    adam_eps: 1.0e-08
    name: adamW

## 並行,1 個節點

parallelism:
  dp: 8  # 跨 8 個 GPU 的資料並行
  tp: 1  # 在 1B 規模不需要 tensor 或 pipeline 並行
  pp: 1

## Tokenizer

tokenizer:
  tokenizer_max_length: 4096
  tokenizer_name_or_path: HuggingFaceTB/SmolLM3-3B

## 批次大小、序列長度和 30B token 的總訓練

tokens:
  batch_accumulation_per_replica: 16
  micro_batch_size: 3 # GBS (global batch size)=dp * batch_acc* MBS * sequence=1.5M tokens
  sequence_length: 4096
  train_steps: 20000 # GBS * 20000 = 30B
```

對於我們的消融實驗,我們將根據測試內容修改不同的部分,同時保持其他所有內容不變:針對[架構選擇](https://huggingfacetb-smol-training-playbook.hf.space/#architecture-choices)修改 `model` 部分,針對[優化器和訓練超參數](https://huggingfacetb-smol-training-playbook.hf.space/#optimiser-and-training-hyperparameters)修改 `optimizer` 部分,針對[資料策劃](https://huggingfacetb-smol-training-playbook.hf.space/#the-art-of-data-curation)修改 `data_stages` 部分。

**一次修改一件事**

每次消融實驗只改變一個變數,同時保持其他所有內容不變。如果你改變多件事並且效能提升,你將不知道是什麼導致的。單獨測試修改,然後組合成功的修改並重新評估。

執行消融實驗時,某些架構變更可能會顯著改變參數數量。例如,從綁定嵌入切換到非綁定嵌入會使我們的嵌入參數翻倍,而從 MHA 到 GQA 或 MQA 會大幅減少我們的注意力參數。為了確保公平比較,我們需要追蹤參數數量,並偶爾調整其他超參數(如隱藏大小或層數)以保持模型大小大致相同。以下是我們用於估計不同配置參數數量的簡單函數:

```python
from transformers import LlamaConfig, LlamaForCausalLM

def count_parameters(tie_embeddings=True,
    num_key_value_heads=4,
    num_attention_heads=32,
    hidden_size=2048,
    num_hidden_layers=16,
    intermediate_size=8192,
    vocab_size=128256,
    sequence_length=4096,):
    config = LlamaConfig(hidden_size=hidden_size,
        num_hidden_layers=num_hidden_layers,
        num_attention_heads=num_attention_heads,
        num_key_value_heads=num_key_value_heads,
        intermediate_size=intermediate_size,
        vocab_size=vocab_size,
        max_position_embeddings=sequence_length,
        tie_word_embeddings=tie_embeddings,
   )
    model = LlamaForCausalLM(config)
    return f"{sum(p.numel() for p in model.parameters())/1e9:.2f}B"
```

我們還提供了一個互動式工具來視覺化 LLM 參數分布,針對密集型 transformer。這在做出架構決策或設置消融實驗配置時非常方便。

**詞彙表大小** 128k
**隱藏大小** 2048
**層數** 16
**注意力頭數** 32
**KV 頭數** 32
**中間大小** 8192
**綁定嵌入** 是/否

**1.34 B 參數**

**嵌入**
輸入 + 輸出投影
262 M
128k × 2048
vocab_size × hidden_size

**注意力層**
Q, K, V, O 投影
268 M
16 × 2048² × 4
layers × hidden_size² × 4

**前饋網路**
Up, Gate, Down 投影
805 M
16 × 2048 × 8192 × 3
layers × hidden_size × intermediate_size × 3

**層正規化**
輸入 + 注意力正規化
68 K
16 × 2048 × 2
layers × hidden_size × 2

**注意力類型** MHA
**嵌入策略** 綁定
**每層參數** 67 M

> 這個計算器假設標準架構選擇:閘控前饋網路、注意力的標準頭維度(hidden_size / num_heads)和每個 transformer 層 2 個層正規化。它不包括偏置項(如果使用)。

#### 理解什麼有效:評估

一旦我們啟動消融實驗,我們如何知道什麼有效或沒效?

任何訓練模型的人的第一直覺可能是查看損失,是的,這確實很重要。你想看到它平穩下降,沒有劇烈的尖峰或不穩定。對於許多架構選擇,損失與下游效能相關良好,可能就足夠了([Y. Chen et al., 2025](https://arxiv.org/abs/2410.08527))。然而,僅僅看損失並不總是可靠的。以資料消融實驗為例,你會發現在 Wikipedia 上訓練比在網頁上訓練給出更低的損失(下一個 token 更容易預測),但這並不意味著你會得到更強大的模型。同樣,如果我們在執行之間改變 tokenizer,損失就不能直接比較,因為文本被不同地分割。某些變更也可能專門影響某些能力,如推理和數學,並在平均損失中被沖淡。最後但同樣重要的是,即使預訓練損失已經收斂,模型仍然可以繼續改進下游任務([Liu et al., 2022](https://arxiv.org/abs/2210.14199))。

我們需要更細緻的評估來看到全貌並理解這些微妙的影響,一個自然的方法是使用下游評估來測試知識、理解、推理以及對我們重要的任何其他領域。

對於這些消融實驗,最好專注於提供良好早期訊號的任務並避免雜訊基準測試。在 [FineTasks](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fine-tasks) 和 [FineWeb2](https://arxiv.org/pdf/2506.20920) 中,可靠的評估任務由四個關鍵原則定義:

- **單調性:** 隨著模型訓練時間延長,基準測試分數應該持續改進。
- **低雜訊:** 當我們使用相同設置但不同隨機種子訓練模型時,基準測試分數不應該劇烈變化。
- **高於隨機效能:** 許多能力只在訓練後期出現,因此在延長時間內顯示隨機級別效能的任務對消融實驗沒有用處。例如,多選格式的 MMLU 就是這種情況,我們稍後會解釋。
- **排名一致性:** 如果一種方法在早期階段優於另一種,這種排序應該在訓練繼續時保持穩定。

任務的品質也取決於任務形式(我們如何向模型提問)和指標選擇(我們如何計算答案分數)。

三種常見的任務形式是多選格式(MCF)、完形填空(CF)和自由生成(FG)。多選格式要求模型從提示中明確呈現並以 A/B/C/D 為前綴的多個選項中選擇一個(例如 MMLU 中所做的)。在完形填空中,我們比較不同選項的可能性,看哪一個更可能,而不在提示中提供它們。在 FG 中,我們查看給定提示的貪婪生成的準確性。FG 需要模型中的大量潛在知識,通常在完整訓練之前對於短預訓練消融實驗來說太困難而無法真正有用。因此,我們在執行小規模消融實驗時專注於多選形式(MCF 或 CF)。

> ⚠️ **注意**
>
> 對於後訓練模型,FG 成為主要形式,因為我們正在評估模型是否實際上可以生成有用的回應。我們將在[後訓練章節](https://huggingfacetb-smol-training-playbook.hf.space/#beyond-base-models--post-training-in-2025)中涵蓋這些模型的評估。

研究還表明,模型在訓練早期與 MCF 鬥爭,只有在大量訓練後才學會這項技能,使 CF 對早期訊號更好([Du et al., 2025](https://arxiv.org/abs/2403.15796); [Gu et al., 2025](https://arxiv.org/abs/2406.08446); J. [Li et al., 2025](https://arxiv.org/abs/2406.11794))。因此,我們對小型消融實驗使用 CF,並在主要執行中整合 MCF,因為一旦模型通過閾值以獲得足夠高的 MCF 訊號雜訊比,它就能提供更好的中期訓練訊號。快速說明一下,在像 CF 這樣的序列可能性評估中評分模型的答案時,我們將準確性計算為正確答案具有最高對數機率(按字元數正規化)的問題百分比。這種正規化防止了對較短答案的偏見。

MMLU MCF 變為非隨機的點取決於模型大小和訓練資料。對於 7B transformer,OLMES 論文([Gu et al., 2025](https://arxiv.org/abs/2406.08446))發現模型在 500B token 後開始顯示非隨機效能。對於 1.7B 模型,我們發現這發生在 SmolLM2 中的 6T token 後([Allal et al., 2025](https://arxiv.org/abs/2502.02737))。[Du et al. (2025)](https://arxiv.org/abs/2403.15796) 認為這從根本上是關於預訓練損失達到某個閾值。

我們的消融實驗評估套件包括來自 [FineWeb](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1) 消融實驗的基準測試,除了我們發現太雜訊的 SIQA。我們添加了數學和程式碼基準測試,如 GSM8K 和 HumanEval,以及長上下文基準測試 RULER 用於長上下文消融實驗。這些任務的聚合測試世界知識、推理和常識,跨越各種格式,如下表所示。為了加快評估速度,犧牲一些額外的雜訊,我們只評估每個基準測試的 1,000 個問題(除了 GSM8K、HumanEval 和 RULER,我們在 3B SmolLM3 消融實驗中完整使用,但在下面的 1B 實驗中省略)。我們還對所有多選基準測試使用完形填空(CF)評估方式,如上所述。請注意,對於多語言消融實驗和實際訓練,我們添加了更多基準測試來測試多語言性,我們稍後會詳細說明。這些評估使用 [LightEval](https://github.com/huggingface/lighteval) 執行,下表總結了每個基準測試的關鍵特徵:

| 基準測試 | 領域 | 任務類型 | 問題數 | 測試內容 |
|---|---|---|---|---|
| MMLU | 知識 | 多選 | 14k | 跨 57 個主題的廣泛學術知識 |
| ARC | 科學與推理 | 多選 | 7k | 小學級別的科學推理 |
| HellaSwag | 常識推理 | 多選 | 10k | 關於日常情況的常識推理(敘事完成) |
| WinoGrande | 常識推理 | 二選一 | 1.7k | 需要世界知識的代詞解析 |
| CommonSenseQA | 常識推理 | 多選 | 1.1k | 關於日常概念的常識推理 |
| OpenBookQA | 科學 | 多選 | 500 | 帶推理的基礎科學事實 |
| PIQA | 物理常識 | 二選一 | 1.8k | 關於日常物品的物理常識 |
| GSM8K | 數學 | 自由生成 | 1.3k | 小學數學應用題 |
| HumanEval | 程式碼 | 自由生成 | 164 | 從文件字串合成 Python 函數 |

讓我們從每個基準測試中看一些範例問題,以具體了解這些評估實際測試什麼:

瀏覽上面的範例,看看每個基準測試中的問題類型。注意 MMLU 和 ARC 如何用多選測試事實知識,GSM8K 需要計算數學問題的數值答案,而 HumanEval 需要生成完整的 Python 程式碼。這種多樣性確保我們在整個消融實驗過程中測試模型能力的不同方面。

**消融實驗使用什麼資料混合?**

對於 _架構消融實驗_,我們在一個固定的高品質資料集混合上訓練,該混合在廣泛的任務範圍內提供早期訊號。我們使用英語([FineWeb-Edu](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu))、數學([FineMath](https://huggingface.co/datasets/HuggingFaceTB/finemath))和程式碼([Stack-Edu-Python](https://huggingface.co/datasets/HuggingFaceTB/stack-edu))。架構發現應該很好地推斷到其他資料集和領域,包括多語言資料,因此我們可以保持資料混合簡單。

對於 _資料消融實驗_,我們採取相反的方法:我們固定架構並系統地改變資料混合,以了解不同的資料來源如何影響模型效能。

有時評估中的差異可能很小。如果你有足夠的算力,可能值得用不同的種子重新執行相同的消融實驗,看看結果變化多大。

穩固的消融實驗設置的真正價值不僅僅在於構建一個好模型。當事情在我們的主要訓練執行期間不可避免地出錯時(無論我們準備多充分,它們都會出錯),我們希望對我們做出的每個決定充滿信心,並快速識別哪些組件沒有得到適當測試並可能導致問題。這種準備節省了除錯時間並保護了我們未來的心理健全。

#### 估算消融實驗成本

消融實驗很棒,但它們需要 GPU 時間,值得了解這些實驗的成本。下表顯示了我們 SmolLM3 預訓練的完整算力分解:主要執行(考慮偶爾的停機時間)、訓練前和訓練期間的消融實驗,以及在意外擴展問題上花費的算力,該問題迫使重新啟動和一些除錯(我們稍後會詳細說明)。

| 階段 | GPU 數 | 天數 | GPU 小時 |
|---|---|---|---|
| 主要預訓練執行 | 384 | 30 | 276,480 |
| 消融實驗(預訓練) | 192 | 15 | 69,120 |
| 消融實驗(訓練中期) | 192 | 10 | 46,080 |
| 訓練重置與除錯 | 384/192 | 3/4 | 46,080 |
| **總成本** | - | - | **437,760** |

我們估計評估成本略低於 10,000 GPU 小時。我們的完整評估套件(英語、多語言、數學和程式碼)每個 GPU 大約需要 1.5 小時,我們在整個 11T token 中每 10B token 評估一次,以及眾多消融實驗。長上下文評估特別昂貴,每次在 8 個 GPU 上執行大約需要 1 小時。

這些數字揭示了一個重要事實:消融實驗和除錯總共消耗了 161,280 GPU 小時,**超過我們主要訓練執行成本的一半**(276,480 GPU 小時)**。** 我們在 SmolLM3 的開發過程中總共執行了 100 多次消融實驗:我們在預訓練消融實驗上花費了 20 天,在訓練中期消融實驗上花費了 10 天,在從意外訓練問題中恢復上花費了 7 天,該問題迫使重新啟動和一些除錯(我們稍後會詳細說明)。

這突顯了為什麼消融實驗成本必須納入你的算力預算:計劃訓練成本加消融實驗加意外緩衝。如果你的目標是 SOTA 效能、實作新的架構變更,或者還沒有經過驗證的配方,消融實驗會成為一個實質性的成本中心,而不是次要實驗。

當 [DeepSeek-V3](https://huggingface.co/deepseek-ai/DeepSeek-V3) 問世時,[全世界都專注於](https://www.forbes.com/sites/markminevich/2025/02/06/the-6-million-ai-bombshell-how-deepseek-shook-wall-street-and-ai-leadership/)其報告的 560 萬美元訓練成本。許多人將這個數字解釋為完整的研發成本。實際上,它只反映了最終的訓練執行。更大的——通常看不見的——費用在於研究本身:導致最終配方的消融實驗、失敗的執行和除錯。考慮到模型的規模和新穎性,他們的研究成本肯定更高。

在我們進入下一節之前,讓我們建立一些每個執行實驗的人都應該遵循的基本規則。
