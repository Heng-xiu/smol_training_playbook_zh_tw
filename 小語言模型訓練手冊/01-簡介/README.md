## 簡介

訓練一個高效能大語言模型，究竟需要什麼？

**閱讀時長：2-4天**

學術論文讓一切看起來如此順理成章：精心設計的架構、用心策劃的資料集、充足的算力。結果打磨得光鮮亮麗，消融實驗條理清晰。回過頭看，每個決策都顯得理所當然。但那些報告只呈現了成功的那一面，還帶著些許美化的回憶——它們不會告訴你凌晨兩點調試資料載入器的煎熬、突如其來的損失峰值、或是那個悄悄破壞訓練的張量並行化 bug（後面會講到！）。真實的情況要混亂得多，充滿了反覆迭代，以及那些永遠不會出現在最終論文裡的決策。

歡迎跟隨我們的腳步，一探 [SmolLM3](https://huggingface.co/HuggingFaceTB/SmolLM3-3B) 訓練的幕後故事——這是一個在 11T tokens 上訓練而成的 3B 多語言推理模型。這不是一篇普通的部落格文章，而是對一張蛛網般交織的決策、發現與死胡同的解構，最終凝聚成構建世界級語言模型的深刻洞見。

這也是我們模型訓練長篇系列的終章：我們曾深入大規模資料集構建（[FineWeb](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1)）、指揮數千塊 GPU 協同作戰（[超大規模實戰手冊](https://huggingface.co/spaces/nanotron/ultrascale-playbook)）、以及在每個環節選擇最佳評估方案（[評估指南](https://github.com/huggingface/evaluation-guidebook)）。如今，我們將所有這些拼圖組合在一起，打造出一個強大的 AI 模型。我們會帶你走過完整的旅程——不只是最終奏效的配方，還有那些失敗、基礎設施崩潰、以及塑造每個決策的除錯過程。

這個故事讀起來像一部戲劇：你會看到那些在小規模上前景光明的消融實驗如何在擴展時失效、我們為何在訓練了 1T tokens 後重新開始、如何在保持強大英語效能的同時平衡多語言、數學和程式碼等相互競爭的目標、以及最終如何後訓練出一個混合推理模型。

我們刻意避免冰冷地羅列所有做過的事，而是把它組織成一個有血有肉的冒險故事。把這當作一本指南吧——獻給所有試圖從「我們有很棒的資料集和 GPU」走向「我們構建了一個真正強大的模型」的人。我們希望這份坦誠能幫助縮小研究與生產之間的鴻溝，讓你的下一次訓練少一些混亂。
