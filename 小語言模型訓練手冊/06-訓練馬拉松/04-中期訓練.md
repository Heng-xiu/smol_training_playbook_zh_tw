### [中期訓練](https://huggingfacetb-smol-training-playbook.hf.space/#mid-training)

現代 LLM 預訓練通常涉及具有不同資料混合的多個階段,通常接著是擴展上下文長度的最終階段。例如,Qwen3 ([A. Yang, Li, et al., 2025](https://arxiv.org/abs/2505.09388)) 使用三階段方法:在 4k 上下文的 30T tokens 上的通用階段,在 5T 更高品質 tokens 上的推理階段,強調 STEM 和編碼,最後是在 32k 上下文長度的數千億 tokens 上的長上下文階段。SmolLM3 遵循類似的哲學,計劃介入以引入更高品質的資料集並擴展上下文,以及基於效能監控的反應性調整。
正如我們在資料策劃部分解釋的那樣,資料混合不必在整個訓練過程中保持固定。多階段訓練允許我們隨著訓練的進展策略性地轉移資料集比例。一些介入從一開始就計劃好了:對於 SmolLM3,我們知道我們會在階段 2 中引入更高品質的 FineMath4+ 和 Stack-Edu,然後在最終衰減階段新增策劃的 Q&A 和推理資料。其他介入是反應性的,由訓練期間的效能監控驅動。例如,在 SmolLM2 中,當我們發現數學和程式碼效能落後於我們的目標時,我們策劃了全新的資料集(FineMath 和 Stack-Edu)並在訓練中期引入它們。這種靈活性 — 無論是遵循計劃的課程還是適應新出現的差距 — 是允許我們最大化計算預算價值的原因。

#### [階段 2 和階段 3 混合](https://huggingfacetb-smol-training-playbook.hf.space/#stage-2-and-stage-3-mixtures)

下圖顯示了我們的 3 個訓練階段以及訓練期間網頁/程式碼/數學比例的進展。SmolLM3 每個階段的訓練配置[可在此處取得](https://github.com/huggingface/smollm/tree/main/text/pretraining/smollm3),包含確切的資料權重。有關每個階段背後的理由和組成的更多細節,請參閱資料策劃部分。
Stage 1: Base trainingStage 2: High quality injectionStage 3: LR Decay0T1T2T3T4T5T6T7T8T9T10T11T0%20%40%60%80%100%Training Progress (Trillion Tokens)Data Mixture (%)
Legend
WebCodeMath
**階段 1:基礎訓練(8T tokens,4k 上下文)** 基礎階段使用我們的核心預訓練混合:網頁資料(FineWeb-Edu、DCLM、FineWeb2、FineWeb2-HQ)、來自 The Stack v2 和 StarCoder2 的程式碼,以及來自 FineMath3+ 和 InfiWebMath3+ 的數學。所有訓練都在 4k 上下文長度進行。
**階段 2:高品質注入(2T tokens,4k 上下文)** 我們引入更高品質的過濾資料集:用於程式碼的 Stack-Edu、用於數學的 FineMath4+ 和 InfiWebMath4+,以及用於高級數學推理的 MegaMath(我們新增 Qwen Q&A 資料、合成重寫和文本-程式碼交錯區塊)。
**階段 3:帶有推理和 Q&A 資料的 LR 衰減(1.1T tokens,4k 上下文)** 在學習率衰減階段,我們進一步對高品質程式碼和數學資料集進行上採樣,同時引入指令和推理資料,如 OpenMathReasoning、OpenCodeReasoning 和 OpenMathInstruct。Q&A 樣本簡單地連接並用新行分隔。

#### [長上下文擴展:從 4k 到 128k tokens](https://huggingfacetb-smol-training-playbook.hf.space/#long-context-extension-from-4k-to-128k-tokens)

上下文長度決定了你的模型可以處理多少文本,它對於分析長文件、維持連貫的多輪對話或處理整個程式碼庫等任務至關重要。SmolLM3 從 4k tokens 開始訓練,但我們需要擴展到 128k 以適用於真實世界的應用程式。
**為什麼在訓練中期擴展上下文?**
從一開始就在長上下文上訓練在計算上是昂貴的,因為注意力機制隨序列長度的平方而擴展。此外,研究表明,在訓練結束時或在持續預訓練期間使用幾十到一百億 tokens 擴展上下文,足以達到良好的長上下文效能 ([Gao et al., 2025](https://arxiv.org/abs/2410.02660))。
**順序擴展:4k→32k→64k**
我們沒有直接跳到 128k。相反,我們逐步擴展階段中的上下文,在進一步推進之前給模型時間在每個長度上適應。我們執行了兩個長上下文階段:首先從 4k 到 32k,然後從 32k 到 64k(128k 能力來自推理時外推,而不是訓練)。我們發現為每個階段在 50B tokens 上開始一個新的學習率排程比在主衰減階段的最後 100B tokens 期間擴展上下文效果更好。在每個階段,我們執行消融實驗以找到良好的長上下文資料混合和 RoPE theta 值,並在 Ruler 基準測試上評估。
基礎模型上的長上下文評估
在長上下文消融實驗期間,我們發現 [HELMET](https://arxiv.org/abs/2410.02694) 基準測試在基礎模型上非常嘈雜(具有不同種子的相同訓練給出可變結果)。[Gao et al.](https://arxiv.org/abs/2410.02660) 建議在頂部進行 SFT 以減少基準測試任務的變異性。相反,我們選擇了 RULER,我們發現它在基礎模型層級上提供了更可靠的訊號。
在這個階段,上採樣長上下文文件(如冗長的網頁和書籍)以改善長上下文效能是常見的 ([Gao et al., 2025](https://arxiv.org/abs/2410.02660))。我們執行了幾個消融實驗,對書籍、文章,甚至合成生成的文件進行上採樣,用於檢索和 fill-in-the-middle 等任務,遵循 Qwen2.5-1M 的方法 ([A. Yang, Yu, et al., 2025](https://arxiv.org/abs/2501.15383)),使用 FineWeb-Edu 和 Python-Edu。令人驚訝的是,與僅使用階段 3 的基線混合相比,我們沒有觀察到任何改善,該混合在 Ruler 上已經與其他最先進的模型(如 Llama 3.2 3B 和 Qwen2.5 3B)競爭。我們假設這是因為基線混合自然包含來自網頁資料和程式碼的長文件(估計為 tokens 的 10%),並且使用 NoPE 有所幫助。
有關長上下文擴展的更多見解,我們建議閱讀論文 [How to Train Long-Context Language Models (Effectively)](https://arxiv.org/abs/2410.02660)
**RoPE ABF(RoPE with Adjusted Base Frequency):** 從 4k 到 32k 時,我們將 RoPE theta(基礎頻率)增加到 2M,從 32k 到 64k 時,我們將其增加到 5M。我們發現使用 10M 等較大值會略微改善 RULER 分數,但會損害某些短上下文任務(如 GSM8k),因此我們保持 5M,這不會影響短上下文。在這個上下文擴展階段,我們還利用機會進一步對數學、程式碼和推理 Q&A 資料進行上採樣,並且我們新增了數十萬個 ChatML 格式的樣本。
我們還在 4k→32k 擴展期間實驗了滑動窗口注意力(窗口大小為 4k、8k 和 16k),但發現與完整注意力相比,它在 RULER 上的效能更差。
**YARN 外推:達到 128k** 即使在 64k 上下文上訓練後,我們希望 SmolLM3 在推理時處理 128k。我們沒有在 128k 序列上訓練(成本過高),而是使用了 YARN(Yet Another RoPE extensioN method)([B. Peng et al., 2023](https://arxiv.org/abs/2309.00071)),它允許模型外推超出其訓練長度。理論上,YARN 允許序列長度增加四倍。我們發現使用 64k 檢查點在 128k 處提供了比使用 32k 檢查點更好的效能,證實了接近目標推理長度訓練的好處。然而,推到 256k(從 64k 的四倍)顯示出降低的 Ruler 效能,因此我們建議將模型使用到 128k。
至此,我們已經走過了 SmolLM3 的完整預訓練之旅,從計劃和消融實驗到最終訓練執行,以及沿途所有幕後挑戰。
