### [擴展的驚喜](https://huggingfacetb-smol-training-playbook.hf.space/#scaling-surprises)

在為 SmolLM3 執行大量消融實驗後,我們已準備好進行全規模執行。我們在 100B tokens 上的 3B 消融實驗看起來很有希望。與 SmolLM2 相比的架構變更(詳見[架構選擇](https://huggingfacetb-smol-training-playbook.hf.space/#architecture-choices):GQA、NoPE、document masking、tokenizer)要麼改善了效能,要麼維持了效能,我們找到了一個平衡英語、多語言、程式碼和數學效能的良好資料混合(請參閱[資料策劃的藝術](https://huggingfacetb-smol-training-playbook.hf.space/#smollm3-curating-the-data-mixture-web-multilingual-math-code)。我們在 384 個 GPU(48 個節點)上優化了我們的配置,達到約 30% MFU。我們在 48 個節點上執行了一些消融實驗,以在啟動執行前驗證輸送量。你可以在基礎設施章節中找到更多關於它們的細節。我們已準備好進行大型執行:11T tokens。這時現實開始拋出曲球。

#### [**謎題 #1 – 消失的輸送量**](https://huggingfacetb-smol-training-playbook.hf.space/#mystery-1--the-vanishing-throughput)

在啟動後的幾個小時內,輸送量急劇下降。這是一個大幅跳躍,伴隨著重複的急劇下降。
為什麼輸送量很重要
輸送量衡量我們的系統在訓練期間每秒處理多少個 tokens。它直接影響我們的訓練時間,輸送量下降 50% 意味著我們為期一個月的執行變成了兩個月的執行。在基礎設施章節中,我們將展示在開始執行前如何為 SmolLM3 優化輸送量。
Reset View
Legend
這在任何消融實驗執行中都沒有發生,那麼什麼改變了?三件事:
  1. 硬體狀態可能隨時間變化。在消融實驗中運作良好的 GPU 可能會失敗,網路連線可能會在持續負載下降低。
  2. 訓練資料集的大小。我們現在使用了完整的 ~24 TB 訓練資料集,而不是消融實驗中的較小子集,儘管資料來源本身是相同的。
  3. 訓練步數。我們為 11T tokens 設定了真實的步數,而不是短期的 100B-token 消融實驗範圍。
其他一切與輸送量消融實驗中完全相同:節點數量、dataloader 配置、模型佈局和並行設置...
直覺上,資料集大小和步數都不應該導致輸送量下降,因此我們自然首先懷疑硬體問題。我們檢查了節點監控指標,這顯示大幅輸送量跳躍與磁碟讀取延遲峰值相關。這直接指向我們的資料儲存。
我們叢集中的儲存選項
我們的叢集有三個訓練資料的儲存層:
  - **FSx**:網路連接儲存,使用 [Weka](https://www.weka.io/),一個「保持熱」快取模型,將經常存取的檔案儲存在本機,並在容量填滿時將不活躍的「冷」檔案驅逐到 S3。
  - **Scratch(本機 NVMe RAID)**:每個節點上的快速本機儲存(RAID 中的 8×3.5TB NVMe 磁碟機),比 FSx 快,但僅限於本機節點存取。
  - **S3**:用於冷資料和備份的遠端物件儲存。
你可以在[基礎設施章節](https://huggingfacetb-smol-training-playbook.hf.space/#infrastructure---the-unsung-hero)中找到更多細節。
對於 SmolLM3 的 24TB 資料集,我們最初將資料儲存在 FSx(Weka)中。有 24TB 的訓練資料,加上其他幾個團隊已經使用的儲存,我們將 Weka 的儲存推到了極限。因此,它開始在訓練中期驅逐資料集分片,這意味著我們必須重新獲取它們,造成停頓,這解釋了大幅輸送量跳躍。更糟的是:沒有辦法將我們的資料集資料夾固定為整個訓練的熱資料。
**修復 #1 – 更改資料儲存**
我們沒有找到在 Weka 中將資料集資料夾固定為整個訓練的熱資料的方法,因此我們嘗試更改儲存方法。直接從 S3 串流很慢,因此我們決定將資料儲存在每個節點的本機儲存 `/scratch` 中。
這帶來了一個問題:如果一個節點死亡並被替換,新的替換 GPU 沒有資料。使用 `s5cmd` 從 S3 下載 24TB 花費了 3 小時。我們透過使用 `fpsync` 從另一個健康節點複製而不是通過 S3,將時間縮短到 1 小時 30 分鐘。鑑於所有節點都在同一個資料中心,這更快。
但是,每次節點故障有 1 小時 30 分鐘的停機時間,以及需要立即手動將資料複製到新節點,這是痛苦的。最終使其變得可以忍受的技巧是:在我們的 Slurm 保留中保留一個預載資料集的備用節點。如果一個節點死亡,我們立即用備用節點替換它,因此零恢復延遲。在空閒時,備用節點執行評估或開發作業,因此它不會被浪費。
這修復了謎題 #1...或者我們是這麼想的。

#### [**謎題 #2 – 持續的輸送量下降**](https://huggingfacetb-smol-training-playbook.hf.space/#mystery-2--the-persisting-throughput-drops)

即使在移動到 scratch 之後,個別輸送量下降仍然發生,儘管我們在硬體監控指標中沒有發現任何異常。下圖比較了我們在修復儲存問題後獲得的輸送量(橙色)與我們在消融實驗期間獲得的輸送量(藍色)。如你所見,下降變得更加尖銳。
Reset View
Legend
仍然懷疑是硬體問題,我們決定在更少的節點上測試。有 384 個 GPU,有很高的機會會有東西失敗。令人驚訝的是,我們可以在單個節點上重現完全相同的輸送量下降,無論我們測試哪個特定節點。這排除了硬體問題。
記得從我們的消融實驗中改變的三件事嗎?我們已經透過移動到本機節點儲存解決了資料儲存問題。硬體現在被排除了。這只剩下一個變數:步數。我們透過回滾到較小的步數(從 3M 到 32k)進行測試,輸送量下降變得更小!較大的步數產生了更尖銳、更頻繁的下降。
為了測試這一點,我們執行了相同的配置,只改變了訓練步數從 32k 到 3.2M。你可以看到[我們使用的確切配置](https://huggingface.co/datasets/HuggingFaceTB/ablations-training-configs/tree/main/throughput_debugging):

```

## Short run (32k steps)

- "lr_decay_starting_step": 2560000
- "lr_decay_steps": 640000
- "train_steps": 3200000

## Long run (3.2M steps)

+ "lr_decay_starting_step": 26000
+ "lr_decay_steps": 6000
+ "train_steps": 32000

```

下圖所示的結果很清楚:較短的執行有小的輸送量下降,而較長的步數產生了更尖銳、更頻繁的下降。因此,問題不是硬體,而是軟體瓶頸,可能在 dataloader 中!鑑於大多數其他訓練組件無論步數如何都相同地處理每個批次。
Reset View
Legend
這時我們意識到我們從未真正使用 nanotron 的 dataloader 進行大規模預訓練。SmolLM2 已經使用 Megatron-LM 衍生的 dataloader([TokenizedBytes](https://github.com/huggingface/nanotron/blob/7bc9923285a03069ebffe994379a311aceaea546/src/nanotron/data/tokenized_bytes.py#L80))透過 nanotron 周圍的內部包裝器以穩定的輸送量進行訓練。對於 SmolLM3,我們切換到 nanotron 的內建 dataloader(`nanosets`)。
在深入研究其實作後,我們發現它天真地建立了一個隨著每個訓練步驟而增長的巨大索引。對於非常大的步驟,這會導致更高的共享記憶體,從而觸發輸送量下降。
**修復 #2 – 引入 TokenizedBytes dataloader**
為了確認 dataloader 確實是罪魁禍首,我們使用內部 SmolLM2 框架和 `TokenizedBytes` dataloader 啟動了相同的配置。沒有下降。即使在 48 個節點上使用相同的資料集。
最快的前進路徑:將此 dataloader 複製到 nanotron。下降消失了,輸送量恢復到目標。
我們已準備好重新啟動...直到下一個曲球。

#### [**謎題 #3 – 嘈雜的損失**](https://huggingfacetb-smol-training-playbook.hf.space/#mystery-3--the-noisy-loss)

使用新的 dataloader,我們沒有輸送量下降,但損失曲線看起來更嘈雜。
`nanosets` 一直產生更平滑的損失,這種差異讓我們想起了一場舊的除錯戰爭:幾年前,我們在預訓練程式碼中發現了一個隨機排列錯誤,其中文件被隨機排列,但批次內的序列沒有,導致小的峰值。
檢查我們的新 dataloader 證實了這一點:它從每個文件順序讀取序列。這對於短檔案來說沒問題,但對於程式碼等領域,單個長的低品質檔案可以填滿整個批次並導致損失峰值。
**修復 #3 – 在序列層級隨機排列**
我們有兩個選項:
  1. 更改 dataloader 以進行隨機存取(風險:更高的記憶體使用)。
  2. 離線預先隨機排列 tokenized 序列。
由於開始執行的時間壓力和我們的叢集保留正在執行,我們選擇了選項 #2 作為更安全、更快的修復。Tokenized 資料已經在每個節點上,因此在本機重新隨機排列很便宜(~1 小時)。我們還為每個 epoch 生成了具有不同種子的隨機排列序列,以避免在 epochs 之間重複隨機排列模式。
知道何時修補 vs. 修復
在面臨緊急截止日期時,採用經過驗證的解決方案或快速解決方法可能比除錯你自己損壞的實作更快。早些時候,我們插入了 TokenizedBytes dataloader 而不是修復 nanosets 的索引實作。在這裡,我們選擇離線預先隨機排列而不是 dataloader 變更。但要知道何時走捷徑,否則你最終會得到一個難以維護或優化的拼湊系統。

#### [**啟動,第二次嘗試**](https://huggingfacetb-smol-training-playbook.hf.space/#launch-take-two)

到現在為止,我們有了:
  - **穩定的輸送量**(scratch 儲存 + 備用節點策略)
  - **沒有步數引起的下降**(`TokenizedBytes` dataloader)
  - **乾淨的序列層級隨機排列**(每個 epoch 離線預先隨機排列)
我們重新啟動。這一次,一切都穩住了。損失曲線平滑,輸送量一致,我們終於可以專注於訓練而不是救火。
**謎題 #4 – 不滿意的效能**
在修復輸送量和 dataloader 問題後,我們再次啟動執行,並在前兩天順利訓練。輸送量穩定,損失曲線如預期,日誌中沒有任何問題的跡象。然而,在大約 1T token 標記處,評估揭示了一些意外的事情。
作為我們監控的一部分,我們評估中間檢查點並將它們與歷史執行進行比較。例如,我們有來自在類似配方上訓練的 SmolLM2(1.7B)的[中間檢查點](https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-intermediate-checkpoints),因此我們可以追蹤兩個模型在訓練的相同階段如何進展。結果令人困惑:儘管有更多參數和更好的資料混合,3B 模型在相同訓練點的效能比 1.7B 差。損失仍在下降,基準測試分數正在改善,但改善速度明顯低於預期。
鑑於我們已經徹底測試了 SmolLM3 與 SmolLM2 相比引入的每個架構和資料變更,我們驗證了訓練框架,兩個訓練設置之間只有少數未經測試的差異。最明顯的是 tensor parallelism。SmolLM2 可以適應單個 GPU,並且在沒有 TP 的情況下訓練,而 SmolLM3 需要 TP=2 以適應記憶體。我們之前沒有懷疑它或想到測試它,因為 TP 在 3B 消融實驗中使用,它們的結果是有道理的。
**修復 #4 - 最終修復**
為了測試 TP 錯誤假設,我們使用與 SmolLM3 完全相同的設置訓練了一個 1.7B 模型 — 相同的架構變更(document masking、NoPE)、相同的資料混合、相同的超參數 — 有和沒有 TP。差異是立即的:TP 版本的損失始終更高,下游效能低於非 TP 版本。這證實了我們正在看到一個與 TP 相關的錯誤。
然後我們詳細檢查了 TP 實作,比較了 TP 和非 TP 執行的權重。問題原來是微妙但重要的:我們在所有 TP 排名中使用相同的隨機種子,而每個排名應該用不同的種子初始化。這導致分片之間的相關權重初始化,這影響了收斂。影響不是災難性的 — 模型仍然訓練和改善 — 但它引入了足夠的低效率來解釋我們在規模上觀察到的差距。以下是錯誤修復:

```

diff --git a/src/nanotron/trainer.py b/src/nanotron/trainer.py
index 1234567..abcdefg 100644
--- a/src/nanotron/trainer.py
+++ b/src/nanotron/trainer.py
@@ -185,7 +185,10 @@ class DistributedTrainer:
     ):
         # Set random states

-        set_random_seed(self.config.general.seed)
+        # Set different random seed for each TP rank to ensure diversity

+        tp_rank = dist.get_rank(self.parallel_context.tp_pg)
+        set_random_seed(self.config.general.seed + tp_rank)
+

```

Reset View
Legend
一旦我們修復了種子,使每個 TP 排名使用不同的種子,我們重複了消融實驗,並證實 TP 和非 TP 執行現在在損失曲線和下游效能方面都匹配。為了確保沒有其他隱藏問題,我們執行了額外的健全性檢查:一個 SmolLM2 風格(架構和資料方面)在 3B 參數下的執行,以及一個單獨的 SmolLM3 在 3B 參數下的執行,將兩者與 SmolLM2 的檢查點進行比較。結果現在與預期一致:1.7B SmolLM2 的效能比 3B SmolLM2 變體差,而 3B SmolLM2 變體又低於 SmolLM3 的 3B 效能。
Legend
HellaSwag
Reset
MMLU
Reset
ARC
Reset
PIQA
Reset
OpenBookQA
Reset
WinoGrande
Reset
這個除錯過程強化了我們在本部落格前面概述的核心原則之一:
「穩健的消融實驗設置的真正價值不僅僅是建立一個好的模型。當主要訓練執行中不可避免地出現問題時(它們會,無論我們準備多少),我們希望對我們做出的每個決定都有信心,並快速識別哪些組件沒有得到適當測試並可能導致問題。這種準備節省了除錯時間並保持了我們的理智。沒有什麼比盯著神秘的訓練失敗而不知道錯誤可能隱藏在哪裡更糟糕的了。」
因為我們訓練中的每個其他組件都已經過驗證,我們可以將 TP 識別為唯一可能的原因,並在檢測到效能差距的一天內修復錯誤。
至此,我們已經解決了自啟動以來出現的一系列意外問題中的最後一個。第三次是魅力,從那時起,剩餘一個月的訓練相對平淡無奇,只是將數兆 tokens 變成成品模型的穩定工作,偶爾因節點故障而重新啟動。
