## [參考文獻](https://huggingfacetb-smol-training-playbook.hf.space/#references)

以下是對我們 LLM 訓練旅程最有幫助的精選論文、書籍和部落格文章列表。

#### [LLM 架構](https://huggingfacetb-smol-training-playbook.hf.space/#llm-architecture)
  - Dense models: [Llama3](https://huggingface.co/papers/2407.21783), [Olmo2](https://huggingface.co/papers/2501.00656), [MobileLLM](https://huggingface.co/papers/2402.14905)
  - MoEs: [DeepSeek V2](https://huggingface.co/papers/2405.04434), [DeepSeek V3](https://huggingface.co/papers/2412.19437), [Scaling Laws of Efficient MoEs](https://huggingface.co/papers/2507.17702)
  - Hybrid: [MiniMax-01,](https://huggingface.co/papers/2501.08313) [Mamba2](https://huggingface.co/papers/2405.21060)

#### [優化器與訓練參數](https://huggingfacetb-smol-training-playbook.hf.space/#optimisers--training-parameters)
  - [Muon is Scalable for LLM Training](https://huggingface.co/papers/2502.16982), [Fantastic pretraining optimisers](https://huggingface.co/papers/2509.02046)
  - [Large Batch Training](https://arxiv.org/abs/1812.06162), [DeepSeekLLM](https://arxiv.org/abs/2401.02954)

#### [資料策劃](https://huggingfacetb-smol-training-playbook.hf.space/#data-curation)
  - Web: [FineWeb & FineWeb-Edu](https://huggingface.co/papers/2406.17557), [FineWeb2](https://huggingface.co/papers/2506.20920), [DCLM](https://huggingface.co/papers/2406.11794)
  - Code: [The Stack v2](https://huggingface.co/papers/2402.19173), [To Code or Not to Code](https://huggingface.co/papers/2408.10914)
  - Math: [DeepSeekMath](https://huggingface.co/papers/2402.03300), [FineMath](https://huggingface.co/papers/2502.02737), [MegaMath](https://huggingface.co/papers/2504.02807)
  - Data mixtures: [SmolLM2](https://huggingface.co/papers/2502.02737), [Does your data spark joy](https://huggingface.co/papers/2406.03476)

#### [擴展定律](https://huggingfacetb-smol-training-playbook.hf.space/#scaling-laws)
  - [Kaplan](https://huggingface.co/papers/2001.08361), [Chinchilla](https://huggingface.co/papers/2203.15556), [Scaling Data-Constrained Language Models](https://huggingface.co/papers/2305.16264)

#### [後訓練](https://huggingfacetb-smol-training-playbook.hf.space/#post-training)
  - [InstructGPT:](https://huggingface.co/papers/2203.02155) OpenAI’s foundational paper to turn base models into helpful assistants. The precursor to ChatGPT and a key step on humanity’s path up the Kardashev scale.
  - [Llama 2](https://huggingface.co/papers/2307.09288) & [3](https://huggingface.co/papers/2407.21783): Extremely detailed tech reports from Meta on the training behind their Llama models (may they rest in peace). They each contain many insights into human data collection, both for human preferences and model evaluation.
  - Secrets of RLHF in LLMs, [Part I](https://huggingface.co/papers/2307.04964) & [II](https://huggingface.co/papers/2401.06080): these papers contain lots of goodies on the nuts and bolts for RLHF, specifically on how to train strong reward models.
  - [Direct Preference Optimisation:](https://huggingface.co/papers/2305.18290) the breakthrough paper from 2023 that convinced everyone to stop doing RL with LLMs.
  - [DeepSeek-R1](https://huggingface.co/papers/2501.12948): the breakthrough paper from 2025 that convinced everyone to start doing RL with LLMs.
  - [Dr. GRPO:](https://huggingface.co/papers/2503.20783) one of the most important papers on understanding the baked-in biases with GRPO and how to fix them.
  - [DAPO: ](https://huggingface.co/papers/2503.14476)Bytedance shares many implementation details to unlock stable R1-Zero-like training for the community.
  - [ScaleRL:](https://huggingface.co/papers/2510.13786) a massive flex from Meta to derive scaling laws for RL. Burns over 400k GPU hours to establish a training recipe that scales reliably over many orders of compute.
  - [LoRA without Regret:](https://thinkingmachines.ai/blog/lora/) a beautifully written blog post which finds that RL with low-rank LoRA can match full-finetuning (a most surprising result).
  - [Command A:](https://huggingface.co/papers/2504.00698) a remarkably detailed tech report from Cohere on various strategies to post-train LLMs effectively.

#### [基礎設施](https://huggingfacetb-smol-training-playbook.hf.space/#infrastructure)
  - [UltraScale Playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook)
  - [Jax scaling book](https://jax-ml.github.io/scaling-book/)
  - [Modal GPU Glossary](https://modal.com/gpu-glossary/readme)

#### [訓練框架](https://huggingfacetb-smol-training-playbook.hf.space/#training-frameworks)
  - [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)
  - [DeepSpeed](https://github.com/deepspeedai/DeepSpeed)
  - [Torchtitan](https://github.com/pytorch/torchtitan)
  - [Nanotron](https://github.com/huggingface/nanotron/)
  - [NanoChat](https://github.com/karpathy/nanochat)
  - [TRL](https://github.com/huggingface/trl)

#### [評估](https://huggingfacetb-smol-training-playbook.hf.space/#evaluation)
  - [The LLM Evaluation Guidebook](https://github.com/huggingface/evaluation-guidebook)
  - [OLMES](https://huggingface.co/papers/2406.08446)
  - [FineTasks](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fine-tasks)
  - [Lessons from the trenches](https://huggingface.co/papers/2405.14782)

### 引用

在學術情境中引用時,請這樣引用本作品

```

Loubna Ben Allal, Lewis Tunstall, Nouamane Tazi, Elie Bakouch, Ed Beeching, Carlos Miguel Patiño, Clémentine Fourrier, Thibaud Frere, Anton Lozhkov, Colin Raffel, Leandro von Werra, Thomas Wolf (2025). "The Smol Training Playbook: The Secrets to Building World-Class LLMs".

```

BibTeX 引用

```

@misc{allal2025_the_smol_training_playbook_the_secrets_to_building_world_class_llms,
  title={The Smol Training Playbook: The Secrets to Building World-Class LLMs},
  author={Loubna Ben Allal and Lewis Tunstall and Nouamane Tazi and Elie Bakouch and Ed Beeching and Carlos Miguel Patiño and Clémentine Fourrier and Thibaud Frere and Anton Lozhkov and Colin Raffel and Leandro von Werra and Thomas Wolf},
  year={2025},

}

```

### References

  1. Agarwal, R., Vieillard, N., Zhou, Y., Stanczyk, P., Ramos, S., Geist, M., & Bachem, O. (2024). _On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes_. <https://arxiv.org/abs/2306.13649>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-gkd-1)
  2. Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y., Lebrón, F., & Sanghai, S. (2023). _GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints_. <https://arxiv.org/abs/2305.13245>[1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-gqa-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-gqa-2)
  3. Allal, L. B., Lozhkov, A., Bakouch, E., Blázquez, G. M., Penedo, G., Tunstall, L., Marafioti, A., Kydlíček, H., Lajarín, A. P., Srivastav, V., Lochner, J., Fahlgren, C., Nguyen, X.-S., Fourrier, C., Burtenshaw, B., Larcher, H., Zhao, H., Zakka, C., Morlon, M., … Wolf, T. (2025). _SmolLM2: When Smol Goes Big – Data-Centric Training of a Small Language Model_. <https://arxiv.org/abs/2502.02737>[1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-smollm2-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-smollm2-2), [3](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-smollm2-3)
  4. Almazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli, A., Cojocaru, R., Debbah, M., Goffinet, É., Hesslow, D., Launay, J., Malartic, Q., Mazzotta, D., Noune, B., Pannier, B., & Penedo, G. (2023). _The Falcon Series of Open Language Models_. <https://arxiv.org/abs/2311.16867>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-almazrouei2023falconseriesopenlanguage-1)
  5. An, C., Huang, F., Zhang, J., Gong, S., Qiu, X., Zhou, C., & Kong, L. (2024). _Training-Free Long-Context Scaling of Large Language Models_. <https://arxiv.org/abs/2402.17463>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-dca-1)
  6. Aryabumi, V., Su, Y., Ma, R., Morisot, A., Zhang, I., Locatelli, A., Fadaee, M., Üstün, A., & Hooker, S. (2024). _To Code, or Not To Code? Exploring Impact of Code in Pre-training_. <https://arxiv.org/abs/2408.10914>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-aryabumi2024codecodeexploringimpact-1)
  7. Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., Hui, B., Ji, L., Li, M., Lin, J., Lin, R., Liu, D., Liu, G., Lu, C., Lu, K., … Zhu, T. (2023). _Qwen Technical Report_. <https://arxiv.org/abs/2309.16609>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-qwen1-1)
  8. Barres, V., Dong, H., Ray, S., Si, X., & Narasimhan, K. (2025). _τ 2-Bench: Evaluating Conversational Agents in a Dual-Control Environment_. <https://arxiv.org/abs/2506.07982>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-taubench-1)
  9. Beck, M., Pöppel, K., Lippe, P., & Hochreiter, S. (2025). _Tiled Flash Linear Attention: More Efficient Linear RNN and xLSTM Kernels_. <https://arxiv.org/abs/2503.14376>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-beck2025tiledflashlinearattention-1)
  10. Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., … Amodei, D. (2020). _Language Models are Few-Shot Learners_. <https://arxiv.org/abs/2005.14165>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-gpt3-1)
  11. Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., … Zaremba, W. (2021). _Evaluating Large Language Models Trained on Code_. <https://arxiv.org/abs/2107.03374>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-codex-1)
  12. Chen, Y., Huang, B., Gao, Y., Wang, Z., Yang, J., & Ji, H. (2025a). _Scaling Laws for Predicting Downstream Performance in LLMs_. <https://arxiv.org/abs/2410.08527>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-chen2025-1)
  13. Chen, Y., Huang, B., Gao, Y., Wang, Z., Yang, J., & Ji, H. (2025b). _Scaling Laws for Predicting Downstream Performance in LLMs_. <https://arxiv.org/abs/2410.08527>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-chen2025scalinglawspredictingdownstream-1)
  14. Child, R., Gray, S., Radford, A., & Sutskever, I. (2019). Generating long sequences with sparse transformers. _arXiv Preprint arXiv:1904.10509_.[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-child2019generating-1)
  15. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., … Fiedel, N. (2022). _PaLM: Scaling Language Modeling with Pathways_. <https://arxiv.org/abs/2204.02311>[1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-palm-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-palm-2), [3](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-palm-3), [4](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-palm-4)
  16. Chu, T., Zhai, Y., Yang, J., Tong, S., Xie, S., Schuurmans, D., Le, Q. V., Levine, S., & Ma, Y. (2025). _SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training_. <https://arxiv.org/abs/2501.17161>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-chu2025-1)
  17. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., & Schulman, J. (2021). _Training Verifiers to Solve Math Word Problems_. <https://arxiv.org/abs/2110.14168>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-gsm8k-1)
  18. Cohere, T., :, Aakanksha, Ahmadian, A., Ahmed, M., Alammar, J., Alizadeh, M., Alnumay, Y., Althammer, S., Arkhangorodsky, A., Aryabumi, V., Aumiller, D., Avalos, R., Aviv, Z., Bae, S., Baji, S., Barbet, A., Bartolo, M., Bebensee, B., … Zhao, Z. (2025). _Command A: An Enterprise-Ready Large Language Model_. <https://arxiv.org/abs/2504.00698>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-commandacohere-1)
  19. Dagan, G., Synnaeve, G., & Rozière, B. (2024). _Getting the most out of your tokenizer for pre-training and domain adaptation_. <https://arxiv.org/abs/2402.01035>[1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-dagan2024gettingtokenizerpretrainingdomain-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-dagan2024gettingtokenizerpretrainingdomain-2)
  20. Dao, T., & Gu, A. (2024). _Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality_. <https://arxiv.org/abs/2405.21060>[1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-mamba2-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-mamba2-2)
  21. DeepSeek-AI. (2025). _DeepSeek-V3.2-Exp: Boosting Long-Context Efficiency with DeepSeek Sparse Attention_. DeepSeek. <https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-dsa-1)
  22. DeepSeek-AI, :, Bi, X., Chen, D., Chen, G., Chen, S., Dai, D., Deng, C., Ding, H., Dong, K., Du, Q., Fu, Z., Gao, H., Gao, K., Gao, W., Ge, R., Guan, K., Guo, D., Guo, J., … Zou, Y. (2024). _DeepSeek LLM: Scaling Open-Source Language Models with Longtermism_. <https://arxiv.org/abs/2401.02954>[1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-deepseekai2024deepseekllmscalingopensource-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-deepseekai2024deepseekllmscalingopensource-2)
  23. DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., … Zhang, Z. (2025). _DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning_. <https://arxiv.org/abs/2501.12948>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-deepseekr1-1)
  24. DeepSeek-AI, Liu, A., Feng, B., Wang, B., Wang, B., Liu, B., Zhao, C., Dengr, C., Ruan, C., Dai, D., Guo, D., Yang, D., Chen, D., Ji, D., Li, E., Lin, F., Luo, F., Hao, G., Chen, G., … Xie, Z. (2024). _DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model_. <https://arxiv.org/abs/2405.04434>[1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-deepseekv2-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-deepseekv2-2)
  25. DeepSeek-AI, Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Guo, D., Yang, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., … Pan, Z. (2025). _DeepSeek-V3 Technical Report_. <https://arxiv.org/abs/2412.19437>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-deepseekv3-1)
  26. Dehghani, M., Djolonga, J., Mustafa, B., Padlewski, P., Heek, J., Gilmer, J., Steiner, A., Caron, M., Geirhos, R., Alabdulmohsin, I., Jenatton, R., Beyer, L., Tschannen, M., Arnab, A., Wang, X., Riquelme, C., Minderer, M., Puigcerver, J., Evci, U., … Houlsby, N. (2023). _Scaling Vision Transformers to 22 Billion Parameters_. <https://arxiv.org/abs/2302.05442>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-dehghani2023scalingvisiontransformers22-1)
  27. Ding, H., Wang, Z., Paolini, G., Kumar, V., Deoras, A., Roth, D., & Soatto, S. (2024). _Fewer Truncations Improve Language Modeling_. <https://arxiv.org/abs/2404.10830>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-bfd-1)
  28. D’Oosterlinck, K., Xu, W., Develder, C., Demeester, T., Singh, A., Potts, C., Kiela, D., & Mehri, S. (2024). _Anchored Preference Optimization and Contrastive Revisions: Addressing Underspecification in Alignment_. <https://arxiv.org/abs/2408.06266>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-apo-1)
  29. Du, Z., Zeng, A., Dong, Y., & Tang, J. (2025). _Understanding Emergent Abilities of Language Models from the Loss Perspective_. <https://arxiv.org/abs/2403.15796>[1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-du2025-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-du2025-2)
  30. Dubois, Y., Galambosi, B., Liang, P., & Hashimoto, T. B. (2025). _Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators_. <https://arxiv.org/abs/2404.04475>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-alpacaeval-1)
  31. Ethayarajh, K., Xu, W., Muennighoff, N., Jurafsky, D., & Kiela, D. (2024). _KTO: Model Alignment as Prospect Theoretic Optimization_. <https://arxiv.org/abs/2402.01306>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-kto-1)
  32. Gandhi, K., Chakravarthy, A., Singh, A., Lile, N., & Goodman, N. D. (2025). _Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs_. <https://arxiv.org/abs/2503.01307>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-cognitivebehaviours-1)
  33. Gao, T., Wettig, A., Yen, H., & Chen, D. (2025). _How to Train Long-Context Language Models (Effectively)_. <https://arxiv.org/abs/2410.02660>[1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-prolong-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-prolong-2), [3](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-prolong-3)
  34. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., Yang, A., Fan, A., Goyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., … Ma, Z. (2024). _The Llama 3 Herd of Models_. <https://arxiv.org/abs/2407.21783>[1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-llama3-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-llama3-2), [3](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-llama3-3)
  35. Gu, A., & Dao, T. (2024). _Mamba: Linear-Time Sequence Modeling with Selective State Spaces_. <https://arxiv.org/abs/2312.00752>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-mamba-1)
  36. Gu, Y., Tafjord, O., Kuehl, B., Haddad, D., Dodge, J., & Hajishirzi, H. (2025). _OLMES: A Standard for Language Model Evaluations_. <https://arxiv.org/abs/2406.08446>[1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-olmes-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-olmes-2)
  37. Guo, S., Zhang, B., Liu, T., Liu, T., Khalman, M., Llinares, F., Rame, A., Mesnard, T., Zhao, Y., Piot, B., Ferret, J., & Blondel, M. (2024). _Direct Language Model Alignment from Online AI Feedback_. <https://arxiv.org/abs/2402.04792>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-onlinedpo-1)
  38. Hägele, A., Bakouch, E., Kosson, A., Allal, L. B., Werra, L. V., & Jaggi, M. (2024). _Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations_. <https://arxiv.org/abs/2405.18392>[1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-wsdhagele-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-wsdhagele-2)
  39. He, Y., Jin, D., Wang, C., Bi, C., Mandyam, K., Zhang, H., Zhu, C., Li, N., Xu, T., Lv, H., Bhosale, S., Zhu, C., Sankararaman, K. A., Helenowski, E., Kambadur, M., Tayade, A., Ma, H., Fang, H., & Wang, S. (2024). _Multi-IF: Benchmarking LLMs on Multi-Turn and Multilingual Instructions Following_. <https://arxiv.org/abs/2410.15553>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-multiif-1)
  40. Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., de Las Casas, D., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., van den Driessche, G., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., … Sifre, L. (2022). _Training Compute-Optimal Large Language Models_. <https://arxiv.org/abs/2203.15556>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-hoffmann2022trainingcomputeoptimallargelanguage-1)
  41. Hong, J., Lee, N., & Thorne, J. (2024). _ORPO: Monolithic Preference Optimization without Reference Model_. <https://arxiv.org/abs/2403.07691>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-orpo-1)
  42. Howard, J., & Ruder, S. (2018). _Universal Language Model Fine-tuning for Text Classification_. <https://arxiv.org/abs/1801.06146>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-ulmfit-1)
  43. Hsieh, C.-P., Sun, S., Kriman, S., Acharya, S., Rekesh, D., Jia, F., Zhang, Y., & Ginsburg, B. (2024). _RULER: What’s the Real Context Size of Your Long-Context Language Models?_ <https://arxiv.org/abs/2404.06654>[1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-ruler-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-ruler-2)
  44. Hu, S., Tu, Y., Han, X., He, C., Cui, G., Long, X., Zheng, Z., Fang, Y., Huang, Y., Zhao, W., Zhang, X., Thai, Z. L., Zhang, K., Wang, C., Yao, Y., Zhao, C., Zhou, J., Cai, J., Zhai, Z., … Sun, M. (2024). _MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies_. <https://arxiv.org/abs/2404.06395>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-hu2024minicpmunveilingpotentialsmall-1)
  45. Huang, S., Noukhovitch, M., Hosseini, A., Rasul, K., Wang, W., & Tunstall, L. (2024). _The N+ Implementation Details of RLHF with PPO: A Case Study on TL;DR Summarization_. <https://arxiv.org/abs/2403.17031>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-ndetailsrlhf-1)
  46. IBM Research. (2025). _IBM Granite 4.0: Hyper-efficient, High Performance Hybrid Models for Enterprise_. <https://www.ibm.com/new/announcements/ibm-granite-4-0-hyper-efficient-high-performance-hybrid-models>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-granite4-1)
  47. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.-A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., & Sayed, W. E. (2023). _Mistral 7B_. <https://arxiv.org/abs/2310.06825>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-jiang2023mistral7b-1)
  48. Kamradt, G. (2023). Needle In A Haystack - pressure testing LLMs. In _GitHub repository_. GitHub. <https://github.com/gkamradt/LLMTest_NeedleInAHaystack>[1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-niah-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-niah-2)
  49. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., & Amodei, D. (2020). _Scaling Laws for Neural Language Models_. <https://arxiv.org/abs/2001.08361>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-kaplan2020scalinglawsneurallanguage-1)
  50. Katsch, T. (2024). _GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling_. <https://arxiv.org/abs/2311.01927>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-katsch2024gateloopfullydatacontrolledlinear-1)
  51. Kazemnejad, A., Padhi, I., Ramamurthy, K. N., Das, P., & Reddy, S. (2023). _The Impact of Positional Encoding on Length Generalization in Transformers_. <https://arxiv.org/abs/2305.19466>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-nope-1)
  52. Khatri, D., Madaan, L., Tiwari, R., Bansal, R., Duvvuri, S. S., Zaheer, M., Dhillon, I. S., Brandfonbrener, D., & Agarwal, R. (2025). _The Art of Scaling Reinforcement Learning Compute for LLMs_. <https://arxiv.org/abs/2510.13786>[1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-scalerl-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-scalerl-2)
  53. Kingma, D. P. (2014). Adam: A method for stochastic optimization. _arXiv Preprint arXiv:1412.6980_.[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-kingma2014adam-1)
  54. Krajewski, J., Ludziejewski, J., Adamczewski, K., Pióro, M., Krutul, M., Antoniak, S., Ciebiera, K., Król, K., Odrzygóźdź, T., Sankowski, P., Cygan, M., & Jaszczur, S. (2024). _Scaling Laws for Fine-Grained Mixture of Experts_. <https://arxiv.org/abs/2402.07871>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-krajewski2024scalinglawsfinegrainedmixture-1)
  55. Lambert, N., Castricato, L., von Werra, L., & Havrilla, A. (2022). Illustrating Reinforcement Learning from Human Feedback (RLHF). _Hugging Face Blog_.[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-rlhf-1)
  56. Lambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., Gu, Y., Malik, S., Graf, V., Hwang, J. D., Yang, J., Bras, R. L., Tafjord, O., Wilhelm, C., Soldaini, L., … Hajishirzi, H. (2025). _Tulu 3: Pushing Frontiers in Open Language Model Post-Training_. <https://arxiv.org/abs/2411.15124>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-tulu3-1)
  57. Lanchantin, J., Chen, A., Lan, J., Li, X., Saha, S., Wang, T., Xu, J., Yu, P., Yuan, W., Weston, J. E., Sukhbaatar, S., & Kulikov, I. (2025). _Bridging Offline and Online Reinforcement Learning for LLMs_. <https://arxiv.org/abs/2506.21495>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-online-offline-1)
  58. Li, J., Fang, A., Smyrnis, G., Ivgi, M., Jordan, M., Gadre, S., Bansal, H., Guha, E., Keh, S., Arora, K., Garg, S., Xin, R., Muennighoff, N., Heckel, R., Mercat, J., Chen, M., Gururangan, S., Wortsman, M., Albalak, A., … Shankar, V. (2025). _DataComp-LM: In search of the next generation of training sets for language models_. <https://arxiv.org/abs/2406.11794>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-datacomp-1)
  59. Li, Q., Cui, L., Zhao, X., Kong, L., & Bi, W. (2024). _GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers_. <https://arxiv.org/abs/2402.19255>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-gsmplus-1)
  60. Li, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., Liu, Q., Zheltonozhskii, E., Zhuo, T. Y., Wang, T., Dehaene, O., Davaadorj, M., Lamy-Poirier, J., Monteiro, J., Shliazhko, O., … de Vries, H. (2023). _StarCoder: may the source be with you!_ <https://arxiv.org/abs/2305.06161>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-starcoder-1)
  61. Li, T., Chiang, W.-L., Frick, E., Dunlap, L., Wu, T., Zhu, B., Gonzalez, J. E., & Stoica, I. (2024). _From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline_. <https://arxiv.org/abs/2406.11939>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-arenahard-1)
  62. Liang, W., Liu, T., Wright, L., Constable, W., Gu, A., Huang, C.-C., Zhang, I., Feng, W., Huang, H., Wang, J., Purandare, S., Nadathur, G., & Idreos, S. (2025). _TorchTitan: One-stop PyTorch native solution for production ready LLM pre-training_. <https://arxiv.org/abs/2410.06511>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-torchtitan-1)
  63. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., & Cobbe, K. (2023). _Let’s Verify Step by Step_. <https://arxiv.org/abs/2305.20050>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-openaiprm-1)
  64. Liu, H., Xie, S. M., Li, Z., & Ma, T. (2022). _Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models_. <https://arxiv.org/abs/2210.14199>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-liu2022-1)
  65. Liu, Q., Zheng, X., Muennighoff, N., Zeng, G., Dou, L., Pang, T., Jiang, J., & Lin, M. (2025). _RegMix: Data Mixture as Regression for Language Model Pre-training_. <https://arxiv.org/abs/2407.01492>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-liu2025regmixdatamixtureregression-1)
  66. Liu, Z., Zhao, C., Iandola, F., Lai, C., Tian, Y., Fedorov, I., Xiong, Y., Chang, E., Shi, Y., Krishnamoorthi, R., Lai, L., & Chandra, V. (2024). _MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases_. <https://arxiv.org/abs/2402.14905>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-mobilellm-1)
  67. Loshchilov, I., & Hutter, F. (2017). _SGDR: Stochastic Gradient Descent with Warm Restarts_. <https://arxiv.org/abs/1608.03983>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-loshchilov2017sgdrstochasticgradientdescent-1)
  68. Lozhkov, A., Li, R., Allal, L. B., Cassano, F., Lamy-Poirier, J., Tazi, N., Tang, A., Pykhtar, D., Liu, J., Wei, Y., Liu, T., Tian, M., Kocetkov, D., Zucker, A., Belkada, Y., Wang, Z., Liu, Q., Abulkhanov, D., Paul, I., … de Vries, H. (2024). _StarCoder 2 and The Stack v2: The Next Generation_. <https://arxiv.org/abs/2402.19173>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-starcoder2-1)
  69. Mao, H. H. (2022). _Fine-Tuning Pre-trained Transformers into Decaying Fast Weights_. <https://arxiv.org/abs/2210.04243>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-mao2022finetuningpretrainedtransformersdecaying-1)
  70. Marafioti, A., Zohar, O., Farré, M., Noyan, M., Bakouch, E., Cuenca, P., Zakka, C., Allal, L. B., Lozhkov, A., Tazi, N., Srivastav, V., Lochner, J., Larcher, H., Morlon, M., Tunstall, L., von Werra, L., & Wolf, T. (2025). _SmolVLM: Redefining small and efficient multimodal models_. <https://arxiv.org/abs/2504.05299>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-smolvlm-1)
  71. McCandlish, S., Kaplan, J., Amodei, D., & Team, O. D. (2018). _An Empirical Model of Large-Batch Training_. <https://arxiv.org/abs/1812.06162>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-mccandlish2018empiricalmodellargebatchtraining-1)
  72. Merrill, W., Arora, S., Groeneveld, D., & Hajishirzi, H. (2025). _Critical Batch Size Revisited: A Simple Empirical Approach to Large-Batch Language Model Training_. <https://arxiv.org/abs/2505.23971>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-merrill2025criticalbatchsizerevisited-1)
  73. Meta AI. (2025). _The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation_. <https://ai.meta.com/blog/llama-4-multimodal-intelligence/>[1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-llama4-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-llama4-2)
  74. Mindermann, S., Brauner, J., Razzak, M., Sharma, M., Kirsch, A., Xu, W., Höltgen, B., Gomez, A. N., Morisot, A., Farquhar, S., & Gal, Y. (2022). _Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt_. <https://arxiv.org/abs/2206.07137>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-mindermann2022prioritizedtrainingpointslearnable-1)
  75. MiniMax, Li, A., Gong, B., Yang, B., Shan, B., Liu, C., Zhu, C., Zhang, C., Guo, C., Chen, D., Li, D., Jiao, E., Li, G., Zhang, G., Sun, H., Dong, H., Zhu, J., Zhuang, J., Song, J., … Wu, Z. (2025). _MiniMax-01: Scaling Foundation Models with Lightning Attention_. <https://arxiv.org/abs/2501.08313>[1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-minimax01-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-minimax01-2), [3](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-minimax01-3)
  76. Mistral AI. (2025). _Mistral Small 3.1_. <https://mistral.ai/news/mistral-small-3-1>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-mistralsmall-1)
  77. Moshkov, I., Hanley, D., Sorokin, I., Toshniwal, S., Henkel, C., Schifferer, B., Du, W., & Gitman, I. (2025). _AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset_. <https://arxiv.org/abs/2504.16891>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-moshkov2025aimo2winningsolutionbuilding-1)
  78. Muennighoff, N., Rush, A. M., Barak, B., Scao, T. L., Piktus, A., Tazi, N., Pyysalo, S., Wolf, T., & Raffel, C. (2025). _Scaling Data-Constrained Language Models_. <https://arxiv.org/abs/2305.16264>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-muennighoff2025scalingdataconstrainedlanguagemodels-1)
  79. Ni, J., Xue, F., Yue, X., Deng, Y., Shah, M., Jain, K., Neubig, G., & You, Y. (2024). _MixEval: Deriving Wisdom of the Crowd from LLM Benchmark Mixtures_. <https://arxiv.org/abs/2406.06565>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-mixeval-1)
  80. Nrusimha, A., Brandon, W., Mishra, M., Shen, Y., Panda, R., Ragan-Kelley, J., & Kim, Y. (2025). _FlashFormer: Whole-Model Kernels for Efficient Low-Batch Inference_. <https://arxiv.org/abs/2505.22758>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-nrusimha2025flashformerwholemodelkernelsefficient-1)
  81. Nvidia, :, Adler, B., Agarwal, N., Aithal, A., Anh, D. H., Bhattacharya, P., Brundyn, A., Casper, J., Catanzaro, B., Clay, S., Cohen, J., Das, S., Dattagupta, A., Delalleau, O., Derczynski, L., Dong, Y., Egert, D., Evans, E., … Zhu, C. (2024). _Nemotron-4 340B Technical Report_. <https://arxiv.org/abs/2406.11704>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-nvidia2024nemotron4340btechnicalreport-1)
  82. NVIDIA, :, Basant, A., Khairnar, A., Paithankar, A., Khattar, A., Renduchintala, A., Malte, A., Bercovich, A., Hazare, A., Rico, A., Ficek, A., Kondratenko, A., Shaposhnikov, A., Bukharin, A., Taghibakhshi, A., Barton, A., Mahabaleshwarkar, A. S., Shen, A., … Chen, Z. (2025). _NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model_. <https://arxiv.org/abs/2508.14444>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-nvidia2025nvidianemotronnano2-1)
  83. NVIDIA, :, Blakeman, A., Basant, A., Khattar, A., Renduchintala, A., Bercovich, A., Ficek, A., Bjorlin, A., Taghibakhshi, A., Deshmukh, A. S., Mahabaleshwarkar, A. S., Tao, A., Shors, A., Aithal, A., Poojary, A., Dattagupta, A., Buddharaju, B., Chen, B., … Chen, Z. (2025). _Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models_. <https://arxiv.org/abs/2504.03624>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-nemotronh-1)
  84. OLMo, T., Walsh, P., Soldaini, L., Groeneveld, D., Lo, K., Arora, S., Bhagia, A., Gu, Y., Huang, S., Jordan, M., Lambert, N., Schwenk, D., Tafjord, O., Anderson, T., Atkinson, D., Brahman, F., Clark, C., Dasigi, P., Dziri, N., … Hajishirzi, H. (2025). _2 OLMo 2 Furious_. <https://arxiv.org/abs/2501.00656>[1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-olmo2-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-olmo2-2), [3](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-olmo2-3)
  85. OpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., Avila, R., Babuschkin, I., Balaji, S., Balcom, V., Baltescu, P., Bao, H., Bavarian, M., Belgum, J., … Zoph, B. (2024). _GPT-4 Technical Report_. <https://arxiv.org/abs/2303.08774>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-gpt4-1)
  86. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., & Lowe, R. (2022). _Training language models to follow instructions with human feedback_. <https://arxiv.org/abs/2203.02155>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-instructgpt-1)
  87. Penedo, G., Kydlíček, H., allal, L. B., Lozhkov, A., Mitchell, M., Raffel, C., Werra, L. V., & Wolf, T. (2024). _The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale_. <https://arxiv.org/abs/2406.17557>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-fineweb-1)
  88. Penedo, G., Kydlíček, H., Sabolčec, V., Messmer, B., Foroutan, N., Kargaran, A. H., Raffel, C., Jaggi, M., Werra, L. V., & Wolf, T. (2025). _FineWeb2: One Pipeline to Scale Them All – Adapting Pre-Training Data Processing to Every Language_. <https://arxiv.org/abs/2506.20920>[1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-fineweb2-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-fineweb2-2)
  89. Peng, B., Goldstein, D., Anthony, Q., Albalak, A., Alcaide, E., Biderman, S., Cheah, E., Du, X., Ferdinan, T., Hou, H., Kazienko, P., GV, K. K., Kocoń, J., Koptyra, B., Krishna, S., Jr., R. M., Lin, J., Muennighoff, N., Obeid, F., … Zhu, R.-J. (2024). _Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence_. <https://arxiv.org/abs/2404.05892>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-peng2024eaglefinchrwkvmatrixvalued-1)
  90. Peng, B., Quesnelle, J., Fan, H., & Shippole, E. (2023). _YaRN: Efficient Context Window Extension of Large Language Models_. <https://arxiv.org/abs/2309.00071>[1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-yarn-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-yarn-2)
  91. Peng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith, N. A., & Kong, L. (2021). _Random Feature Attention_. <https://arxiv.org/abs/2103.02143>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-peng2021randomfeatureattention-1)
  92. Petty, J., van Steenkiste, S., Dasgupta, I., Sha, F., Garrette, D., & Linzen, T. (2024). _The Impact of Depth on Compositional Generalization in Transformer Language Models_. <https://arxiv.org/abs/2310.19956>[1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-petty2024impactdepthcompositionalgeneralization-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-petty2024impactdepthcompositionalgeneralization-2)
  93. Polo, F. M., Weber, L., Choshen, L., Sun, Y., Xu, G., & Yurochkin, M. (2024). _tinyBenchmarks: evaluating LLMs with fewer examples_. <https://arxiv.org/abs/2402.14992>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-tinybenchmarks-1)
  94. Press, O., Smith, N. A., & Lewis, M. (2022). _Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation_. <https://arxiv.org/abs/2108.12409>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-alibi-1)
  95. Pyatkin, V., Malik, S., Graf, V., Ivison, H., Huang, S., Dasigi, P., Lambert, N., & Hajishirzi, H. (2025). _Generalizing Verifiable Instruction Following_. <https://arxiv.org/abs/2507.02833>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-ifbench-1)
  96. Qin, Z., Han, X., Sun, W., Li, D., Kong, L., Barnes, N., & Zhong, Y. (2022). _The Devil in Linear Transformer_. <https://arxiv.org/abs/2210.10340>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-qin2022devillineartransformer-1)
  97. Qin, Z., Yang, S., Sun, W., Shen, X., Li, D., Sun, W., & Zhong, Y. (2024). _HGRN2: Gated Linear RNNs with State Expansion_. <https://arxiv.org/abs/2404.07904>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-qin2024hgrn2gatedlinearrnns-1)
  98. Qiu, Z., Huang, Z., Zheng, B., Wen, K., Wang, Z., Men, R., Titov, I., Liu, D., Zhou, J., & Lin, J. (2025). _Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models_. <https://arxiv.org/abs/2501.11873>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-qiu2025demonsdetailimplementingload-1)
  99. Qwen Team. (2025). _Qwen3-Next: Towards Ultimate Training & Inference Efficiency_. Alibaba Cloud. [https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&from=research.latest-advancements-list](https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&from=research.latest-advancements-list)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-qwen3next-1)
  100. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., & others. (2019). Language models are unsupervised multitask learners. In _OpenAI blog_ (Vol. 1, p. 9).[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-gpt2-1)
  101. Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., & Finn, C. (2024). _Direct Preference Optimization: Your Language Model is Secretly a Reward Model_. <https://arxiv.org/abs/2305.18290>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-dpo-1)
  102. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., & Bowman, S. R. (2024). Gpqa: A graduate-level google-proof q&a benchmark. _First Conference on Language Modeling_.[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-gpqa-1)
  103. Rozière, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Sauvestre, R., Remez, T., Rapin, J., Kozhevnikov, A., Evtimov, I., Bitton, J., Bhatt, M., Ferrer, C. C., Grattafiori, A., Xiong, W., Défossez, A., … Synnaeve, G. (2024). _Code Llama: Open Foundation Models for Code_. <https://arxiv.org/abs/2308.12950>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-rozi%C3%A8re2024codellamaopenfoundation-1)
  104. Sennrich, R., Haddow, B., & Birch, A. (2016). _Neural Machine Translation of Rare Words with Subword Units_. <https://arxiv.org/abs/1508.07909>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-sennrich2016neuralmachinetranslationrare-1)
  105. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y. K., Wu, Y., & Guo, D. (2024). _DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models_. <https://arxiv.org/abs/2402.03300>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-grpo-1)
  106. Shazeer, N. (2019). _Fast Transformer Decoding: One Write-Head is All You Need_. <https://arxiv.org/abs/1911.02150>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-mqa-1)
  107. Shi, F., Suzgun, M., Freitag, M., Wang, X., Srivats, S., Vosoughi, S., Chung, H. W., Tay, Y., Ruder, S., Zhou, D., Das, D., & Wei, J. (2022). _Language Models are Multilingual Chain-of-Thought Reasoners_. <https://arxiv.org/abs/2210.03057>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-mgsm-1)
  108. Shukor, M., Aubakirova, D., Capuano, F., Kooijmans, P., Palma, S., Zouitine, A., Aractingi, M., Pascal, C., Russi, M., Marafioti, A., Alibert, S., Cord, M., Wolf, T., & Cadene, R. (2025). _SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics_. <https://arxiv.org/abs/2506.01844>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-smolvla-1)
  109. Singh, S., Romanou, A., Fourrier, C., Adelani, D. I., Ngui, J. G., Vila-Suero, D., Limkonchotiwat, P., Marchisio, K., Leong, W. Q., Susanto, Y., Ng, R., Longpre, S., Ko, W.-Y., Ruder, S., Smith, M., Bosselut, A., Oh, A., Martins, A. F. T., Choshen, L., … Hooker, S. (2025). _Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation_. <https://arxiv.org/abs/2412.03304>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-globalmmlu-1)
  110. Sirdeshmukh, V., Deshpande, K., Mols, J., Jin, L., Cardona, E.-Y., Lee, D., Kritz, J., Primack, W., Yue, S., & Xing, C. (2025). _MultiChallenge: A Realistic Multi-Turn Conversation Evaluation Benchmark Challenging to Frontier LLMs_. <https://arxiv.org/abs/2501.17399>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-multichallenge-1)
  111. Smith, L. N., & Topin, N. (2018). _Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates_. <https://arxiv.org/abs/1708.07120>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-smith2018superconvergencefasttrainingneural-1)
  112. Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., & Liu, Y. (2023). _RoFormer: Enhanced Transformer with Rotary Position Embedding_. <https://arxiv.org/abs/2104.09864>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-rope-1)
  113. Sun, Y., Dong, L., Zhu, Y., Huang, S., Wang, W., Ma, S., Zhang, Q., Wang, J., & Wei, F. (2024). _You Only Cache Once: Decoder-Decoder Architectures for Language Models_. <https://arxiv.org/abs/2405.05254>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-sun2024cacheoncedecoderdecoderarchitectures-1)
  114. Takase, S., Kiyono, S., Kobayashi, S., & Suzuki, J. (2025). _Spike No More: Stabilizing the Pre-training of Large Language Models_. <https://arxiv.org/abs/2312.16903>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-takase2025spikemorestabilizingpretraining-1)
  115. Team, 5, Zeng, A., Lv, X., Zheng, Q., Hou, Z., Chen, B., Xie, C., Wang, C., Yin, D., Zeng, H., Zhang, J., Wang, K., Zhong, L., Liu, M., Lu, R., Cao, S., Zhang, X., Huang, X., Wei, Y., … Tang, J. (2025). _GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models_. <https://arxiv.org/abs/2508.06471>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-glm45-1)
  116. team, F. C., Copet, J., Carbonneaux, Q., Cohen, G., Gehring, J., Kahn, J., Kossen, J., Kreuk, F., McMilin, E., Meyer, M., Wei, Y., Zhang, D., Zheng, K., Armengol-Estapé, J., Bashiri, P., Beck, M., Chambon, P., Charnalia, A., Cummins, C., … Synnaeve, G. (2025). _CWM: An Open-Weights LLM for Research on Code Generation with World Models_. <https://arxiv.org/abs/2510.02387>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-cwm-1)
  117. Team, G., Kamath, A., Ferret, J., Pathak, S., Vieillard, N., Merhej, R., Perrin, S., Matejovicova, T., Ramé, A., Rivière, M., Rouillard, L., Mesnard, T., Cideron, G., bastien Jean-Grill, Ramos, S., Yvinec, E., Casbon, M., Pot, E., Penchev, I., … Hussenot, L. (2025). _Gemma 3 Technical Report_. <https://arxiv.org/abs/2503.19786>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-gemma3-1)
  118. Team, K., Bai, Y., Bao, Y., Chen, G., Chen, J., Chen, N., Chen, R., Chen, Y., Chen, Y., Chen, Y., Chen, Z., Cui, J., Ding, H., Dong, M., Du, A., Du, C., Du, D., Du, Y., Fan, Y., … Zu, X. (2025). _Kimi K2: Open Agentic Intelligence_. <https://arxiv.org/abs/2507.20534>[1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-kimik2-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-kimik2-2), [3](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-kimik2-3)
  119. Team, L., Han, B., Tang, C., Liang, C., Zhang, D., Yuan, F., Zhu, F., Gao, J., Hu, J., Li, L., Li, M., Zhang, M., Jiang, P., Jiao, P., Zhao, Q., Yang, Q., Shen, W., Yang, X., Zhang, Y., … Zhou, J. (2025). _Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning_. <https://arxiv.org/abs/2510.19338>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-lingteam2025attentionmattersefficienthybrid-1)
  120. Team, L., Zeng, B., Huang, C., Zhang, C., Tian, C., Chen, C., Jin, D., Yu, F., Zhu, F., Yuan, F., Wang, F., Wang, G., Zhai, G., Zhang, H., Li, H., Zhou, J., Liu, J., Fang, J., Ou, J., … He, Z. (2025). _Every FLOP Counts: Scaling a 300B Mixture-of-Experts LING LLM without Premium GPUs_. <https://arxiv.org/abs/2503.05139>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-ling15-1)
  121. Team, M., Xiao, C., Li, Y., Han, X., Bai, Y., Cai, J., Chen, H., Chen, W., Cong, X., Cui, G., Ding, N., Fan, S., Fang, Y., Fu, Z., Guan, W., Guan, Y., Guo, J., Han, Y., He, B., … Sun, M. (2025). _MiniCPM4: Ultra-Efficient LLMs on End Devices_. <https://arxiv.org/abs/2506.07900>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-minicpm4-1)
  122. Tian, C., Chen, K., Liu, J., Liu, Z., Zhang, Z., & Zhou, J. (2025). _Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models_. <https://arxiv.org/abs/2507.17702>[1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-antgroup-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-antgroup-2), [3](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-antgroup-3)
  123. Toshniwal, S., Moshkov, I., Narenthiran, S., Gitman, D., Jia, F., & Gitman, I. (2024). _OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset_. <https://arxiv.org/abs/2402.10176>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-toshniwal2024openmathinstruct118millionmath-1)
  124. Tunstall, L., Beeching, E., Lambert, N., Rajani, N., Rasul, K., Belkada, Y., Huang, S., von Werra, L., Fourrier, C., Habib, N., Sarrazin, N., Sanseviero, O., Rush, A. M., & Wolf, T. (2023). _Zephyr: Direct Distillation of LM Alignment_. <https://arxiv.org/abs/2310.16944>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-zephyr-1)
  125. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2023). _Attention Is All You Need_. <https://arxiv.org/abs/1706.03762>[1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-transformer-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-transformer-2), [3](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-transformer-3)
  126. Waleffe, R., Byeon, W., Riach, D., Norick, B., Korthikanti, V., Dao, T., Gu, A., Hatamizadeh, A., Singh, S., Narayanan, D., Kulshreshtha, G., Singh, V., Casper, J., Kautz, J., Shoeybi, M., & Catanzaro, B. (2024). _An Empirical Study of Mamba-based Language Models_. <https://arxiv.org/abs/2406.07887>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-waleffe2024empiricalstudymambabasedlanguage-1)
  127. Wang, B., & Komatsuzaki, A. (2021). _GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model_. <https://github.com/kingoflolz/mesh-transformer-jax>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-gptj-1)
  128. Wei, J., Karina, N., Chung, H. W., Jiao, Y. J., Papay, S., Glaese, A., Schulman, J., & Fedus, W. (2024). Measuring short-form factuality in large language models. _arXiv Preprint arXiv:2411.04368_.[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-simpleqa-1)
  129. Wen, K., Hall, D., Ma, T., & Liang, P. (2025). _Fantastic Pretraining Optimizers and Where to Find Them_. <https://arxiv.org/abs/2509.02046>[1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-wen2025fantasticpretrainingoptimizers-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-wen2025fantasticpretrainingoptimizers-2)
  130. Xie, S. M., Pham, H., Dong, X., Du, N., Liu, H., Lu, Y., Liang, P., Le, Q. V., Ma, T., & Yu, A. W. (2023). _DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining_. <https://arxiv.org/abs/2305.10429>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-xie2023doremioptimizingdatamixtures-1)
  131. Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P., Hou, R., Martin, L., Rungta, R., Sankararaman, K. A., Oguz, B., Khabsa, M., Fang, H., Mehdad, Y., Narang, S., Malik, K., Fan, A., Bhosale, S., Edunov, S., Lewis, M., … Ma, H. (2023a). _Effective Long-Context Scaling of Foundation Models_. <https://arxiv.org/abs/2309.16039>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-xiong2023effectivelongcontextscalingfoundation-1)
  132. Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P., Hou, R., Martin, L., Rungta, R., Sankararaman, K. A., Oguz, B., Khabsa, M., Fang, H., Mehdad, Y., Narang, S., Malik, K., Fan, A., Bhosale, S., Edunov, S., Lewis, M., … Ma, H. (2023b). _Effective Long-Context Scaling of Foundation Models_. <https://arxiv.org/abs/2309.16039>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-ropeabf-1)
  133. Xu, H., Peng, B., Awadalla, H., Chen, D., Chen, Y.-C., Gao, M., Kim, Y. J., Li, Y., Ren, L., Shen, Y., Wang, S., Xu, W., Gao, J., & Chen, W. (2025). _Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language Models in Math_. <https://arxiv.org/abs/2504.21233>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-phi4reasoning-1)
  134. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., Zheng, C., Liu, D., Zhou, F., Huang, F., Hu, F., Ge, H., Wei, H., Lin, H., Tang, J., … Qiu, Z. (2025). _Qwen3 Technical Report_. <https://arxiv.org/abs/2505.09388>[1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-qwen3-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-qwen3-2), [3](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-qwen3-3)
  135. Yang, A., Yu, B., Li, C., Liu, D., Huang, F., Huang, H., Jiang, J., Tu, J., Zhang, J., Zhou, J., Lin, J., Dang, K., Yang, K., Yu, L., Li, M., Sun, M., Zhu, Q., Men, R., He, T., … Zhang, Z. (2025). _Qwen2.5-1M Technical Report_. <https://arxiv.org/abs/2501.15383>[1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-qwen1million-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-qwen1million-2)
  136. Yang, B., Venkitesh, B., Talupuru, D., Lin, H., Cairuz, D., Blunsom, P., & Locatelli, A. (2025). _Rope to Nope and Back Again: A New Hybrid Attention Strategy_. <https://arxiv.org/abs/2501.18795>[1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-rnope-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-rnope-2)
  137. Yang, G., & Hu, E. J. (2022). _Feature Learning in Infinite-Width Neural Networks_. <https://arxiv.org/abs/2011.14522>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-mup-1)
  138. Yen, H., Gao, T., Hou, M., Ding, K., Fleischer, D., Izsak, P., Wasserblat, M., & Chen, D. (2025). _HELMET: How to Evaluate Long-Context Language Models Effectively and Thoroughly_. <https://arxiv.org/abs/2410.02694>[1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-helmet-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-helmet-2)
  139. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Dai, W., Fan, T., Liu, G., Liu, L., Liu, X., Lin, H., Lin, Z., Ma, B., Sheng, G., Tong, Y., Zhang, C., Zhang, M., Zhang, W., … Wang, M. (2025). _DAPO: An Open-Source LLM Reinforcement Learning System at Scale_. <https://arxiv.org/abs/2503.14476>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-dapo-1)
  140. Yuan, J., Gao, H., Dai, D., Luo, J., Zhao, L., Zhang, Z., Xie, Z., Wei, Y. X., Wang, L., Xiao, Z., Wang, Y., Ruan, C., Zhang, M., Liang, W., & Zeng, W. (2025). _Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention_. <https://arxiv.org/abs/2502.11089>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-nsa-1)
  141. Yue, Y., Chen, Z., Lu, R., Zhao, A., Wang, Z., Yue, Y., Song, S., & Huang, G. (2025). _Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?_ <https://arxiv.org/abs/2504.13837>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-yue2025-1)
  142. Zhao, Y., Qu, Y., Staniszewski, K., Tworkowski, S., Liu, W., Miłoś, P., Wu, Y., & Minervini, P. (2024). Analysing The Impact of Sequence Composition on Language Model Pre-Training. _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_ , 7897–7912. [10.18653/v1/2024.acl-long.427](https://doi.org/10.18653/v1/2024.acl-long.427)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-zhao2024-1)
  143. Zhou, F., Wang, Z., Ranjan, N., Cheng, Z., Tang, L., He, G., Liu, Z., & Xing, E. P. (2025). _MegaMath: Pushing the Limits of Open Math Corpora_. <https://arxiv.org/abs/2504.02807>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-zhou2025megamathpushinglimitsopen-1)
  144. Zhou, J., Lu, T., Mishra, S., Brahma, S., Basu, S., Luan, Y., Zhou, D., & Hou, L. (2023). _Instruction-Following Evaluation for Large Language Models_. <https://arxiv.org/abs/2311.07911>[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-ifeval-1)
  145. Zhu, T., Liu, Q., Wang, H., Chen, S., Gu, X., Pang, T., & Kan, M.-Y. (2025). _SkyLadder: Better and Faster Pretraining via Context Window Scheduling_. <https://arxiv.org/abs/2503.15450>[1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-skyladder-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-skyladder-2)
  146. Zuo, J., Velikanov, M., Chahed, I., Belkada, Y., Rhayem, D. E., Kunsch, G., Hacid, H., Yous, H., Farhat, B., Khadraoui, I., Farooq, M., Campesan, G., Cojocaru, R., Djilali, Y., Hu, S., Chaabane, I., Khanna, P., Seddik, M. E. A., Huynh, N. D., … Frikha, S. (2025). _Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance_. <https://arxiv.org/abs/2507.22448>[1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-falconh1-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-falconh1-2)

### 腳註

  1. The idea to compute these statistics comes from the Llama 3 tech report ([Grattafiori et al., 2024](https://arxiv.org/abs/2407.21783)). [](https://huggingfacetb-smol-training-playbook.hf.space/#user-content-fnref-f1)
  2. For vLLM see: [Reasoning parsers, ](https://docs.vllm.ai/en/v0.10.1.1/features/reasoning_outputs.html)[Tool parsers](https://huggingfacetb-smol-training-playbook.hf.space/2421384ebcac80fbaa7cf939fc39269d). For SGLang, see: [Reasoning parsers, ](https://docs.sglang.ai/advanced_features/separate_reasoning.html)[Tool parsers](https://docs.sglang.ai/advanced_features/tool_parser.html) [](https://huggingfacetb-smol-training-playbook.hf.space/#user-content-fnref-f2)
  3. The Transformers team has recently added [parsers](https://huggingface.co/docs/transformers/main/en/chat_response_parsing) for extract tool calling and reasoning outputs. If adopted by engines like vLLM, the compatibility criterion may become less important in the future. [](https://huggingfacetb-smol-training-playbook.hf.space/#user-content-fnref-f3)
Made with ❤️ with [research article template](https://huggingface.co/spaces/tfrere/research-article-template)

