### [建立彈性訓練系統](https://huggingfacetb-smol-training-playbook.hf.space/#building-resilient-training-systems)

擁有快速的硬體只是擁有良好且穩定的 LLM 訓練基礎設施的入場券。要從訓練業餘者成為專業人士,我們需要超越原始速度的思考,專注於那些不太迷人但至關重要的基礎設施部分,這些部分使整個訓練體驗更順暢,並將停機時間降至最低。

在本節中,我們從硬體和軟體優化轉向**生產就緒**:建構**強健**到足以在不可避免的失敗中倖存、**自動化**到足以在沒有持續監督的情況下執行,以及**靈活**到足以在出現問題時適應的系統。

#### [**節點健康監控和更換**](https://huggingfacetb-smol-training-playbook.hf.space/#node-health-monitoring-and-replacement)

擁有足夠快的 GPU 對訓練很重要,但由於 LLM 訓練執行數週或數月而不是單日,隨著時間追蹤 GPU 健康變得至關重要。通過初始基準測試的 GPU 可能在長時間訓練執行期間出現熱節流、記憶體錯誤或效能降級。在本節中,我們將分享我們如何應對這一挑戰以及我們使用的工具。

**前期測試:** 在啟動 SmolLM3 之前,我們使用多個工具執行了全面的 GPU 診斷。我們使用 [GPU Fryer](https://github.com/huggingface/gpu-fryer),一個內部工具,對 GPU 進行壓力測試以檢測熱節流、記憶體錯誤和效能異常。我們還執行了 [NVIDIA's DCGM diagnostics](https://docs.nvidia.com/datacenter/dcgm/latest/user-guide/dcgm-diagnostics.html),這是一個廣泛使用的工具,用於驗證 GPU 硬體、監控效能,並透過涵蓋計算、PCIe 連接、記憶體完整性和熱穩定性的深度診斷測試識別失敗或功率異常的根本原因。這些前期測試捕獲了兩個有問題的 GPU,它們本來會在訓練期間造成問題。

你可以在下表中看到可以使用 DCGM 診斷工具測試什麼:

Test Level | Duration | Software | PCIe + NVLink | GPU Memory | Memory Bandwidth | Diagnostics | Targeted Stress | Targeted Power | NVBandwidth | Memory Stress | Input EDPp
---|---|---|---|---|---|---|---|---|---|---|---
r1 (Short) | Seconds | ✓ | ✓ | ✓ |  |  |  |  |  |  |
r2 (Medium) | < 2 mins | ✓ | ✓ | ✓ | ✓ | ✓ |  |  |  |  |
r3 (Long) | < 30 mins | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |  |
r4 (Extra Long) | 1-2 hours | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓

_DCGM 診斷執行級別。來源:_ [_NVIDIA DCGM Documentation_](https://docs.nvidia.com/datacenter/dcgm/latest/user-guide/dcgm-diagnostics.html#run-levels-and-tests)

```

$ dcgmi diag -r 2 -v -d VERB
Successfully ran diagnostic for group.
+---------------------------+------------------------------------------------+
| Diagnostic | Result |

+===========================+================================================+
| -----  Metadata  ----------+------------------------------------------------ |
| DCGM Version | 3.3.1 |
| Driver Version Detected | 575.57.08 |
| GPU Device IDs Detected | 2330,2330,2330,2330,2330,2330,2330,2330 |
| -----  Deployment  --------+------------------------------------------------ |
| Denylist | Pass |
| NVML Library | Pass |
| CUDA Main Library | Pass |
| Permissions and OS Blocks | Pass |
| Persistence Mode | Pass |
| Environment Variables | Pass |
| Page Retirement/Row Remap | Pass |
| Graphics Processes | Pass |
| Inforom | Pass |

+-----  Integration  -------+------------------------------------------------+
| PCIe | Pass - All |
| Info | GPU 0 GPU to Host bandwidth:  14.26 GB/s, GPU |
| 0 Host to GPU bandwidth:  8.66 GB/s, GPU 0 b |
| idirectional bandwidth: 10.91 GB/s, GPU 0 GPU |
| to Host latency:  2.085 us, GPU 0 Host to GP |
| U latency:  2.484 us, GPU 0 bidirectional lat |
| ency:  3.813 us |

...
+-----  Hardware  ----------+------------------------------------------------+
| GPU Memory | Pass - All |
| Info | GPU 0 Allocated 83892938283 bytes (98.4%) |
| Info | GPU 1 Allocated 83892938283 bytes (98.4%) |
| Info | GPU 2 Allocated 83892938283 bytes (98.4%) |
| Info | GPU 3 Allocated 83892938283 bytes (98.4%) |
| Info | GPU 4 Allocated 83892938283 bytes (98.4%) |
| Info | GPU 5 Allocated 83892938283 bytes (98.4%) |
| Info | GPU 6 Allocated 83892938283 bytes (98.4%) |
| Info | GPU 7 Allocated 83892938283 bytes (98.4%) |

+-----  Stress  ------------+------------------------------------------------+

```

**節點預留:** 由於 SmolLM3 是在 Slurm 管理的叢集上訓練的,我們為整個執行預訂了固定的 48 個節點。這種設定使我們能夠隨著時間追蹤完全相同節點的健康和效能,這對於解決我們討論過的資料儲存問題也是必要的。我們還預留了一個備用節點(像汽車的備胎),所以如果一個失敗了,我們可以立即替換它,而無需等待修復。在閒置時,備用節點執行評估作業或開發實驗。

**持續監控:** 在訓練期間,我們追蹤所有節點的關鍵指標,如 GPU 溫度、記憶體使用、計算利用率和輸送量波動。我們使用 [Prometheus](https://prometheus.io/) 從所有 GPU 收集 [DCGM](https://github.com/NVIDIA/DCGM) 指標,並在 [Grafana](https://grafana.com/) 儀表板中視覺化它們以進行即時監控。有關在 AWS 基礎設施上部署 Prometheus 和 Grafana 進行 GPU 監控的詳細設定說明,請參閱 [這個範例設定指南](https://github.com/aws-samples/awsome-distributed-training/tree/3ae961d022399021cc4053c3ba19b182ca6b8dc8/4.validation_and_observability/4.prometheus-grafana)。Slack 機器人在任何節點顯示可疑行為時警告我們,使我們能夠在失敗硬體崩潰整個訓練執行之前主動更換它。

[存取儀表板](https://huggingfacetb-smol-training-playbook.hf.space/screencapture-grafana-huggingface.pdf) 這種多層方法意味著硬體問題變成了可管理的中斷。

**熱現實檢查:當 GPU 減速時**

行銷規格假設完美的冷卻,但現實更混亂。GPU 在過熱時會自動降低時脈速度,即使在精心設計的系統中也會將效能降至理論最大值以下。

![Image](https://huggingfacetb-smol-training-playbook.hf.space/_astro/image_27d1384e-bcac-80b1-9ffb-ec29d0021ccc.D54wWyJ9_2jmnNO.webp)

這個 Grafana 儀表板顯示了我們 GPU 叢集中的熱節流事件。底部面板中的條形表示 GPU 何時由於過熱而自動降低時脈速度。

我們監控來自 [NVIDIA's DCGM](https://github.com/NVIDIA/DCGM/tree/master) 的 `DCGM_FI_DEV_CLOCK_THROTTLE_REASONS` 指標以檢測熱節流。當此指標顯示非零值時,GPU 由於過熱而自動降低時脈速度。上面的儀表板顯示了這些節流事件在實務中如何表現。

熱節流不僅傷害受影響的 GPU;它會波及整個分散式訓練設定。在我們的測試期間,我們觀察到單個節流節點如何顯著影響集體通訊效能。

我們壓力測試期間跨節點的 AllReduce 頻寬降級。14 個節點後的急劇下降(從 350 GB/s 到 100 GB/s)是由單個熱節流 GPU 引起的,展示了一個慢節點如何成為整個分散式訓練管線的瓶頸。

上圖顯示了當我們從 1 個節點擴展到 16 個節點時 AllReduce 頻寬降級。注意 14 個節點後的急劇下降,從 **350 GB/s** 到 **100 GB/s**,而我們期望頻寬保持在 300GB/s 以上,正如我們之前看到的。這不是網路問題:單個具有熱節流的節點成為瓶頸,迫使所有其他節點在梯度同步期間等待。在分散式訓練中,你的速度只有最慢節點那麼快。

👉 **關鍵教訓:** 在承諾長時間訓練執行之前,使用前面提到的工具對硬體進行壓力測試,以識別熱和功率限制。使用 DCGM 遙測持續監控溫度,並為實際熱限制做計劃。驗證 GPU 時脈設定為最大效能也是一個好習慣。要深入了解為什麼 GPU 由於功率約束無法維持其廣告效能,請參閱 [這篇關於功率節流的出色分析](https://www.thonking.ai/p/strangely-matrix-multiplications)。

#### [**Checkpoint 管理**](https://huggingfacetb-smol-training-playbook.hf.space/#checkpoint-management)

Checkpoints 是我們在長時間訓練執行期間的安全網。我們定期儲存它們有三個實際原因:從失敗中恢復、透過評估監控訓練進度以及與社群分享中間模型進行研究。恢復方面最重要。如果我們的執行失敗,我們希望從最新儲存的 checkpoint 重新啟動,這樣如果我們立即恢復,我們最多失去儲存間隔(例如,如果我們每 4 小時儲存一次,則為 4 小時的訓練)。

自動化你的恢復程序

嘗試自動化你的恢復程序。例如,在 Slurm 上,你可以只使用 `SBATCH --requeue`,這樣作業會自動從最新的 checkpoint 重新啟動。這樣,你可以避免浪費時間等待某人注意到失敗並手動重新啟動。

在實現你的恢復機制時,有兩個重要的細節要記住:
  - Checkpoint 儲存應該在背景進行,不影響訓練輸送量。
  - 注意你的儲存,在 24 天的執行中,每 4 小時儲存一次意味著~144 個 checkpoints。對於大型模型和優化器狀態,這會快速累積。在我們的情況下,我們一次只儲存一個本地 checkpoint(最新儲存的),並將其餘的卸載到 S3,以避免填滿叢集儲存。

**來自過去的痛苦教訓:**

在我們的第一次大規模執行(StarCoder 15B)期間,訓練在多次重新啟動後順利進行。在最後一天,我們發現整個 checkpoint 資料夾已被腳本最後的遺留 `rm -rf $CHECKPOINT_PATH` 命令刪除,這是來自舊輸送量測試的。這個破壞性命令只在 Slurm 作業實際完成時觸發,這在之前的重新啟動中沒有發生。

幸運的是,我們儲存了前一天的 checkpoint,所以它只花費了我們一天的重新訓練。教訓很明確:永遠不要在生產腳本中留下破壞性命令,並在儲存後立即自動化 checkpoint 備份,而不是依賴手動干預。

在我們的 nanotron 訓練中,我們每 2 小時在本地儲存 checkpoints,立即將每個上傳到 S3,然後在確認備份後刪除本地副本。恢復時,如果最新的 checkpoint 不在本地可用,我們從 S3 拉取。這種方法節省儲存,確保備份,並實現快速恢復。

#### [**自動化評估**](https://huggingfacetb-smol-training-playbook.hf.space/#automated-evaluations)

手動執行評估很快就會成為瓶頸。它們看起來很簡單,直到你重複進行它們。執行基準測試、追蹤和繪製每次執行的結果累積成顯著的開銷。解決方案?預先自動化一切。

對於 SmolLM3,我們使用 [LightEval](https://github.com/huggingface/lighteval) 在 nanotron checkpoints 上執行評估。每個儲存的 checkpoint 都會在叢集上觸發評估作業。結果直接推送到 Weights & Biases 或 [Trackio](https://github.com/gradio-app/trackio),所以我們只需打開儀表板並觀察曲線演變。這為我們節省了大量時間,並在整個執行過程中保持評估追蹤一致。

如果你只能在訓練設定中自動化一件事,那就自動化評估。

最後,讓我們看看如何優化訓練佈局,即模型如何分佈在可用的 GPU 上,以最大化輸送量。
