### [什麼是好的資料混合以及為什麼它最重要](https://huggingfacetb-smol-training-playbook.hf.space/#whats-a-good-data-mixture-and-why-it-matters-most)

我們對語言模型期望很多,它們應該能夠幫助我們編寫程式碼、給我們建議、回答幾乎任何問題、使用工具完成任務等等。像網路這樣豐富的預訓練資料來源並不涵蓋這些任務所需的全部知識和能力範圍。因此,最近的模型還依賴於針對數學和編碼等特定領域的更專業的預訓練資料集。我們在策劃資料集方面做了很多過去的工作,但對於 SmolLM3,我們主要利用了預先存在的資料集。要了解更多關於資料集策劃的資訊,請查看我們關於建立 [FineWeb 和 FineWeb-Edu](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1)、[FineWeb2](https://arxiv.org/abs/2506.20920)、[Stack-Edu 和 FineMath](https://arxiv.org/abs/2502.02737) 的報告。

#### [**資料混合的反直覺本質**](https://huggingfacetb-smol-training-playbook.hf.space/#the-unintuitive-nature-of-data-mixtures)

如果你是訓練語言模型的新手,找到一個好的資料混合可能看起來很簡單:確定你的目標能力,為每個領域收集高品質資料集,並組合它們。現實更複雜,因為某些領域可能會為你的訓練預算相互競爭。當專注於某些特定能力如編碼時,很容易增加任務相關資料(如原始碼)的權重。然而,增加一個來源的權重會隱含地降低所有其他來源的權重,這可能會損害語言模型在其他設置中的能力。因此,在不同來源的集合上訓練涉及在下游能力之間取得某種平衡。

此外,在所有這些來源和領域中,通常有一個「高品質」資料的子集,特別有助於改善語言模型的能力。為什麼不只是丟掉所有較低品質的資料,只在最高品質的資料上訓練?對於 SmolLM3 的 11T tokens 大訓練預算,進行如此極端的過濾將導致多次重複資料。先前的工作表明,這種重複可能是有害的 ([Muennighoff et al., 2025](https://arxiv.org/abs/2305.16264)),因此我們理想情況下應該能夠利用更高和更低的品質,同時仍然最大化模型效能。

為了平衡跨來源的資料並利用高品質資料,我們需要仔細設計_混合_:來自每個來源的訓練文件的相對比例。由於語言模型在某些特定任務或領域上的效能在很大程度上取決於它看到的與該任務相關的資料量,調整混合權重提供了一種在領域之間平衡模型能力的直接方法。因為這些權衡是依賴於模型的並且難以預測,消融實驗是必不可少的。

但混合不必在整個訓練過程中保持固定。透過隨著訓練的進展調整混合,我們稱之為多階段訓練或課程,我們可以更好地利用高品質和較低品質的資料。

#### [**訓練課程的演變**](https://huggingfacetb-smol-training-playbook.hf.space/#the-evolution-of-training-curricula)

在大型語言模型訓練的早期,標準方法是為整個訓練執行固定單一資料混合。像 GPT3 和早期版本的 Llama 這樣的模型從頭到尾在靜態混合上訓練。最近,該領域已經轉向**多階段訓練** ([Allal et al., 2025](https://arxiv.org/abs/2502.02737)),其中資料混合在訓練過程中發生變化。主要動機是語言模型的最終行為受到訓練結束時看到的資料的強烈影響 ([Y. Chen et al., 2025b](https://arxiv.org/abs/2410.08527))。這一洞見使一個實用策略成為可能:在訓練早期增加更豐富來源的權重,在結束時混合較小、更高品質的來源。

一個常見問題是:你如何決定何時改變混合?雖然沒有普遍規則,但我們通常遵循這些原則:

  1. **效能驅動的介入**:監控關鍵基準測試的評估指標,並調整資料集混合以解決特定的能力瓶頸。例如,如果數學效能達到平台期而其他能力繼續改善,這是引入更高品質數學資料的訊號。
  2. **為後期階段保留高品質資料**:小型高品質數學和程式碼資料集在退火階段(具有學習率 decay 的最後階段)引入時最有影響力。

現在我們已經確定了為什麼混合很重要以及課程如何工作,讓我們討論如何調整兩者。
