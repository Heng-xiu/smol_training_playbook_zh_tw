## [結論](https://huggingfacetb-smol-training-playbook.hf.space/#conclusion)

我們從一個簡單的問題開始這段旅程:在 2025 年訓練高效能 LLM 實際上需要什麼?在走過完整的管線—從預訓練到後訓練—我們向你展示的不僅是技術,還有使它們工作的方法論。

**大規模預訓練。** 我們透過訓練羅盤框架來決定是否要訓練,然後展示了如何將目標轉化為具體的架構決策。你已經看到如何設定可靠的消融管線、單獨測試變更,以及從數十億 token 實驗擴展到數兆 token 執行。我們記錄了可能在規模上出現的基礎設施挑戰(輸送量崩潰、dataloader 瓶頸、微妙的錯誤)以及監控和系統性降低風險如何幫助你早期捕捉它們並快速除錯。

**實務中的後訓練。** 我們展示了從基礎模型到生產助手需要自己的系統方法:在訓練任何東西之前建立評估、迭代 SFT 資料混合、應用偏好優化,以及可選地使用 RL 進一步推進。你已經看到 vibe testing 如何捕捉指標錯過的錯誤、chat templates 如何悄悄破壞指令遵循,以及為什麼資料混合平衡在後訓練中和在預訓練中一樣重要。

在這兩個階段中,我們不斷回到相同的核心見解:透過實驗驗證一切、一次改變一件事、期望規模以新的方式破壞事物,以及讓你的用例驅動決策而不是追逐每一篇新論文。遵循這個過程,我們訓練了 SmolLM3:一個具有長上下文的有競爭力的 3B 多語言推理器。在這個過程中,我們學到了很多關於什麼有效、什麼失敗,以及當事情出錯時如何除錯。我們試圖記錄所有這些,成功和失敗都一樣。

**接下來是什麼?**

這個部落格涵蓋了現代 LLM 訓練的基礎知識,但該領域發展迅速。以下是深入的方法:
  - **自己執行實驗。** 閱讀關於消融的內容很有用;執行你自己的實驗會教你什麼真正重要。選擇一個小型模型,設定評估,然後開始實驗。
  - **閱讀原始碼。** 像 nanotron、TRL 等訓練框架都是開源的。理解它們的實現揭示了論文掩蓋的細節。
  - **關注最近的工作。** 最近最先進模型的論文顯示了該領域的發展方向。參考文獻部分包含我們精選的有影響力的論文和資源列表。

我們希望這個部落格幫助你以清晰和信心接近你的下一個訓練專案,無論你是在大型實驗室推動前沿還是小團隊解決特定問題。

現在去訓練一些東西。當你的 loss 在凌晨 2 點神秘地飆升時,記住:每個偉大的模型背後都有除錯故事。願開源和開放科學的力量永遠與你同在!

#### [**致謝**](https://huggingfacetb-smol-training-playbook.hf.space/#acknowledgments)

我們感謝 [Guilherme](https://huggingface.co/guipenedo)、[Hugo](https://huggingface.co/hlarcher) 和 [Mario](https://huggingface.co/mariolr) 的寶貴回饋,以及 [Abubakar](https://huggingface.co/abidlabs) 對 Trackio 功能的幫助。
