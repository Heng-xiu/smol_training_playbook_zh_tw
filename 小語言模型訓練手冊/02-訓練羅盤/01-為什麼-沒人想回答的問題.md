### 為什麼：沒人想回答的問題

讓我們直白地說說現實中發生的事。某人（如果夠幸運的話）獲得了一個 GPU 叢集的存取權限，可能是透過研究經費，可能是公司的閒置算力，然後思維過程大致是這樣的：「我們有 100 塊 H100，可以用三個月。來訓練個模型吧！」模型大小被隨意選定，資料集從手頭能找到的東西拼湊而成。訓練開始了。六個月後，算力預算和團隊士氣都燒光了，訓練出的模型閒置未用——因為從來沒人問過**為什麼**。

以下是一些**不應該**訓練模型的理由：

**「我們有空閒算力」**
→ 那是資源，不是目標

**「其他人都在做」**
→ 那是同儕壓力，不是戰略

**「AI 是未來」**
→ 那是陳詞濫調，不是計畫

**「我們想要最好的模型」**
→ 這不夠具體，無法指導決策

「我們訓練了自己的模型」這句話的誘惑力很強大，但在投入大量時間和資源之前，有必要問一問：**你為什麼需要訓練這個模型？**

下面的流程圖指導你在開始大型預訓練專案前應該經歷的思考過程。從技術角度講，你基本上應該首先弄清楚是否已經有一個可以透過提示詞或微調來完成工作的現有模型。

```
是否應該訓練自己的模型？
↓
現有模型能處理你的用例嗎？
├─ 是 → 僅用提示詞就能工作？
│         ├─ 是 → ❌ 別訓練，用現有模型
│         └─ 否 → 微調能解決問題嗎？
│                   ├─ 是 → ❌ 別從零訓練（用後訓練/持續預訓練）
│                   └─ 否 → 在以下類別之一訓練模型：
│                             🔬 研究
│                             🏭 生產
│                             🌐 戰略開源
└─ 否 → （繼續評估）
```

我們討論的「為什麼」是關於從零開始訓練。本文不涵蓋蒸餾或剪枝。它們是通往高效模型的有效路徑，但代表的是與從零訓練不同的工作流程。我們推薦 NVIDIA 的 [Minitron 論文](https://arxiv.org/abs/2408.11796)來了解這些主題。

定製預訓練有意義的場景本質上有三種：你想做新穎的研究、你有非常特定的生產用例需求、或者你想填補開源模型生態系統中的空白。讓我們快速看看每一種：

#### 研究：你想理解什麼？

在大語言模型領域可以做的研究有很多。大語言模型研究專案的共同點是，你通常從一個清晰定義的問題開始：

- 我們能將這個新優化器的訓練擴展到 10B+ 模型嗎？來自 [Muon is Scalable for LLM Training](https://huggingface.co/papers/2502.16982)
- 僅靠強化學習，不用 SFT，能產生推理能力嗎？來自 [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://huggingface.co/papers/2501.12948)
- 我們能僅在純合成教科書資料上訓練出好的小模型嗎？來自 [Textbooks Are All You Need](https://huggingface.co/papers/2306.11644)
- 我們能僅在開放授權資料上訓練就達到有競爭力的效能嗎？來自 [The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text](https://huggingface.co/papers/2506.05209)

將假設盡可能具體化，並思考必要的實驗規模，可以提高成功的機會。

#### 生產：為什麼不能用現有模型？

公司不能使用現成模型的主要原因有三個。其中兩個是技術性的，另一個是治理相關的。

訓練自己模型的第一個原因是**領域特異性**：當你的資料或任務涉及現有模型無法很好處理的高度專業化詞彙或結構時。例如：
- 具有獨特詞彙表和長程依賴關係的 DNA 模型
- 需要深度熟悉領域特定術語和邏輯的法律或金融模型

第二個相關原因是部署約束：當你需要一個針對你的硬體、延遲或隱私要求量身定製的模型時。例如，在無人機上執行的 LLM，或在使用 FPGA 等定製硬體的本地系統上執行。

這裡有個簡單的測試：花幾天時間在 Qwen3、Gemma3 或其他當前 SOTA 模型之上構建。你能透過提示詞、工具使用或後訓練達到效能目標嗎？如果不能，那可能是時候訓練自己的模型了。

即使滿足你需求所需的後訓練預算很龐大，它可能仍然比從零開始便宜。為 1T tokens 微調你的模型仍然比從零開始訓練 10T+ tokens 更經濟。

> 💡 此時大語言模型訓練者開始奇蹟般地稱其為中期訓練而不是後訓練

構建自己內部語言模型的第三個原因是**安全與治理**：你需要完全控制訓練資料、模型行為和更新週期，因為你處於受監管的行業或高風險應用中。你需要**確切**知道模型中包含了什麼，並能夠向監管機構證明。在某些情況下，你可能別無選擇，只能構建自己的模型。

這些是公司訓練內部模型的主要原因，但那些發布開源模型的公司或組織呢？

#### 戰略開源：你看到可以填補的空白了嗎？

經驗豐富的 AI 實驗室發布新開源模型的最常見原因之一，是他們在開源生態系統中發現了特定的空白或新的 AI 用例。

典型的模式是這樣的：你注意到一個未被充分探索的領域，也許沒有強大的支援超長上下文的端側模型，或者多語言模型存在但在低資源語言上很弱，或者領域正在向像 [Genie3](https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/) 這樣的互動式世界模型發展，但沒有好的開放權重模型存在。

你有理由相信自己能做得更好；也許你策劃了更好的訓練資料、開發了更好的訓練配方、或者有算力在別人不能的地方過度訓練。你的目標是具體的：不是「史上最好的模型」，而是「最好的 3B 端側模型」或「第一個支援 1M 上下文的小模型」。

這是一個真實的目標，成功會創造價值：開發者採用你的模型，它成為他人的基礎設施，或建立技術信譽。但成功需要經驗。你需要知道什麼是真正可行的，以及如何在競爭空間中可靠地執行。為了讓這更具體，讓我們看看 Hugging Face 是如何思考這個問題的。

#### Hugging Face 的旅程

那麼 Hugging Face 為什麼訓練開源模型呢？答案很簡單：我們構建對開源生態系統有用的東西，填補很少有人在填補的空白。

儘管有數百萬個開放權重模型，但真正訓練完全開放模型的組織卻很少。除了 Hugging Face，還有 [Ai2](https://allenai.org/) 和 [Stanford 的 Marin 社群](https://marin.community/)。

這包括資料集、工具和訓練模型。我們開始的每個 LLM 訓練專案都始於注意到一個空白，並相信我們可以貢獻一些有意義的東西。

我們在 GPT-3 ([Brown et al., 2020](https://arxiv.org/abs/2005.14165)) 發布後開始了第一個 LLM 專案。當時感覺沒有人在構建開放的替代品，我們擔心這些知識最終會被鎖在少數幾個工業實驗室裡。所以我們啟動了 [BigScience workshop](https://bigscience.huggingface.co/) 來訓練 GPT-3 的開放版本。最終的模型是 [Bloom](https://huggingface.co/bigscience/bloom)，來自數十位貢獻者工作了一年來構建訓練堆疊、分詞器和預訓練語料庫，以預訓練一個 175B 參數的模型。

Bloom 的繼任者是 2022 年的 StarCoder ([Li et al., 2023](https://arxiv.org/abs/2305.06161))。OpenAI 為 GitHub Copilot 開發了 Codex ([Chen et al., 2021](https://arxiv.org/abs/2107.03374))，但它是閉源的。構建一個開源替代品顯然會為生態系統提供價值。因此，在 [BigCode](https://huggingface.co/bigcode) 的保護下，與 ServiceNow 合作，我們構建了 [The Stack](https://huggingface.co/datasets/bigcode/the-stack) 資料集，並訓練了 [StarCoder 15B](https://huggingface.co/bigcode/starcoder) 來複現 Codex。[StarCoder2](https://huggingface.co/collections/bigcode/starcoder2-65de6da6e87db3383572be1a) ([Lozhkov et al., 2024](https://arxiv.org/abs/2402.19173)) 來自我們認識到可以訓練更久，以及意識到訓練更久的小模型可能比一個大模型更有價值。我們訓練了一個系列（3B/7B/15B），在數萬億 tokens 上訓練，遠超當時任何人為開源程式碼模型所做的。

[SmolLM 系列](https://huggingface.co/HuggingFaceTB)遵循了類似的模式。我們注意到很少有強大的小模型，而我們剛剛構建了 [FineWeb-Edu](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu) ([Penedo et al., 2024](https://arxiv.org/abs/2406.17557))，這是一個強大的預訓練資料集。[SmolLM](https://huggingface.co/collections/HuggingFaceTB/smollm-6695016cad7167254ce15966) (135M/360M/1.7B) 是我們的第一個版本。[SmolLM2](https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9) ([Allal et al., 2025](https://arxiv.org/abs/2502.02737)) 專注於更好的資料和更長的訓練，在多個方面達到了 SOTA 效能。[SmolLM3](https://huggingface.co/HuggingFaceTB/SmolLM3-3B) 擴展到 3B，同時添加了混合推理、多語言和長上下文——這些是社群在 2025 年重視的特性。

這種模式延伸到預訓練之外：我們訓練了 [Zephyr](https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha) ([Tunstall et al., 2023](https://arxiv.org/abs/2310.16944)) 來展示 DPO 在規模上的效果，啟動了 [Open-R1](https://github.com/huggingface/open-r1) 來複現 DeepSeek R1 的蒸餾管線，並發布了 [OlympicCoder](https://huggingface.co/open-r1/OlympicCoder-7B) 用於競技程式設計，在國際資訊學奧林匹克競賽中達到 SOTA 效能。我們還探索了其他模態，有用於視覺的 [SmolVLM](https://huggingface.co/collections/HuggingFaceTB/smolvlm-6740bd584b2dcbf51ecb1f39) ([Marafioti et al., 2025](https://arxiv.org/abs/2504.05299)) 和用於機器人的 [SmolVLA](https://huggingface.co/lerobot/smolvla_base) ([Shukor et al., 2025](https://arxiv.org/abs/2506.01844))。

> 如果你對 HF 科學專案感興趣，可以在這裡找到概覽：<https://huggingface.co/science>

希望這個章節已經說服你，深入思考為什麼要訓練模型是有價值的。

在本文的其餘部分，我們將假設你已經做過這種靈魂拷問，並有一個合理的理由去訓練。
