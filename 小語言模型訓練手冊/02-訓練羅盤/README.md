## 訓練羅盤：為什麼 → 是什麼 → 怎麼做

**為什麼要訓練？**
- 研究
- 生產
- 戰略

**訓練什麼？**
- 架構
- 模型規模
- 資料配比
- 助手類型
- 消融實驗
- ...

**怎麼訓練？**
- 搭建基礎設施
- 訓練框架
- 處理損失峰值
- 中期訓練
- SFT vs RL
- ...

機器學習領域對優化有著近乎痴迷的執念。我們盯著損失曲線、模型架構和吞吐量——畢竟，機器學習從根本上說就是優化模型的損失函數。然而，在深入這些技術細節之前，有一個更根本的問題常常被忽略：**我們真的應該訓練這個模型嗎？**

如下圖所示，開源 AI 生態系統幾乎每天都在發布世界級的模型：Qwen、Gemma、DeepSeek、Kimi、Llama 🪦、Olmo，這份清單每個月都在變長。這些不是研究原型或玩具範例，而是生產級的模型，涵蓋了從多語言理解到程式碼生成和推理等令人驚嘆的廣泛用例。它們大多帶著寬鬆的授權，還有活躍的社群隨時準備幫你使用它們。

這引出了一個讓人不太舒服的真相：也許**你根本不需要訓練自己的模型**。

這似乎是開啟「大語言模型訓練指南」的一個奇怪方式。但許多失敗的訓練專案，失敗的原因不是糟糕的超參數或有 bug 的程式碼，而是有人決定訓練一個他們根本不需要的模型。所以在你承諾開始訓練，並深入研究**如何**執行之前，你需要回答兩個問題：**為什麼**要訓練這個模型？以及**應該訓練什麼**模型？沒有清晰的答案，你會浪費數月的算力和工程時間，去構建一個世界上已經有的東西，或者更糟——一個沒人需要的東西。

讓我們從**為什麼**開始，因為不理解你的目的，你就無法對後續的任何事情做出連貫的決策。

**關於本章節**

這個章節與部落格其他部分不同：它更少關注實驗和技術細節，更多關注戰略規劃。我們會引導你決定**是否需要從零開始訓練，以及應該構建什麼模型**。如果你已經深思熟慮過你的「為什麼」和「是什麼」，可以直接跳到[從小型消融實驗開始](https://huggingfacetb-smol-training-playbook.hf.space/#every-big-model-starts-with-a-small-ablation)章節進行技術深潛。但如果你還不確定，在這裡投入時間會為你之後省下大量精力。
