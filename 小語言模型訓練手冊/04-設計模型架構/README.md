## 設計模型架構

既然我們已經有了實驗框架,是時候做出將定義我們模型的重大決策了。我們做出的每一個選擇,從模型大小到注意力機制到 tokenizer 選擇,都會創造限制和機會,影響模型訓練和使用。

記住[訓練羅盤](https://huggingfacetb-smol-training-playbook.hf.space/#training-compass-why--what--how):在做任何技術選擇之前,我們需要明確 _為什麼_ 和 _什麼_。我們為什麼要訓練這個模型,它應該是什麼樣子?

這聽起來很明顯,但正如我們在訓練羅盤中解釋的,在這裡深思熟慮會塑造我們的決策,並防止我們在無盡的可能實驗空間中迷失。我們的目標是英語 SOTA 模型嗎?長上下文是優先事項嗎?還是我們正在嘗試驗證新架構?在所有這些情況下,訓練迴圈可能看起來相似,但我們執行的實驗和接受的權衡會有所不同。儘早回答這個問題有助於我們決定如何平衡資料和架構工作之間的時間,以及在開始執行之前在每個方面進行多少創新。

那麼,讓我們以身作則,走過指導 SmolLM3 設計的目標。我們想要一個用於端側應用的強大模型,具有競爭力的多語言效能、紮實的數學和編碼能力,以及強大的長上下文處理能力。正如我們之前提到的,這引導我們選擇了一個具有 3B 參數的密集型模型:足夠大以實現強大的能力,但又足夠小以舒適地適應手機。考慮到邊緣設備的記憶體限制和我們的專案時間表(大約 3 個月),我們選擇了密集型 transformer,而不是 MoE 或混合型。

我們從 SmolLM2 那裡有一個在較小規模(1.7B 參數)下用於英語的有效配方,但擴大規模意味著重新驗證一切,並應對新的挑戰,如多語言性和擴展上下文長度。一個明確的例子說明了定義目標如何塑造我們的方法。例如,在 SmolLM2 中,我們在預訓練結束時努力擴展上下文長度,因此對於 SmolLM3,我們從一開始就做出了架構選擇——比如使用 NoPE 和文件內遮罩(稍後會看到)——以最大化我們做對的機會,而且確實奏效了。

> 💡 SmolLM2 是我們上一代的小型語言模型,有三個變體,分別為 135M、360M 和 1.7B 參數,專為端側部署而設計。它們僅支援英語,上下文長度為 8k。

一旦我們的目標明確,我們就可以開始做出將它們變為現實的技術決策。在本章中,我們將介紹我們對這些核心決策的系統化方法:架構、資料和超參數。把這看作我們的戰略規劃階段,正確掌握這些基本要素將使我們在實際的訓練馬拉松中避免代價高昂的錯誤。
